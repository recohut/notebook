<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Concept - TransformerLM Quick Start and Guide | reconb</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Concept - TransformerLM Quick Start and Guide" />
<meta name="author" content="<a href='https://github.com/jalammar'>Jay Alammar</a>" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In this notebook, we will Use a pre-trained TransformerLM, Train a TransformerLM model, and Looking inside the Trax TransformerLM" />
<meta property="og:description" content="In this notebook, we will Use a pre-trained TransformerLM, Train a TransformerLM model, and Looking inside the Trax TransformerLM" />
<link rel="canonical" href="https://nb.recohut.com/concept/nlp/transformer/2021/07/09/concept-transformer-lm.html" />
<meta property="og:url" content="https://nb.recohut.com/concept/nlp/transformer/2021/07/09/concept-transformer-lm.html" />
<meta property="og:site_name" content="reconb" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-07-09T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Concept - TransformerLM Quick Start and Guide" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"<a href='https://github.com/jalammar'>Jay Alammar</a>"},"dateModified":"2021-07-09T00:00:00-05:00","datePublished":"2021-07-09T00:00:00-05:00","description":"In this notebook, we will Use a pre-trained TransformerLM, Train a TransformerLM model, and Looking inside the Trax TransformerLM","headline":"Concept - TransformerLM Quick Start and Guide","mainEntityOfPage":{"@type":"WebPage","@id":"https://nb.recohut.com/concept/nlp/transformer/2021/07/09/concept-transformer-lm.html"},"url":"https://nb.recohut.com/concept/nlp/transformer/2021/07/09/concept-transformer-lm.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://nb.recohut.com/feed.xml" title="reconb" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">reconb</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Concept - TransformerLM Quick Start and Guide</h1><p class="page-description">In this notebook, we will Use a pre-trained TransformerLM, Train a TransformerLM model, and Looking inside the Trax TransformerLM</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-07-09T00:00:00-05:00" itemprop="datePublished">
        Jul 9, 2021
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name"><a href='https://github.com/jalammar'>Jay Alammar</a></span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      12 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#Concept">Concept</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#NLP">NLP</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#Transformer">Transformer</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/recohut/notebook/tree/master/_notebooks/2021-07-09-concept-transformer-lm.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/recohut/notebook/master?filepath=_notebooks%2F2021-07-09-concept-transformer-lm.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/recohut/notebook/blob/master/_notebooks/2021-07-09-concept-transformer-lm.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Frecohut%2Fnotebook%2Fblob%2Fmaster%2F_notebooks%2F2021-07-09-concept-transformer-lm.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#Using-a-pre-trained-TransformerLM">Using a pre-trained TransformerLM </a></li>
<li class="toc-entry toc-h2"><a href="#Train-a-TransformerLM-Model">Train a TransformerLM Model </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Create-the-Model">Create the Model </a></li>
<li class="toc-entry toc-h3"><a href="#Prepare-the-Dataset">Prepare the Dataset </a></li>
<li class="toc-entry toc-h3"><a href="#Train-the-model">Train the model </a></li>
<li class="toc-entry toc-h3"><a href="#Make-predictions">Make predictions </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Transformer-vs.-TransformerLM">Transformer vs. TransformerLM </a></li>
<li class="toc-entry toc-h2"><a href="#Looking-inside-the-Trax-TransformerLM">Looking inside the Trax TransformerLM </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Layers">Layers </a></li>
<li class="toc-entry toc-h3"><a href="#Input,-Decoder-Blocks,-and-Output-Layers">Input, Decoder Blocks, and Output Layers </a></li>
<li class="toc-entry toc-h3"><a href="#Transformer-Decoder-Block">Transformer Decoder Block </a></li>
<li class="toc-entry toc-h3"><a href="#Multiple-Inputs/Outputs,-Branch,-and-Residual">Multiple Inputs/Outputs, Branch, and Residual </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Multiple-Inputs/Outputs">Multiple Inputs/Outputs </a></li>
<li class="toc-entry toc-h4"><a href="#Branch">Branch </a></li>
<li class="toc-entry toc-h4"><a href="#Residual">Residual </a></li>
</ul>
</li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-07-09-concept-transformer-lm.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Language models are machine learning models that power some of the most impressive applications involving text and language (e.g. machine translation, sentiment analysis, chatbots, summarization). At the time of this writing, some of the largest ML models in existence are language models. They are also based on the <a href="https://arxiv.org/abs/1706.03762">transformer</a> architecture. The transformer language model (TransformerLM) is a simpler <a href="https://arxiv.org/pdf/1801.10198.pdf">variation</a> of the original transformer architecture and is useful for plenty of tasks.</p>
<p><img src="https://storage.googleapis.com/ml-intro/t/transformerLM-1.png" alt=""></p>
<p>The <a href="https://trax-ml.readthedocs.io/en/latest/">Trax</a> implementation of TransformerLM focuses on clear code and speed.  It runs without any changes on CPUs, GPUs and TPUs.</p>
<p>In this notebook, we will:</p>
<ol>
<li>Use a pre-trained TransformerLM</li>
<li>Train a TransformerLM model</li>
<li>Looking inside the Trax TransformerLM</li>
</ol>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="o">!</span> pip install -q -U trax
<span class="kn">import</span> <span class="nn">trax</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>     |████████████████████████████████| 419kB 2.8MB/s 
     |████████████████████████████████| 1.5MB 8.4MB/s 
     |████████████████████████████████| 163kB 21.2MB/s 
     |████████████████████████████████| 2.6MB 18.7MB/s 
     |████████████████████████████████| 194kB 35.5MB/s 
     |████████████████████████████████| 368kB 37.9MB/s 
     |████████████████████████████████| 307kB 49.1MB/s 
     |████████████████████████████████| 983kB 47.3MB/s 
     |████████████████████████████████| 358kB 49.9MB/s 
     |████████████████████████████████| 81kB 9.3MB/s 
     |████████████████████████████████| 5.3MB 49.0MB/s 
     |████████████████████████████████| 655kB 50.9MB/s 
     |████████████████████████████████| 71kB 8.3MB/s 
     |████████████████████████████████| 1.1MB 49.3MB/s 
     |████████████████████████████████| 3.5MB 49.2MB/s 
     |████████████████████████████████| 1.1MB 34.8MB/s 
     |████████████████████████████████| 245kB 51.3MB/s 
     |████████████████████████████████| 51kB 5.5MB/s 
     |████████████████████████████████| 890kB 48.7MB/s 
     |████████████████████████████████| 3.0MB 49.9MB/s 
  Building wheel for bz2file (setup.py) ... done
  Building wheel for pypng (setup.py) ... done
  Building wheel for sacremoses (setup.py) ... done
<span class="ansi-red-fg">ERROR: kfac 0.2.3 has requirement tensorflow-probability==0.8, but you'll have tensorflow-probability 0.7.0 which is incompatible.</span>
INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Using-a-pre-trained-TransformerLM">
<a class="anchor" href="#Using-a-pre-trained-TransformerLM" aria-hidden="true"><span class="octicon octicon-link"></span></a>Using a pre-trained TransformerLM<a class="anchor-link" href="#Using-a-pre-trained-TransformerLM"> </a>
</h2>
<p>The following cell loads a pre-trained TransformerLM that sorts a list of four integers.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Have to use the same configuration of the pre-trained model we'll load next</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">trax</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">TransformerLM</span><span class="p">(</span>  
    <span class="n">d_model</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
    <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">'predict'</span><span class="p">)</span>

<span class="c1"># Initialize using pre-trained weights.</span>
<span class="n">model</span><span class="o">.</span><span class="n">init_from_file</span><span class="p">(</span><span class="s1">'gs://ml-intro/models/sort-transformer.pkl.gz'</span><span class="p">,</span>
                     <span class="n">weights_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                     <span class="n">input_signature</span><span class="o">=</span><span class="n">trax</span><span class="o">.</span><span class="n">shapes</span><span class="o">.</span><span class="n">ShapeDtype</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">))</span>

<span class="c1"># Input sequence</span>
<span class="c1"># The 0s indicate the beginning and end of the input sequence</span>
<span class="nb">input</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>


<span class="c1"># Run the model</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">trax</span><span class="o">.</span><span class="n">supervised</span><span class="o">.</span><span class="n">decoding</span><span class="o">.</span><span class="n">autoregressive_sample</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">input</span><span class="p">]),</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Show us the output</span>
<span class="n">output</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([[ 3,  9, 14, 15]])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is a trivial example to get you started and put a toy transformer into your hands. Language models get their name from their ability to assign probabilities to sequences of words. This property makes them useful for generating text (and other types of sequences) by probabilistically choosing the next item in the sequence (often the highest probability one)  -- exactly like the next-word suggestion feature of your smartphone keyboard.</p>
<p>In Trax, TransformerLM is a series of <a href="">Layers</a> combined using the <a href="">Serial</a> combinator. A high level view of the TransformerLM we've declared above can look like this:</p>
<p><img src="https://storage.googleapis.com/ml-intro/t/transformerLM-layers-1.png" alt=""></p>
<p>The model has two decoder layers because we set <code>n_layers</code> to 2. TransformerLM makes predictions by being fed one token at a time, with output tokens typically fed back as inputs (that's the <code>autoregressive</code> part of the <code>autoregressive_sample</code> method we used to generate the output from the model).</p>
<p>If we're to think of a simple model trained to generate the fibonacci sequence, we can give it a number in the sequence and it would continue to generate the next items in the sequence:</p>
<p><img src="https://storage.googleapis.com/ml-intro/t/transformerLM-input-output-fib.gif" alt=""></p>
<h2 id="Train-a-TransformerLM-Model">
<a class="anchor" href="#Train-a-TransformerLM-Model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Train a TransformerLM Model<a class="anchor-link" href="#Train-a-TransformerLM-Model"> </a>
</h2>
<p>Let's train a TransformerLM model. We'll train this one to reverse a list of integers. This is another toy task that we can train a small transformer to do. But using the concepts we'll go over, you'll be able to train proper language models on larger dataset.</p>
<p><strong>Example</strong>: This model is to take a sequence like <code>[1, 2, 3, 4]</code> and return <code>[4, 3, 2, 1]</code>.</p>
<ol>
<li>Create the Model</li>
<li>Prepare the Dataset</li>
<li>Train the model using <code>Trainer</code>
</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Create-the-Model">
<a class="anchor" href="#Create-the-Model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Create the Model<a class="anchor-link" href="#Create-the-Model"> </a>
</h3>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">tiny_transformer_lm</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">'train'</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">trax</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">TransformerLM</span><span class="p">(</span>  
          <span class="n">d_model</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
          <span class="n">vocab_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Refer to <a href="https://trax-ml.readthedocs.io/en/latest/trax.models.html#trax.models.transformer.TransformerLM">TransferLM in the API reference</a> to understand each of its parameters and their default values. We have chosen to create a small model using these values for <code>d_model</code>, <code>d_ff</code>, and <code>n_layers</code> to be able to train the model more quickly on this simple task.</p>
<p><img src="https://storage.googleapis.com/ml-intro/t/untrained-transformer.png" alt=""></p>
<h3 id="Prepare-the-Dataset">
<a class="anchor" href="#Prepare-the-Dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>Prepare the Dataset<a class="anchor-link" href="#Prepare-the-Dataset"> </a>
</h3>
<p>Trax models are trained on streams of data represented as python iterators. <code>trax.data</code> gives you the tools to construct your datapipeline. Trax also gives you readily available access to <a href="https://www.tensorflow.org/datasets">TensorFlow Datasets</a>.</p>
<p>For this simple task, we will create a python generator. Every time we invoke it, it returns a batch of training examples.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">reverse_ints_task</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
  <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">random_ints</span> <span class="o">=</span> <span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span><span class="n">length</span><span class="p">))</span>
    <span class="n">source</span> <span class="o">=</span> <span class="n">random_ints</span>

    <span class="n">target</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">zero</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">zero</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">zero</span><span class="p">,</span> <span class="n">target</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">loss_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">length</span><span class="o">+</span><span class="mi">2</span><span class="p">)),</span>
                                    <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">length</span><span class="p">))],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">yield</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">loss_weights</span><span class="p">)</span>  <span class="c1"># Here inputs and targets are the same.</span>

<span class="n">reverse_ints_inputs</span> <span class="o">=</span>  <span class="n">reverse_ints_task</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This function prepares a dataset and returns one batch at a time. If we ask for a batch size of 8, for example, it returns the following:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">reverse_ints_task</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span>
<span class="n">sequence_batch</span><span class="p">,</span> <span class="n">_</span> <span class="p">,</span> <span class="n">masks</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="n">sequence_batch</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>array([[ 0,  2,  1,  8, 11,  0, 11,  8,  1,  2],
       [ 0, 14,  6, 19,  9,  0,  9, 19,  6, 14],
       [ 0,  9, 13, 24, 27,  0, 27, 24, 13,  9],
       [ 0,  9, 12,  2, 28,  0, 28,  2, 12,  9],
       [ 0, 27, 29, 28, 16,  0, 16, 28, 29, 27],
       [ 0, 15, 18, 11, 28,  0, 28, 11, 18, 15],
       [ 0, 24, 28, 19,  3,  0,  3, 19, 28, 24],
       [ 0, 28,  7,  8, 20,  0, 20,  8,  7, 28]])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You can see that each example starts with 0, then a list of integers, then another 0, then the reverse of the list of integers. The function will give us as many examples and batches as we request.</p>
<p>In addition to the example, the generator returns a mask vector. During the training process, the model is challenged to predict the tokens hidden by the mask (which have a value of 1 associated with that position. So for example, if the first element in the batch is the following vector:</p>
<table><tr>
<td><strong>0</strong></td>
<td>5</td>
<td>6</td>
<td>7</td>
<td>8</td>
<td><strong>0</strong></td>
<td>8</td>
<td>7</td>
<td>6</td>
<td>5</td>
</tr></table>
<p>And the associated mask vector for this example is:</p>
<table><tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr></table>
<p>Then the model will only be presented with the following prefix items, and it has to predict the rest:</p>
<table><tr>
<td><strong>0</strong></td>
<td>5</td>
<td>6</td>
<td>7</td>
<td>8</td>
<td><strong>0</strong></td>
<td>_</td>
<td>_</td>
<td>_ </td>
<td>_</td>
</tr></table>
<p>It's important here to note that while <code>5, 6, 7, 8</code> constitute the input sequence, the <strong>zeros</strong> serve a different purpose. We are using them as special tokens to delimit where the source sequence begins and ends.</p>
<p>With this, we now have a method that streams the dataset in addition to the method that creates the model.</p>
<p><img src="https://storage.googleapis.com/ml-intro/t/untrained-transformer-and-dataset.png" alt=""></p>
<h3 id="Train-the-model">
<a class="anchor" href="#Train-the-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Train the model<a class="anchor-link" href="#Train-the-model"> </a>
</h3>
<p>Trax's <a href="https://trax-ml.readthedocs.io/en/latest/notebooks/trax_intro.html#Supervised-training">training</a> takes care of the training process. We hand it the model, define training and eval tasks, and create the training loop. We then start the training loop.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">trax.supervised</span> <span class="kn">import</span> <span class="n">training</span>
<span class="kn">from</span> <span class="nn">trax</span> <span class="kn">import</span> <span class="n">layers</span> <span class="k">as</span> <span class="n">tl</span>

<span class="c1"># Training task.</span>
<span class="n">train_task</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">TrainTask</span><span class="p">(</span>
    <span class="n">labeled_data</span><span class="o">=</span><span class="n">reverse_ints_inputs</span><span class="p">,</span>
    <span class="n">loss_layer</span><span class="o">=</span><span class="n">tl</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(),</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">trax</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="mf">0.01</span><span class="p">),</span>
    <span class="n">n_steps_per_checkpoint</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
<span class="p">)</span>


<span class="c1"># Evaluaton task.</span>
<span class="n">eval_task</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">EvalTask</span><span class="p">(</span>
    <span class="n">labeled_data</span><span class="o">=</span><span class="n">reverse_ints_inputs</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">tl</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(),</span> <span class="n">tl</span><span class="o">.</span><span class="n">Accuracy</span><span class="p">()],</span>
    <span class="n">n_eval_batches</span><span class="o">=</span><span class="mi">20</span>  <span class="c1"># For less variance in eval numbers.</span>
<span class="p">)</span>

<span class="n">output_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="s1">'~/train_dir/'</span><span class="p">)</span>
<span class="o">!</span>rm -f ~/train_dir/model.pkl.gz  # Remove old model.

<span class="c1"># Train tiny model with Loop.</span>
<span class="n">training_loop</span> <span class="o">=</span> <span class="n">training</span><span class="o">.</span><span class="n">Loop</span><span class="p">(</span>
    <span class="n">tiny_transformer_lm</span><span class="p">(),</span>
    <span class="n">train_task</span><span class="p">,</span>
    <span class="n">eval_tasks</span><span class="o">=</span><span class="p">[</span><span class="n">eval_task</span><span class="p">],</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="n">output_dir</span><span class="p">)</span>

<span class="c1"># run 1000 steps (batches)</span>
<span class="n">training_loop</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>
Step      1: Ran 1 train steps in 17.93 secs
Step      1: train CrossEntropyLoss |  4.14618683
Step      1: eval  CrossEntropyLoss |  3.74931383
Step      1: eval          Accuracy |  0.03359375

Step    500: Ran 499 train steps in 23.67 secs
Step    500: train CrossEntropyLoss |  0.62780923
Step    500: eval  CrossEntropyLoss |  0.01693780
Step    500: eval          Accuracy |  0.99609375

Step   1000: Ran 500 train steps in 5.34 secs
Step   1000: train CrossEntropyLoss |  0.00926041
Step   1000: eval  CrossEntropyLoss |  0.00390428
Step   1000: eval          Accuracy |  0.99921875
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The Trainer is the third key component in this process that helps us arrive at the trained model.</p>
<p><img src="https://storage.googleapis.com/ml-intro/t/transformerLM-training.png" alt=""></p>
<h3 id="Make-predictions">
<a class="anchor" href="#Make-predictions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Make predictions<a class="anchor-link" href="#Make-predictions"> </a>
</h3>
<p>Let's take our newly minted model for a ride. To do that, we load it up, and use the handy <code>autoregressive_sample</code> method to feed it our input sequence and return the output sequence. These components now look like this:</p>
<p><img src="https://storage.googleapis.com/ml-intro/t/transformerLM-sampling-prediction.png" alt=""></p>
<p>And this is the code to do just that:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>

<span class="c1"># Initialize model for inference.</span>
<span class="n">predict_model</span> <span class="o">=</span> <span class="n">tiny_transformer_lm</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s1">'predict'</span><span class="p">)</span>
<span class="n">predict_signature</span> <span class="o">=</span> <span class="n">trax</span><span class="o">.</span><span class="n">shapes</span><span class="o">.</span><span class="n">ShapeDtype</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">predict_model</span><span class="o">.</span><span class="n">init_from_file</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">output_dir</span><span class="p">,</span> <span class="s2">"model.pkl.gz"</span><span class="p">),</span>
                             <span class="n">weights_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">input_signature</span><span class="o">=</span><span class="n">predict_signature</span><span class="p">)</span>

<span class="c1"># Run the model</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">trax</span><span class="o">.</span><span class="n">supervised</span><span class="o">.</span><span class="n">decoding</span><span class="o">.</span><span class="n">autoregressive_sample</span><span class="p">(</span>
    <span class="n">predict_model</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

<span class="c1"># Print the contents of output</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[[10  8  6  4]]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If things go correctly, the model would be able to reverse the string and output <code>[[10 8 6 4]]</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Transformer-vs.-TransformerLM">
<a class="anchor" href="#Transformer-vs.-TransformerLM" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transformer vs. TransformerLM<a class="anchor-link" href="#Transformer-vs.-TransformerLM"> </a>
</h2>
<p>TransformerLM is a great place to start learning about Transformer architectures. The main difference between it and the original Transformer is that it's made up of a decoder stack, while Transformer is made up of an encoder stack and decoder stack (with the decoder stack being nearly identical to TransformerLM).</p>
<p><img src="https://storage.googleapis.com/ml-intro/t/transformer-vs-transformerlm.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Looking-inside-the-Trax-TransformerLM">
<a class="anchor" href="#Looking-inside-the-Trax-TransformerLM" aria-hidden="true"><span class="octicon octicon-link"></span></a>Looking inside the Trax TransformerLM<a class="anchor-link" href="#Looking-inside-the-Trax-TransformerLM"> </a>
</h2>
<p>In Trax, TransformerLM is implemented as a single Serial layer</p>
<p><img src="https://storage.googleapis.com/ml-intro/t/transformerLM-serial-trax-layer.png" alt=""></p>
<p>This graph shows you two of the central concepts in Trax. Layers are the basic building blocks. Serial is the most common way to compose multiple layers together in sequence.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Layers">
<a class="anchor" href="#Layers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Layers<a class="anchor-link" href="#Layers"> </a>
</h3>
<p>Layers are best described in the <a href="https://trax-ml.readthedocs.io/en/latest/notebooks/layers_intro.html">Trax Layers Intro</a>.</p>
<p>For a Transformer to make a calculation (translate a sentence, summarize an article, or generate text), input tokens pass through many steps of transformation and
computation (e.g. embedding, positional encoding, self-attention, feed-forward neural networks...tec). Each of these steps is a layer (some with their own sublayers).</p>
<p>Each layer you use or define takes a fixed number of input tensors and returns a fixed number of output tensors (n_in and n_out respectively, both of which default to 1).</p>
<p><img src="https://storage.googleapis.com/ml-intro/t/trax-layer-inputs-outputs.png" alt=""></p>
<p>A simple example of a layer is the ReLU activation function:</p>
<p><img src="https://storage.googleapis.com/ml-intro/t/relu-trax-layer.png" alt=""></p>
<p>Trax is a deep learning library, though. And so, a layer can also contain weights. An example of this is the Dense layer. Here is a dense layer that multiplies the input tensor with a weight matrix (<code>W</code>) and adds a bias (<code>b</code>) (both W and b are saved inside the <code>weights</code> property of the layer):</p>
<p><img src="https://storage.googleapis.com/ml-intro/t/dense-trax-layer.png" alt=""></p>
<p>In practice, Dense and Relu often go hand in hand. With Dense first working on a tensor, and ReLu then processing the output of the Dense layer. This is a perfect job for Serial, which, in simple cases, chains two or more layers and hands over the output of the first layer to the following one:</p>
<p><img src="https://storage.googleapis.com/ml-intro/t/serial-dense-relu-trax.png" alt=""></p>
<p>The Serial combinator is a layer itself. So we can think of it as a layer containing a number of sublayers:</p>
<p><img src="https://storage.googleapis.com/ml-intro/t/serial-layer-dense-relu-trax.png" alt=""></p>
<p>With these concepts in mind, let's go back and unpack the layers inside the TransformerLM Serial.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Input,-Decoder-Blocks,-and-Output-Layers">
<a class="anchor" href="#Input,-Decoder-Blocks,-and-Output-Layers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Input, Decoder Blocks, and Output Layers<a class="anchor-link" href="#Input,-Decoder-Blocks,-and-Output-Layers"> </a>
</h3>
<p>It's straightforward to read the delcaration of TransformerLM to understand the layers that make it up. In general, you can group these layers into a set of input layers, then Transformer decoder blocks, and a set of output blocks. The number of Transformer blocks (<code>n_layers</code>) is one of the key parameters when creating a TransformerLM model. This is a way to think of the layer groups of a TransformerLM:</p>
<div align="center">
<figure>
  
    <img class="docimage" src="https://storage.googleapis.com/ml-intro/t/TransformerLM-layer-groups.png" alt="">
    
    
</figure>

</div>
<ul>
<li>The <strong>input layers</strong> take each input token id and look up its proper embedding and positional encoding.</li>
<li>The prediction calculations happen in the stack of <strong>decoder blocks</strong>.</li>
<li>The <strong>output layers</strong> take the output of the final Decoder block and project it to the output vocabulary. The LogSoftmax layer then turns the scoring of each potential output token into a probability score.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Transformer-Decoder-Block">
<a class="anchor" href="#Transformer-Decoder-Block" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transformer Decoder Block<a class="anchor-link" href="#Transformer-Decoder-Block"> </a>
</h3>
<p>A decoder block has two major components:</p>
<ul>
<li>A <strong>Causal self-attention</strong> layer. Self-attention incorporates information from other tokens that could help make more sense of the current token being processed. Causal attention only allows the incorporation of information from previous positions. One key parameter when creating a TransformerLM model is <code>n_heads</code>, which is the number of "attention heads".</li>
<li>A <strong>FeedForward</strong> component. This is where the primary prediction computation is calculated. The key parameter associated with this layer is <code>d_ff</code>, which specifies the dimensions of the neural network layer used in this block. </li>
</ul>
<p><img src="https://storage.googleapis.com/ml-intro/t/transformerLM-d_self-attention-ff.png" alt=""></p>
<p>This figure also shows the <code>d_model</code> parameter, which specifies the dimension of tensors at most points in the model, including the embedding, and the majority of tensors handed off between the various layers in the model.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Multiple-Inputs/Outputs,-Branch,-and-Residual">
<a class="anchor" href="#Multiple-Inputs/Outputs,-Branch,-and-Residual" aria-hidden="true"><span class="octicon octicon-link"></span></a>Multiple Inputs/Outputs, Branch, and Residual<a class="anchor-link" href="#Multiple-Inputs/Outputs,-Branch,-and-Residual"> </a>
</h3>
<p>There are a couple more central Trax concept to cover to gain a deeper understanding of how Trax implements TransformerLM</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Multiple-Inputs/Outputs">
<a class="anchor" href="#Multiple-Inputs/Outputs" aria-hidden="true"><span class="octicon octicon-link"></span></a>Multiple Inputs/Outputs<a class="anchor-link" href="#Multiple-Inputs/Outputs"> </a>
</h4>
<p>The layers we've seen so far all have one input tensor and one output tensor. A layer could have more. For example, the Concatenate layer:</p>
<p><img src="https://storage.googleapis.com/ml-intro/t/trax-concatenate-layer.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Branch">
<a class="anchor" href="#Branch" aria-hidden="true"><span class="octicon octicon-link"></span></a>Branch<a class="anchor-link" href="#Branch"> </a>
</h4>
<p>We saw the Serial combinator that combines layers serially. Branch combines layers in parallel. It supplies input copies to each of its sublayers.</p>
<p>For example, if we wrap two layers (each expecting one input) in a Branch layer, and we pass a tensor to Branch, it copies it as the input to both of its sublayers as shown here:</p>
<p><img src="https://storage.googleapis.com/ml-intro/t/branch-combinator-trax-inputs.png" alt=""></p>
<p>Since the sublayers have two outputs (one from each), then the Branch layer would also end up outputing both of those tensors:</p>
<p><img src="https://storage.googleapis.com/ml-intro/t/branch-combinator-trax-output.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Residual">
<a class="anchor" href="#Residual" aria-hidden="true"><span class="octicon octicon-link"></span></a>Residual<a class="anchor-link" href="#Residual"> </a>
</h4>
<p>Residual connections are an important component of Transformer architectures. Inside a Decoder Block, both the causal-attention layer and the
feed-forward layer have residual connections around them:</p>
<p><img src="https://storage.googleapis.com/ml-intro/t/trax-residual-input.png" alt=""></p>
<p>What that means, is that a copy of the input tensor is added to the output of the Attention layer:</p>
<p><img src="https://storage.googleapis.com/ml-intro/t/trax-residual-output.png" alt=""></p>
<p>In Trax, this is achieved using the Residual layer, which combines both the Serial and Branch combinators:</p>
<p><img src="https://storage.googleapis.com/ml-intro/t/trax-residual-layers-1.png" alt=""></p>
<p>Similarly, the feed-forward sublayer has another residual connection around it:</p>
<p><img src="https://storage.googleapis.com/ml-intro/t/trax-transformer-residual-layers-2.png" alt=""></p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="recohut/notebook"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/concept/nlp/transformer/2021/07/09/concept-transformer-lm.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Jupyter notebook database.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/recohut" target="_blank" title="recohut"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
