<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Sentence Classification with Transformers | reconb</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Sentence Classification with Transformers" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Jupyter notebook database." />
<meta property="og:description" content="Jupyter notebook database." />
<link rel="canonical" href="https://nb.recohut.com/2022/01/27/sentence-classification.html" />
<meta property="og:url" content="https://nb.recohut.com/2022/01/27/sentence-classification.html" />
<meta property="og:site_name" content="reconb" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-01-27T00:00:00-06:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Sentence Classification with Transformers" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-01-27T00:00:00-06:00","datePublished":"2022-01-27T00:00:00-06:00","description":"Jupyter notebook database.","headline":"Sentence Classification with Transformers","mainEntityOfPage":{"@type":"WebPage","@id":"https://nb.recohut.com/2022/01/27/sentence-classification.html"},"url":"https://nb.recohut.com/2022/01/27/sentence-classification.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://nb.recohut.com/feed.xml" title="reconb" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">reconb</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Sentence Classification with Transformers</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-01-27T00:00:00-06:00" itemprop="datePublished">
        Jan 27, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      36 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/recohut/notebook/tree/master/_notebooks/2022-01-27-sentence-classification.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/recohut/notebook/master?filepath=_notebooks%2F2022-01-27-sentence-classification.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/recohut/notebook/blob/master/_notebooks/2022-01-27-sentence-classification.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Frecohut%2Fnotebook%2Fblob%2Fmaster%2F_notebooks%2F2022-01-27-sentence-classification.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-01-27-sentence-classification.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install transformers
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'll use <a href="https://nyu-mll.github.io/CoLA/">The Corpus of Linguistic Acceptability (CoLA)</a> dataset for single sentence classification. It's a set of sentences labeled as grammatically correct or incorrect. It was first published in May of 2018, and is one of the tests included in the "GLUE Benchmark" on which models like BERT are competing.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.1.-Download-&amp;-Extract">2.1. Download &amp; Extract<a class="anchor-link" href="#2.1.-Download-&amp;-Extract"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The dataset is hosted on GitHub in this repo: <a href="https://nyu-mll.github.io/CoLA/">https://nyu-mll.github.io/CoLA/</a></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>wget cola_public_1.1.zip <span class="s1">&#39;https://nyu-mll.github.io/CoLA/cola_public_1.1.zip&#39;</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>--2020-05-07 17:34:58--  http://cola_public_1.1.zip/
Resolving cola_public_1.1.zip (cola_public_1.1.zip)... failed: Name or service not known.
wget: unable to resolve host address ‘cola_public_1.1.zip’
--2020-05-07 17:34:58--  https://nyu-mll.github.io/CoLA/cola_public_1.1.zip
Resolving nyu-mll.github.io (nyu-mll.github.io)... 185.199.108.153, 185.199.110.153, 185.199.111.153, ...
Connecting to nyu-mll.github.io (nyu-mll.github.io)|185.199.108.153|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 255330 (249K) [application/zip]
Saving to: ‘cola_public_1.1.zip.1’

cola_public_1.1.zip 100%[===================&gt;] 249.35K  --.-KB/s    in 0.06s   

2020-05-07 17:34:58 (4.35 MB/s) - ‘cola_public_1.1.zip.1’ saved [255330/255330]

FINISHED --2020-05-07 17:34:58--
Total wall clock time: 0.2s
Downloaded: 1 files, 249K in 0.06s (4.35 MB/s)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Unzip the dataset to the file system. You can browse the file system of the Colab instance in the sidebar on the left.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="s1">&#39;./cola_public/&#39;</span><span class="p">):</span>
    <span class="o">!</span>unzip cola_public_1.1.zip
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Archive:  cola_public_1.1.zip
   creating: cola_public/
  inflating: cola_public/README      
   creating: cola_public/tokenized/
  inflating: cola_public/tokenized/in_domain_dev.tsv  
  inflating: cola_public/tokenized/in_domain_train.tsv  
  inflating: cola_public/tokenized/out_of_domain_dev.tsv  
   creating: cola_public/raw/
  inflating: cola_public/raw/in_domain_dev.tsv  
  inflating: cola_public/raw/in_domain_train.tsv  
  inflating: cola_public/raw/out_of_domain_dev.tsv  
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.2.-Parse">2.2. Parse<a class="anchor-link" href="#2.2.-Parse"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see from the file names that both <code>tokenized</code> and <code>raw</code> versions of the data are available.</p>
<p>We can't use the pre-tokenized version because, in order to apply the pre-trained BERT, we <em>must</em> use the tokenizer provided by the model. This is because (1) the model has a specific, fixed vocabulary and (2) the BERT tokenizer has a particular way of handling out-of-vocabulary words.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'll use pandas to parse the "in-domain" training set and look at a few of its properties and data points.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Load the dataset into a pandas dataframe.</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;./cola_public/raw/in_domain_train.tsv&quot;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sentence_source&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="s1">&#39;label_notes&#39;</span><span class="p">,</span> <span class="s1">&#39;sentence&#39;</span><span class="p">])</span>

<span class="c1"># Report the number of sentences.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of training sentences: </span><span class="si">{:,}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

<span class="c1"># Display 10 random rows from the data.</span>
<span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Number of training sentences: 8,551

</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sentence_source</th>
      <th>label</th>
      <th>label_notes</th>
      <th>sentence</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>4166</th>
      <td>ks08</td>
      <td>1</td>
      <td>NaN</td>
      <td>We listened to as little of his speech as poss...</td>
    </tr>
    <tr>
      <th>8388</th>
      <td>ad03</td>
      <td>1</td>
      <td>NaN</td>
      <td>After the executioner left, Poseidon wept.</td>
    </tr>
    <tr>
      <th>1147</th>
      <td>r-67</td>
      <td>1</td>
      <td>NaN</td>
      <td>He figured out the answer.</td>
    </tr>
    <tr>
      <th>2511</th>
      <td>l-93</td>
      <td>1</td>
      <td>NaN</td>
      <td>Philippa cried her eyes dry.</td>
    </tr>
    <tr>
      <th>6825</th>
      <td>m_02</td>
      <td>1</td>
      <td>NaN</td>
      <td>The car was driven by Alison.</td>
    </tr>
    <tr>
      <th>3726</th>
      <td>ks08</td>
      <td>1</td>
      <td>NaN</td>
      <td>This car stinks.</td>
    </tr>
    <tr>
      <th>4408</th>
      <td>ks08</td>
      <td>0</td>
      <td>*</td>
      <td>John stopped can to sign in tune.</td>
    </tr>
    <tr>
      <th>4533</th>
      <td>ks08</td>
      <td>1</td>
      <td>NaN</td>
      <td>Kim is happy and Sandy is too.</td>
    </tr>
    <tr>
      <th>5349</th>
      <td>b_73</td>
      <td>0</td>
      <td>*</td>
      <td>Mary speaks so much gently.</td>
    </tr>
    <tr>
      <th>1435</th>
      <td>r-67</td>
      <td>1</td>
      <td>NaN</td>
      <td>The boy's loud playing of the piano drove ever...</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The two properties we actually care about are the the <code>sentence</code> and its <code>label</code>, which is referred to as the "acceptibility judgment" (0=unacceptable, 1=acceptable).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here are five sentences which are labeled as not grammatically acceptible. Note how much more difficult this task is than something like sentiment analysis!</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">)[[</span><span class="s1">&#39;sentence&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">]]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sentence</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>3931</th>
      <td>In the box put John the book.</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2727</th>
      <td>We offered a job behind her.</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1138</th>
      <td>We elected my father, who had just turned 60, ...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2390</th>
      <td>The captain named the ship as Seafarer.</td>
      <td>0</td>
    </tr>
    <tr>
      <th>714</th>
      <td>I saw the men all.</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's extract the sentences and labels of our training set as numpy ndarrays.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sentences</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sentence</span><span class="o">.</span><span class="n">values</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">values</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Tokenization-&amp;-Input-Formatting">Tokenization &amp; Input Formatting<a class="anchor-link" href="#Tokenization-&amp;-Input-Formatting"> </a></h2><p>In this section, we'll transform our dataset into the format that BERT can be trained on.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3.1.-BERT-Tokenizer">3.1. BERT Tokenizer<a class="anchor-link" href="#3.1.-BERT-Tokenizer"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.</p>
<p>The tokenization must be performed by the tokenizer included with BERT--the below cell will download this for us. We'll be using the "uncased" version here.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span>

<span class="c1"># Load the BERT tokenizer.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Loading BERT tokenizer...&#39;</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">,</span> <span class="n">do_lower_case</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Loading BERT tokenizer...

</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's apply the tokenizer to one sentence just to see the output.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39; Original: &#39;</span><span class="p">,</span> <span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># Print the sentence split into tokens.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Tokenized: &#39;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

<span class="c1"># Print the sentence mapped to token ids.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Token IDs: &#39;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_tokens_to_ids</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre> Original:  Our friends won&#39;t buy this analysis, let alone the next one we propose.
Tokenized:  [&#39;our&#39;, &#39;friends&#39;, &#39;won&#39;, &#34;&#39;&#34;, &#39;t&#39;, &#39;buy&#39;, &#39;this&#39;, &#39;analysis&#39;, &#39;,&#39;, &#39;let&#39;, &#39;alone&#39;, &#39;the&#39;, &#39;next&#39;, &#39;one&#39;, &#39;we&#39;, &#39;propose&#39;, &#39;.&#39;]
Token IDs:  [2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012]
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>When we actually convert all of our sentences, we'll use the <code>tokenize.encode</code> function to handle both steps, rather than calling <code>tokenize</code> and <code>convert_tokens_to_ids</code> separately.</p>
<p>Before we can do that, though, we need to talk about some of BERT's formatting requirements.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3.2.-Required-Formatting">3.2. Required Formatting<a class="anchor-link" href="#3.2.-Required-Formatting"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The above code left out a few required formatting steps that we'll look at here.</p>
<p><em>Side Note: The input format to BERT seems "over-specified" to me... We are required to give it a number of pieces of information which seem redundant, or like they could easily be inferred from the data without us explicity providing it. But it is what it is, and I suspect it will make more sense once I have a deeper understanding of the BERT internals.</em></p>
<p>We are required to:</p>
<ol>
<li>Add special tokens to the start and end of each sentence.</li>
<li>Pad &amp; truncate all sentences to a single constant length.</li>
<li>Explicitly differentiate real tokens from padding tokens with the "attention mask".</li>
</ol>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Special-Tokens">Special Tokens<a class="anchor-link" href="#Special-Tokens"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong><code>[SEP]</code></strong></p>
<p>At the end of every sentence, we need to append the special <code>[SEP]</code> token.</p>
<p>This token is an artifact of two-sentence tasks, where BERT is given two separate sentences and asked to determine something (e.g., can the answer to the question in sentence A be found in sentence B?).</p>
<p>I am not certain yet why the token is still required when we have only single-sentence input, but it is!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong><code>[CLS]</code></strong></p>
<p>For classification tasks, we must prepend the special <code>[CLS]</code> token to the beginning of every sentence.</p>
<p>This token has special significance. BERT consists of 12 Transformer layers. Each transformer takes in a list of token embeddings, and produces the same number of embeddings on the output (but with the feature values changed, of course!).</p>
<p><img src="http://www.mccormickml.com/assets/BERT/CLS_token_500x606.png" alt="Illustration of CLS token purpose" /></p>
<p>On the output of the final (12th) transformer, <em>only the first embedding (corresponding to the [CLS] token) is used by the classifier</em>.</p>
<blockquote><p>"The first token of every sequence is always a special classification token (<code>[CLS]</code>). The final hidden state
corresponding to this token is used as the aggregate sequence representation for classification
tasks." (from the <a href="https://arxiv.org/pdf/1810.04805.pdf">BERT paper</a>)
You might think to try some pooling strategy over the final embeddings, but this isn't necessary. Because BERT is trained to only use this [CLS] token for classification, we know that the model has been motivated to encode everything it needs for the classification step into that single 768-value embedding vector. It's already done the pooling for us!</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Sentence-Length-&amp;-Attention-Mask">Sentence Length &amp; Attention Mask<a class="anchor-link" href="#Sentence-Length-&amp;-Attention-Mask"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The sentences in our dataset obviously have varying lengths, so how does BERT handle this?</p>
<p>BERT has two constraints:</p>
<ol>
<li>All sentences must be padded or truncated to a single, fixed length.</li>
<li>The maximum sentence length is 512 tokens.</li>
</ol>
<p>Padding is done with a special <code>[PAD]</code> token, which is at index 0 in the BERT vocabulary. The below illustration demonstrates padding out to a "MAX_LEN" of 8 tokens.</p>
<p><img src="http://www.mccormickml.com/assets/BERT/padding_and_mask.png" alt="" /></p>
<p>The "Attention Mask" is simply an array of 1s and 0s indicating which tokens are padding and which aren't (seems kind of redundant, doesn't it?!). This mask tells the "Self-Attention" mechanism in BERT not to incorporate these PAD tokens into its interpretation of the sentence.</p>
<p>The maximum length does impact training and evaluation speed, however. 
For example, with a Tesla K80:</p>
<p><code>MAX_LEN = 128  --&gt;  Training epochs take ~5:28 each</code></p>
<p><code>MAX_LEN = 64   --&gt;  Training epochs take ~2:57 each</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3.3.-Tokenize-Dataset">3.3. Tokenize Dataset<a class="anchor-link" href="#3.3.-Tokenize-Dataset"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The transformers library provides a helpful <code>encode</code> function which will handle most of the parsing and data prep steps for us.</p>
<p>Before we are ready to encode our text, though, we need to decide on a <strong>maximum sentence length</strong> for padding / truncating to.</p>
<p>The below cell will perform one tokenization pass of the dataset in order to measure the maximum sentence length.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">max_len</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># For every sentence...</span>
<span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>

    <span class="c1"># Tokenize the text and add `[CLS]` and `[SEP]` tokens.</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sent</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Update the maximum sentence length.</span>
    <span class="n">max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Max sentence length: &#39;</span><span class="p">,</span> <span class="n">max_len</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Max sentence length:  47
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Just in case there are some longer test sentences, I'll set the maximum length to 64.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we're ready to perform the real tokenization.</p>
<p>The <code>tokenizer.encode_plus</code> function combines multiple steps for us:</p>
<ol>
<li>Split the sentence into tokens.</li>
<li>Add the special <code>[CLS]</code> and <code>[SEP]</code> tokens.</li>
<li>Map the tokens to their IDs.</li>
<li>Pad or truncate all sentences to the same length.</li>
<li>Create the attention masks which explicitly differentiate real tokens from <code>[PAD]</code> tokens.</li>
</ol>
<p>The first four features are in <code>tokenizer.encode</code>, but I'm using <code>tokenizer.encode_plus</code> to get the fifth item (attention masks). Documentation is <a href="https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus">here</a>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">input_ids</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">attention_masks</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># For every sentence...</span>
<span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
    <span class="c1"># `encode_plus` will:</span>
    <span class="c1">#   (1) Tokenize the sentence.</span>
    <span class="c1">#   (2) Prepend the `[CLS]` token to the start.</span>
    <span class="c1">#   (3) Append the `[SEP]` token to the end.</span>
    <span class="c1">#   (4) Map tokens to their IDs.</span>
    <span class="c1">#   (5) Pad or truncate the sentence to `max_length`</span>
    <span class="c1">#   (6) Create attention masks for [PAD] tokens.</span>
    <span class="n">encoded_dict</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_plus</span><span class="p">(</span>
                        <span class="n">sent</span><span class="p">,</span>                      <span class="c1"># Sentence to encode.</span>
                        <span class="n">add_special_tokens</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="c1"># Add &#39;[CLS]&#39; and &#39;[SEP]&#39;</span>
                        <span class="n">max_length</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>           <span class="c1"># Pad &amp; truncate all sentences.</span>
                        <span class="n">pad_to_max_length</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                        <span class="n">return_attention_mask</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>   <span class="c1"># Construct attn. masks.</span>
                        <span class="n">return_tensors</span> <span class="o">=</span> <span class="s1">&#39;pt&#39;</span><span class="p">,</span>     <span class="c1"># Return pytorch tensors.</span>
                   <span class="p">)</span>
    
    <span class="c1"># Add the encoded sentence to the list.    </span>
    <span class="n">input_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">encoded_dict</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">])</span>
    
    <span class="c1"># And its attention mask (simply differentiates padding from non-padding).</span>
    <span class="n">attention_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">encoded_dict</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">])</span>

<span class="c1"># Convert the lists into tensors.</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">attention_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">attention_masks</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

<span class="c1"># Print sentence 0, now as a list of IDs.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Original: &#39;</span><span class="p">,</span> <span class="n">sentences</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Token IDs:&#39;</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Original:  Our friends won&#39;t buy this analysis, let alone the next one we propose.
Token IDs: tensor([  101,  2256,  2814,  2180,  1005,  1056,  4965,  2023,  4106,  1010,
         2292,  2894,  1996,  2279,  2028,  2057, 16599,  1012,   102,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
            0,     0,     0,     0])
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3.4.-Training-&amp;-Validation-Split">3.4. Training &amp; Validation Split<a class="anchor-link" href="#3.4.-Training-&amp;-Validation-Split"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Divide up our training set to use 90% for training and 10% for validation.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">TensorDataset</span><span class="p">,</span> <span class="n">random_split</span>

<span class="c1"># Combine the training inputs into a TensorDataset.</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_masks</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

<span class="c1"># Create a 90-10 train-validation split.</span>

<span class="c1"># Calculate the number of samples to include in each set.</span>
<span class="n">train_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.9</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>
<span class="n">val_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="o">-</span> <span class="n">train_size</span>

<span class="c1"># Divide the dataset by randomly selecting samples.</span>
<span class="n">train_dataset</span><span class="p">,</span> <span class="n">val_dataset</span> <span class="o">=</span> <span class="n">random_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="p">[</span><span class="n">train_size</span><span class="p">,</span> <span class="n">val_size</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{:&gt;5,}</span><span class="s1"> training samples&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_size</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{:&gt;5,}</span><span class="s1"> validation samples&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">val_size</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>7,695 training samples
  856 validation samples
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">RandomSampler</span><span class="p">,</span> <span class="n">SequentialSampler</span>

<span class="c1"># The DataLoader needs to know our batch size for training, so we specify it </span>
<span class="c1"># here. For fine-tuning BERT on a specific task, the authors recommend a batch </span>
<span class="c1"># size of 16 or 32.</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>

<span class="c1"># Create the DataLoaders for our training and validation sets.</span>
<span class="c1"># We&#39;ll take training samples in random order. </span>
<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">train_dataset</span><span class="p">,</span>  <span class="c1"># The training samples.</span>
            <span class="n">sampler</span> <span class="o">=</span> <span class="n">RandomSampler</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">),</span> <span class="c1"># Select batches randomly</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="c1"># Trains with this batch size.</span>
        <span class="p">)</span>

<span class="c1"># For validation the order doesn&#39;t matter, so we&#39;ll just read them sequentially.</span>
<span class="n">validation_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="n">val_dataset</span><span class="p">,</span> <span class="c1"># The validation samples.</span>
            <span class="n">sampler</span> <span class="o">=</span> <span class="n">SequentialSampler</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">),</span> <span class="c1"># Pull out batches sequentially.</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="c1"># Evaluate with this batch size.</span>
        <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Train-Our-Classification-Model">Train Our Classification Model<a class="anchor-link" href="#Train-Our-Classification-Model"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that our input data is properly formatted, it's time to fine tune the BERT model.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="4.1.-BertForSequenceClassification">4.1. BertForSequenceClassification<a class="anchor-link" href="#4.1.-BertForSequenceClassification"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task.</p>
<p>Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.</p>
<p>Here is the current list of classes provided for fine-tuning:</p>
<ul>
<li>BertModel</li>
<li>BertForPreTraining</li>
<li>BertForMaskedLM</li>
<li>BertForNextSentencePrediction</li>
<li><strong>BertForSequenceClassification</strong> - The one we'll use.</li>
<li>BertForTokenClassification</li>
<li>BertForQuestionAnswering</li>
</ul>
<p>The documentation for these can be found under <a href="https://huggingface.co/transformers/v2.2.0/model_doc/bert.html">here</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'll be using <a href="https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#bertforsequenceclassification">BertForSequenceClassification</a>. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>OK, let's load BERT! There are a few different pre-trained BERT models available. "bert-base-uncased" means the version that has only lowercase letters ("uncased") and is the smaller version of the two ("base" vs "large").</p>
<p>The documentation for <code>from_pretrained</code> can be found <a href="https://huggingface.co/transformers/v2.2.0/main_classes/model.html#transformers.PreTrainedModel.from_pretrained">here</a>, with the additional parameters defined <a href="https://huggingface.co/transformers/v2.2.0/main_classes/configuration.html#transformers.PretrainedConfig">here</a>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertForSequenceClassification</span><span class="p">,</span> <span class="n">AdamW</span><span class="p">,</span> <span class="n">BertConfig</span>

<span class="c1"># Load BertForSequenceClassification, the pretrained BERT model with a single </span>
<span class="c1"># linear classification layer on top. </span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">,</span> <span class="c1"># Use the 12-layer BERT model, with an uncased vocab.</span>
    <span class="n">num_labels</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="c1"># The number of output labels--2 for binary classification.</span>
                    <span class="c1"># You can increase this for multi-class tasks.   </span>
    <span class="n">output_attentions</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="c1"># Whether the model returns attentions weights.</span>
    <span class="n">output_hidden_states</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="c1"># Whether the model returns all hidden-states.</span>
<span class="p">)</span>

<span class="c1"># Tell pytorch to run this model on the GPU.</span>
<span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>

</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>BertForSequenceClassification(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
  (classifier): Linear(in_features=768, out_features=2, bias=True)
)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Just for curiosity's sake, we can browse all of the model's parameters by name here.</p>
<p>In the below cell, I've printed out the names and dimensions of the weights for:</p>
<ol>
<li>The embedding layer.</li>
<li>The first of the twelve transformers.</li>
<li>The output layer.</li>
</ol>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The BERT model has </span><span class="si">{:}</span><span class="s1"> different named parameters.</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)))</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;==== Embedding Layer ====</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:&lt;55}</span><span class="s2"> </span><span class="si">{:&gt;12}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()))))</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">==== First Transformer ====</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">[</span><span class="mi">5</span><span class="p">:</span><span class="mi">21</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:&lt;55}</span><span class="s2"> </span><span class="si">{:&gt;12}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()))))</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">==== Output Layer ====</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">:]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:&lt;55}</span><span class="s2"> </span><span class="si">{:&gt;12}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()))))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>The BERT model has 201 different named parameters.

==== Embedding Layer ====

bert.embeddings.word_embeddings.weight                  (30522, 768)
bert.embeddings.position_embeddings.weight                (512, 768)
bert.embeddings.token_type_embeddings.weight                (2, 768)
bert.embeddings.LayerNorm.weight                              (768,)
bert.embeddings.LayerNorm.bias                                (768,)

==== First Transformer ====

bert.encoder.layer.0.attention.self.query.weight          (768, 768)
bert.encoder.layer.0.attention.self.query.bias                (768,)
bert.encoder.layer.0.attention.self.key.weight            (768, 768)
bert.encoder.layer.0.attention.self.key.bias                  (768,)
bert.encoder.layer.0.attention.self.value.weight          (768, 768)
bert.encoder.layer.0.attention.self.value.bias                (768,)
bert.encoder.layer.0.attention.output.dense.weight        (768, 768)
bert.encoder.layer.0.attention.output.dense.bias              (768,)
bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)
bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)
bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)
bert.encoder.layer.0.intermediate.dense.bias                 (3072,)
bert.encoder.layer.0.output.dense.weight                 (768, 3072)
bert.encoder.layer.0.output.dense.bias                        (768,)
bert.encoder.layer.0.output.LayerNorm.weight                  (768,)
bert.encoder.layer.0.output.LayerNorm.bias                    (768,)

==== Output Layer ====

bert.pooler.dense.weight                                  (768, 768)
bert.pooler.dense.bias                                        (768,)
classifier.weight                                           (2, 768)
classifier.bias                                                 (2,)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="4.2.-Optimizer-&amp;-Learning-Rate-Scheduler">4.2. Optimizer &amp; Learning Rate Scheduler<a class="anchor-link" href="#4.2.-Optimizer-&amp;-Learning-Rate-Scheduler"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.</p>
<p>For the purposes of fine-tuning, the authors recommend choosing from the following values (from Appendix A.3 of the <a href="https://arxiv.org/pdf/1810.04805.pdf">BERT paper</a>):</p>
<blockquote><ul>
<li><strong>Batch size:</strong> 16, 32  - <strong>Learning rate (Adam):</strong> 5e-5, 3e-5, 2e-5  </li>
<li><strong>Number of epochs:</strong> 2, 3, 4 </li>
</ul>
</blockquote>
<p>We chose:</p>
<ul>
<li>Batch size: 32 (set when creating our DataLoaders)</li>
<li>Learning rate: 2e-5</li>
<li>Epochs: 4 (we'll see that this is probably too many...)</li>
</ul>
<p>The epsilon parameter <code>eps = 1e-8</code> is "a very small number to prevent any division by zero in the implementation" (from <a href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/">here</a>).</p>
<p>You can find the creation of the AdamW optimizer in <code>run_glue.py</code> <a href="https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109">here</a>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># I believe the &#39;W&#39; stands for &#39;Weight Decay fix&quot;</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                  <span class="n">lr</span> <span class="o">=</span> <span class="mf">2e-5</span><span class="p">,</span> <span class="c1"># args.learning_rate - default is 5e-5, our notebook had 2e-5</span>
                  <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-8</span> <span class="c1"># args.adam_epsilon  - default is 1e-8.</span>
                <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">get_linear_schedule_with_warmup</span>

<span class="c1"># Number of training epochs. The BERT authors recommend between 2 and 4. </span>
<span class="c1"># We chose to run for 4, but we&#39;ll see later that this may be over-fitting the</span>
<span class="c1"># training data.</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">4</span>

<span class="c1"># Total number of training steps is [number of batches] x [number of epochs]. </span>
<span class="c1"># (Note that this is not the same as the number of training samples).</span>
<span class="n">total_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span> <span class="o">*</span> <span class="n">epochs</span>

<span class="c1"># Create the learning rate scheduler.</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">get_linear_schedule_with_warmup</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> 
                                            <span class="n">num_warmup_steps</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="c1"># Default value in run_glue.py</span>
                                            <span class="n">num_training_steps</span> <span class="o">=</span> <span class="n">total_steps</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="4.3.-Training-Loop">4.3. Training Loop<a class="anchor-link" href="#4.3.-Training-Loop"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Below is our training loop. There's a lot going on, but fundamentally for each pass in our loop we have a trianing phase and a validation phase.</p>
<blockquote><p><em>Thank you to <a href="https://ca.linkedin.com/in/stasbekman">Stas Bekman</a> for contributing the insights and code for using validation loss to detect over-fitting!</em>
<strong>Training:</strong></p>
<ul>
<li>Unpack our data inputs and labels</li>
<li>Load data onto the GPU for acceleration</li>
<li>Clear out the gradients calculated in the previous pass. <ul>
<li>In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out.</li>
</ul>
</li>
<li>Forward pass (feed input data through the network)</li>
<li>Backward pass (backpropagation)</li>
<li>Tell the network to update parameters with optimizer.step()</li>
<li>Track variables for monitoring progress</li>
</ul>
</blockquote>
<p><strong>Evalution:</strong></p>
<ul>
<li>Unpack our data inputs and labels</li>
<li>Load data onto the GPU for acceleration</li>
<li>Forward pass (feed input data through the network)</li>
<li>Compute loss on our validation data and track variables for monitoring progress</li>
</ul>
<p>Pytorch hides all of the detailed calculations from us, but we've commented the code to point out which of the above steps are happening on each line.</p>
<blockquote><p><em>PyTorch also has some <a href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py">beginner tutorials</a> which you may also find helpful.</em></p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Define a helper function for calculating accuracy.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># Function to calculate the accuracy of our predictions vs labels</span>
<span class="k">def</span> <span class="nf">flat_accuracy</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">pred_flat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">labels_flat</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pred_flat</span> <span class="o">==</span> <span class="n">labels_flat</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels_flat</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Helper function for formatting elapsed times as <code>hh:mm:ss</code></p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">datetime</span>

<span class="k">def</span> <span class="nf">format_time</span><span class="p">(</span><span class="n">elapsed</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Takes a time in seconds and returns a string hh:mm:ss</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="c1"># Round to the nearest second.</span>
    <span class="n">elapsed_rounded</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">((</span><span class="n">elapsed</span><span class="p">)))</span>
    
    <span class="c1"># Format as hh:mm:ss</span>
    <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="n">elapsed_rounded</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We're ready to kick off the training!</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># This training code is based on the `run_glue.py` script here:</span>
<span class="c1"># https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128</span>

<span class="c1"># Set the seed value all over the place to make this reproducible.</span>
<span class="n">seed_val</span> <span class="o">=</span> <span class="mi">42</span>

<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed_val</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed_val</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed_val</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed_val</span><span class="p">)</span>

<span class="c1"># We&#39;ll store a number of quantities such as training and validation loss, </span>
<span class="c1"># validation accuracy, and timings.</span>
<span class="n">training_stats</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Measure the total training time for the whole run.</span>
<span class="n">total_t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="c1"># For each epoch...</span>
<span class="k">for</span> <span class="n">epoch_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
    
    <span class="c1"># ========================================</span>
    <span class="c1">#               Training</span>
    <span class="c1"># ========================================</span>
    
    <span class="c1"># Perform one full pass over the training set.</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;======== Epoch </span><span class="si">{:}</span><span class="s1"> / </span><span class="si">{:}</span><span class="s1"> ========&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch_i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training...&#39;</span><span class="p">)</span>

    <span class="c1"># Measure how long the training epoch takes.</span>
    <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="c1"># Reset the total loss for this epoch.</span>
    <span class="n">total_train_loss</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Put the model into training mode. Don&#39;t be mislead--the call to </span>
    <span class="c1"># `train` just changes the *mode*, it doesn&#39;t *perform* the training.</span>
    <span class="c1"># `dropout` and `batchnorm` layers behave differently during training</span>
    <span class="c1"># vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

    <span class="c1"># For each batch of training data...</span>
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>

        <span class="c1"># Progress update every 40 batches.</span>
        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">40</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Calculate elapsed time in minutes.</span>
            <span class="n">elapsed</span> <span class="o">=</span> <span class="n">format_time</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span>
            
            <span class="c1"># Report progress.</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;  Batch </span><span class="si">{:&gt;5,}</span><span class="s1">  of  </span><span class="si">{:&gt;5,}</span><span class="s1">.    Elapsed: </span><span class="si">{:}</span><span class="s1">.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">),</span> <span class="n">elapsed</span><span class="p">))</span>

        <span class="c1"># Unpack this training batch from our dataloader. </span>
        <span class="c1">#</span>
        <span class="c1"># As we unpack the batch, we&#39;ll also copy each tensor to the GPU using the </span>
        <span class="c1"># `to` method.</span>
        <span class="c1">#</span>
        <span class="c1"># `batch` contains three pytorch tensors:</span>
        <span class="c1">#   [0]: input ids </span>
        <span class="c1">#   [1]: attention masks</span>
        <span class="c1">#   [2]: labels </span>
        <span class="n">b_input_ids</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">b_input_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">b_labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Always clear any previously calculated gradients before performing a</span>
        <span class="c1"># backward pass. PyTorch doesn&#39;t do this automatically because </span>
        <span class="c1"># accumulating the gradients is &quot;convenient while training RNNs&quot;. </span>
        <span class="c1"># (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>        

        <span class="c1"># Perform a forward pass (evaluate the model on this training batch).</span>
        <span class="c1"># The documentation for this `model` function is here: </span>
        <span class="c1"># https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification</span>
        <span class="c1"># It returns different numbers of parameters depending on what arguments</span>
        <span class="c1"># arge given and what flags are set. For our useage here, it returns</span>
        <span class="c1"># the loss (because we provided labels) and the &quot;logits&quot;--the model</span>
        <span class="c1"># outputs prior to activation.</span>
        <span class="n">loss</span><span class="p">,</span> <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">b_input_ids</span><span class="p">,</span> 
                             <span class="n">token_type_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                             <span class="n">attention_mask</span><span class="o">=</span><span class="n">b_input_mask</span><span class="p">,</span> 
                             <span class="n">labels</span><span class="o">=</span><span class="n">b_labels</span><span class="p">)</span>

        <span class="c1"># Accumulate the training loss over all of the batches so that we can</span>
        <span class="c1"># calculate the average loss at the end. `loss` is a Tensor containing a</span>
        <span class="c1"># single value; the `.item()` function just returns the Python value </span>
        <span class="c1"># from the tensor.</span>
        <span class="n">total_train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># Perform a backward pass to calculate the gradients.</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Clip the norm of the gradients to 1.0.</span>
        <span class="c1"># This is to help prevent the &quot;exploding gradients&quot; problem.</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="mf">1.0</span><span class="p">)</span>

        <span class="c1"># Update parameters and take a step using the computed gradient.</span>
        <span class="c1"># The optimizer dictates the &quot;update rule&quot;--how the parameters are</span>
        <span class="c1"># modified based on their gradients, the learning rate, etc.</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># Update the learning rate.</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="c1"># Calculate the average loss over all of the batches.</span>
    <span class="n">avg_train_loss</span> <span class="o">=</span> <span class="n">total_train_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span>            
    
    <span class="c1"># Measure how long this epoch took.</span>
    <span class="n">training_time</span> <span class="o">=</span> <span class="n">format_time</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  Average training loss: </span><span class="si">{0:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">avg_train_loss</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  Training epcoh took: </span><span class="si">{:}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">training_time</span><span class="p">))</span>
        
    <span class="c1"># ========================================</span>
    <span class="c1">#               Validation</span>
    <span class="c1"># ========================================</span>
    <span class="c1"># After the completion of each training epoch, measure our performance on</span>
    <span class="c1"># our validation set.</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Running Validation...&quot;</span><span class="p">)</span>

    <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="c1"># Put the model in evaluation mode--the dropout layers behave differently</span>
    <span class="c1"># during evaluation.</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

    <span class="c1"># Tracking variables </span>
    <span class="n">total_eval_accuracy</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_eval_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">nb_eval_steps</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="c1"># Evaluate data for one epoch</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">validation_dataloader</span><span class="p">:</span>
        
        <span class="c1"># Unpack this training batch from our dataloader. </span>
        <span class="c1">#</span>
        <span class="c1"># As we unpack the batch, we&#39;ll also copy each tensor to the GPU using </span>
        <span class="c1"># the `to` method.</span>
        <span class="c1">#</span>
        <span class="c1"># `batch` contains three pytorch tensors:</span>
        <span class="c1">#   [0]: input ids </span>
        <span class="c1">#   [1]: attention masks</span>
        <span class="c1">#   [2]: labels </span>
        <span class="n">b_input_ids</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">b_input_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">b_labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="c1"># Tell pytorch not to bother with constructing the compute graph during</span>
        <span class="c1"># the forward pass, since this is only needed for backprop (training).</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>        

            <span class="c1"># Forward pass, calculate logit predictions.</span>
            <span class="c1"># token_type_ids is the same as the &quot;segment ids&quot;, which </span>
            <span class="c1"># differentiates sentence 1 and 2 in 2-sentence tasks.</span>
            <span class="c1"># The documentation for this `model` function is here: </span>
            <span class="c1"># https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification</span>
            <span class="c1"># Get the &quot;logits&quot; output by the model. The &quot;logits&quot; are the output</span>
            <span class="c1"># values prior to applying an activation function like the softmax.</span>
            <span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">b_input_ids</span><span class="p">,</span> 
                                   <span class="n">token_type_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                                   <span class="n">attention_mask</span><span class="o">=</span><span class="n">b_input_mask</span><span class="p">,</span>
                                   <span class="n">labels</span><span class="o">=</span><span class="n">b_labels</span><span class="p">)</span>
            
        <span class="c1"># Accumulate the validation loss.</span>
        <span class="n">total_eval_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># Move logits and labels to CPU</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">label_ids</span> <span class="o">=</span> <span class="n">b_labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="c1"># Calculate the accuracy for this batch of test sentences, and</span>
        <span class="c1"># accumulate it over all batches.</span>
        <span class="n">total_eval_accuracy</span> <span class="o">+=</span> <span class="n">flat_accuracy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label_ids</span><span class="p">)</span>
        

    <span class="c1"># Report the final accuracy for this validation run.</span>
    <span class="n">avg_val_accuracy</span> <span class="o">=</span> <span class="n">total_eval_accuracy</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">validation_dataloader</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  Accuracy: </span><span class="si">{0:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">avg_val_accuracy</span><span class="p">))</span>

    <span class="c1"># Calculate the average loss over all of the batches.</span>
    <span class="n">avg_val_loss</span> <span class="o">=</span> <span class="n">total_eval_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">validation_dataloader</span><span class="p">)</span>
    
    <span class="c1"># Measure how long the validation run took.</span>
    <span class="n">validation_time</span> <span class="o">=</span> <span class="n">format_time</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  Validation Loss: </span><span class="si">{0:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">avg_val_loss</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  Validation took: </span><span class="si">{:}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">validation_time</span><span class="p">))</span>

    <span class="c1"># Record all statistics from this epoch.</span>
    <span class="n">training_stats</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s1">&#39;epoch&#39;</span><span class="p">:</span> <span class="n">epoch_i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span>
            <span class="s1">&#39;Training Loss&#39;</span><span class="p">:</span> <span class="n">avg_train_loss</span><span class="p">,</span>
            <span class="s1">&#39;Valid. Loss&#39;</span><span class="p">:</span> <span class="n">avg_val_loss</span><span class="p">,</span>
            <span class="s1">&#39;Valid. Accur.&#39;</span><span class="p">:</span> <span class="n">avg_val_accuracy</span><span class="p">,</span>
            <span class="s1">&#39;Training Time&#39;</span><span class="p">:</span> <span class="n">training_time</span><span class="p">,</span>
            <span class="s1">&#39;Validation Time&#39;</span><span class="p">:</span> <span class="n">validation_time</span>
        <span class="p">}</span>
    <span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training complete!&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Total training took </span><span class="si">{:}</span><span class="s2"> (h:mm:ss)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">format_time</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span><span class="o">-</span><span class="n">total_t0</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>
======== Epoch 1 / 4 ========
Training...
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha)
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>  Batch    40  of    241.    Elapsed: 0:00:25.
  Batch    80  of    241.    Elapsed: 0:00:50.
  Batch   120  of    241.    Elapsed: 0:01:15.
  Batch   160  of    241.    Elapsed: 0:01:40.
  Batch   200  of    241.    Elapsed: 0:02:05.
  Batch   240  of    241.    Elapsed: 0:02:30.

  Average training loss: 0.49
  Training epcoh took: 0:02:30

Running Validation...
  Accuracy: 0.80
  Validation Loss: 0.42
  Validation took: 0:00:05

======== Epoch 2 / 4 ========
Training...
  Batch    40  of    241.    Elapsed: 0:00:25.
  Batch    80  of    241.    Elapsed: 0:00:50.
  Batch   120  of    241.    Elapsed: 0:01:15.
  Batch   160  of    241.    Elapsed: 0:01:40.
  Batch   200  of    241.    Elapsed: 0:02:05.
  Batch   240  of    241.    Elapsed: 0:02:30.

  Average training loss: 0.29
  Training epcoh took: 0:02:30

Running Validation...
  Accuracy: 0.81
  Validation Loss: 0.50
  Validation took: 0:00:05

======== Epoch 3 / 4 ========
Training...
  Batch    40  of    241.    Elapsed: 0:00:25.
  Batch    80  of    241.    Elapsed: 0:00:50.
  Batch   120  of    241.    Elapsed: 0:01:15.
  Batch   160  of    241.    Elapsed: 0:01:40.
  Batch   200  of    241.    Elapsed: 0:02:05.
  Batch   240  of    241.    Elapsed: 0:02:30.

  Average training loss: 0.18
  Training epcoh took: 0:02:30

Running Validation...
  Accuracy: 0.81
  Validation Loss: 0.57
  Validation took: 0:00:05

======== Epoch 4 / 4 ========
Training...
  Batch    40  of    241.    Elapsed: 0:00:25.
  Batch    80  of    241.    Elapsed: 0:00:50.
  Batch   120  of    241.    Elapsed: 0:01:15.
  Batch   160  of    241.    Elapsed: 0:01:40.
  Batch   200  of    241.    Elapsed: 0:02:05.
  Batch   240  of    241.    Elapsed: 0:02:30.

  Average training loss: 0.13
  Training epcoh took: 0:02:30

Running Validation...
  Accuracy: 0.83
  Validation Loss: 0.62
  Validation took: 0:00:05

Training complete!
Total training took 0:10:22 (h:mm:ss)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's view the summary of the training process.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Display floats with two decimal places.</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;precision&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Create a DataFrame from our training statistics.</span>
<span class="n">df_stats</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">training_stats</span><span class="p">)</span>

<span class="c1"># Use the &#39;epoch&#39; as the row index.</span>
<span class="n">df_stats</span> <span class="o">=</span> <span class="n">df_stats</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;epoch&#39;</span><span class="p">)</span>

<span class="c1"># A hack to force the column headers to wrap.</span>
<span class="c1">#df = df.style.set_table_styles([dict(selector=&quot;th&quot;,props=[(&#39;max-width&#39;, &#39;70px&#39;)])])</span>

<span class="c1"># Display the table.</span>
<span class="n">df_stats</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Training Loss</th>
      <th>Valid. Loss</th>
      <th>Valid. Accur.</th>
      <th>Training Time</th>
      <th>Validation Time</th>
    </tr>
    <tr>
      <th>epoch</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.49</td>
      <td>0.42</td>
      <td>0.80</td>
      <td>0:02:30</td>
      <td>0:00:05</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.29</td>
      <td>0.50</td>
      <td>0.81</td>
      <td>0:02:30</td>
      <td>0:00:05</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.18</td>
      <td>0.57</td>
      <td>0.81</td>
      <td>0:02:30</td>
      <td>0:00:05</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.13</td>
      <td>0.62</td>
      <td>0.83</td>
      <td>0:02:30</td>
      <td>0:00:05</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice that, while the the training loss is going down with each epoch, the validation loss is increasing! This suggests that we are training our model too long, and it's over-fitting on the training data.</p>
<p>(For reference, we are using 7,695 training samples and 856 validation samples).</p>
<p>Validation Loss is a more precise measure than accuracy, because with accuracy we don't care about the exact output value, but just which side of a threshold it falls on.</p>
<p>If we are predicting the correct answer, but with less confidence, then validation loss will catch this, while accuracy will not.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span> <span class="n">matplotlib</span> <span class="n">inline</span>

<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="c1"># Use plot styling from seaborn.</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s1">&#39;darkgrid&#39;</span><span class="p">)</span>

<span class="c1"># Increase the plot size and font size.</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s2">&quot;figure.figsize&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>

<span class="c1"># Plot the learning curve.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_stats</span><span class="p">[</span><span class="s1">&#39;Training Loss&#39;</span><span class="p">],</span> <span class="s1">&#39;b-o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_stats</span><span class="p">[</span><span class="s1">&#39;Valid. Loss&#39;</span><span class="p">],</span> <span class="s1">&#39;g-o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Validation&quot;</span><span class="p">)</span>

<span class="c1"># Label the plot.</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Training &amp; Validation Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Epoch&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.
  import pandas.util.testing as tm
</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Performance-On-Test-Set">Performance On Test Set<a class="anchor-link" href="#Performance-On-Test-Set"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we'll load the holdout dataset and prepare inputs just as we did with the training set. Then we'll evaluate predictions using <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html">Matthew's correlation coefficient</a> because this is the metric used by the wider NLP community to evaluate performance on CoLA. With this metric, +1 is the best score, and -1 is the worst score. This way, we can see how well we perform against the state of the art models for this specific task.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="5.1.-Data-Preparation">5.1. Data Preparation<a class="anchor-link" href="#5.1.-Data-Preparation"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We'll need to apply all of the same steps that we did for the training data to prepare our test data set.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Load the dataset into a pandas dataframe.</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;./cola_public/raw/out_of_domain_dev.tsv&quot;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sentence_source&#39;</span><span class="p">,</span> <span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="s1">&#39;label_notes&#39;</span><span class="p">,</span> <span class="s1">&#39;sentence&#39;</span><span class="p">])</span>

<span class="c1"># Report the number of sentences.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of test sentences: </span><span class="si">{:,}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

<span class="c1"># Create sentence and label lists</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sentence</span><span class="o">.</span><span class="n">values</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">values</span>

<span class="c1"># Tokenize all of the sentences and map the tokens to thier word IDs.</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">attention_masks</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># For every sentence...</span>
<span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">:</span>
    <span class="c1"># `encode_plus` will:</span>
    <span class="c1">#   (1) Tokenize the sentence.</span>
    <span class="c1">#   (2) Prepend the `[CLS]` token to the start.</span>
    <span class="c1">#   (3) Append the `[SEP]` token to the end.</span>
    <span class="c1">#   (4) Map tokens to their IDs.</span>
    <span class="c1">#   (5) Pad or truncate the sentence to `max_length`</span>
    <span class="c1">#   (6) Create attention masks for [PAD] tokens.</span>
    <span class="n">encoded_dict</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_plus</span><span class="p">(</span>
                        <span class="n">sent</span><span class="p">,</span>                      <span class="c1"># Sentence to encode.</span>
                        <span class="n">add_special_tokens</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="c1"># Add &#39;[CLS]&#39; and &#39;[SEP]&#39;</span>
                        <span class="n">max_length</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>           <span class="c1"># Pad &amp; truncate all sentences.</span>
                        <span class="n">pad_to_max_length</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                        <span class="n">return_attention_mask</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>   <span class="c1"># Construct attn. masks.</span>
                        <span class="n">return_tensors</span> <span class="o">=</span> <span class="s1">&#39;pt&#39;</span><span class="p">,</span>     <span class="c1"># Return pytorch tensors.</span>
                   <span class="p">)</span>
    
    <span class="c1"># Add the encoded sentence to the list.    </span>
    <span class="n">input_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">encoded_dict</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">])</span>
    
    <span class="c1"># And its attention mask (simply differentiates padding from non-padding).</span>
    <span class="n">attention_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">encoded_dict</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">])</span>

<span class="c1"># Convert the lists into tensors.</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">attention_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">attention_masks</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

<span class="c1"># Set the batch size.  </span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>  

<span class="c1"># Create the DataLoader.</span>
<span class="n">prediction_data</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_masks</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">prediction_sampler</span> <span class="o">=</span> <span class="n">SequentialSampler</span><span class="p">(</span><span class="n">prediction_data</span><span class="p">)</span>
<span class="n">prediction_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">prediction_data</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">prediction_sampler</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Number of test sentences: 516

</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="5.2.-Evaluate-on-Test-Set">5.2. Evaluate on Test Set<a class="anchor-link" href="#5.2.-Evaluate-on-Test-Set"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With the test set prepared, we can apply our fine-tuned model to generate predictions on the test set.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Predicting labels for </span><span class="si">{:,}</span><span class="s1"> test sentences...&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)))</span>

<span class="c1"># Put model in evaluation mode</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># Tracking variables </span>
<span class="n">predictions</span> <span class="p">,</span> <span class="n">true_labels</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="c1"># Predict </span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">prediction_dataloader</span><span class="p">:</span>
  <span class="c1"># Add batch to GPU</span>
  <span class="n">batch</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">)</span>
  
  <span class="c1"># Unpack the inputs from our dataloader</span>
  <span class="n">b_input_ids</span><span class="p">,</span> <span class="n">b_input_mask</span><span class="p">,</span> <span class="n">b_labels</span> <span class="o">=</span> <span class="n">batch</span>
  
  <span class="c1"># Telling the model not to compute or store gradients, saving memory and </span>
  <span class="c1"># speeding up prediction</span>
  <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
      <span class="c1"># Forward pass, calculate logit predictions</span>
      <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">b_input_ids</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                      <span class="n">attention_mask</span><span class="o">=</span><span class="n">b_input_mask</span><span class="p">)</span>

  <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

  <span class="c1"># Move logits and labels to CPU</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
  <span class="n">label_ids</span> <span class="o">=</span> <span class="n">b_labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
  
  <span class="c1"># Store predictions and true labels</span>
  <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
  <span class="n">true_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label_ids</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;    DONE.&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Predicting labels for 516 test sentences...
    DONE.
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Accuracy on the CoLA benchmark is measured using the "<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html">Matthews correlation coefficient</a>" (MCC).</p>
<p>We use MCC here because the classes are imbalanced:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Positive samples: </span><span class="si">%d</span><span class="s1"> of </span><span class="si">%d</span><span class="s1"> (</span><span class="si">%.2f%%</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="p">),</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="p">)</span> <span class="o">*</span> <span class="mf">100.0</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Positive samples: 354 of 516 (68.60%)
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">matthews_corrcoef</span>

<span class="n">matthews_set</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Evaluate each test batch using Matthew&#39;s correlation coefficient</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Calculating Matthews Corr. Coef. for each batch...&#39;</span><span class="p">)</span>

<span class="c1"># For each input batch...</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">true_labels</span><span class="p">)):</span>
  
  <span class="c1"># The predictions for this batch are a 2-column ndarray (one column for &quot;0&quot; </span>
  <span class="c1"># and one column for &quot;1&quot;). Pick the label with the highest value and turn this</span>
  <span class="c1"># in to a list of 0s and 1s.</span>
  <span class="n">pred_labels_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
  
  <span class="c1"># Calculate and store the coef for this batch.  </span>
  <span class="n">matthews</span> <span class="o">=</span> <span class="n">matthews_corrcoef</span><span class="p">(</span><span class="n">true_labels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">pred_labels_i</span><span class="p">)</span>                
  <span class="n">matthews_set</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">matthews</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Calculating Matthews Corr. Coef. for each batch...
</pre>
</div>
</div>

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:900: RuntimeWarning: invalid value encountered in double_scalars
  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The final score will be based on the entire test set, but let's take a look at the scores on the individual batches to get a sense of the variability in the metric between batches.</p>
<p>Each batch has 32 sentences in it, except the last batch which has only (516 % 32) = 4 test sentences in it.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">matthews_set</span><span class="p">))),</span> <span class="n">y</span><span class="o">=</span><span class="n">matthews_set</span><span class="p">,</span> <span class="n">ci</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;MCC Score per Batch&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;MCC Score (-1 to +1)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Batch #&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we'll combine the results for all of the batches and calculate our final MCC score.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">flat_predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># For each sample, pick the label (0 or 1) with the higher score.</span>
<span class="n">flat_predictions</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">flat_predictions</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="c1"># Combine the correct labels for each batch into a single list.</span>
<span class="n">flat_true_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">true_labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Calculate the MCC</span>
<span class="n">mcc</span> <span class="o">=</span> <span class="n">matthews_corrcoef</span><span class="p">(</span><span class="n">flat_true_labels</span><span class="p">,</span> <span class="n">flat_predictions</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Total MCC: </span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">mcc</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Total MCC: 0.535
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Cool! In about half an hour and without doing any hyperparameter tuning (adjusting the learning rate, epochs, batch size, ADAM properties, etc.) we are able to get a good score.</p>
<blockquote><p><em>Note:To maximize the score, we should remove the "validation set" (which we used to help determine how many epochs to train for) and train on the entire training set.</em>
The library documents the expected accuracy for this benchmark <a href="https://huggingface.co/transformers/examples.html#glue">here</a> as <code>49.23</code>.</p>
</blockquote>
<p>You can also look at the official leaderboard <a href="https://gluebenchmark.com/leaderboard/submission/zlssuBTm5XRs0aSKbFYGVIVdvbj1/-LhijX9VVmvJcvzKymxy">here</a>.</p>
<p>Note that (due to the small dataset size?) the accuracy can vary significantly between runs.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This post demonstrates that with a pre-trained BERT model you can quickly and effectively create a high quality model with minimal effort and training time using the pytorch interface, regardless of the specific NLP task you are interested in.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="A1.-Saving-&amp;-Loading-Fine-Tuned-Model">A1. Saving &amp; Loading Fine-Tuned Model<a class="anchor-link" href="#A1.-Saving-&amp;-Loading-Fine-Tuned-Model"> </a></h2><p>This first cell (taken from <code>run_glue.py</code> <a href="https://github.com/huggingface/transformers/blob/35ff345fc9df9e777b27903f11fa213e4052595b/examples/run_glue.py#L495">here</a>) writes the model and tokenizer out to disk.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()</span>

<span class="n">output_dir</span> <span class="o">=</span> <span class="s1">&#39;./model_save/&#39;</span>

<span class="c1"># Create output directory if needed</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">output_dir</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Saving model to </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">output_dir</span><span class="p">)</span>

<span class="c1"># Save a trained model, configuration and tokenizer using `save_pretrained()`.</span>
<span class="c1"># They can then be reloaded using `from_pretrained()`</span>
<span class="n">model_to_save</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">module</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;module&#39;</span><span class="p">)</span> <span class="k">else</span> <span class="n">model</span>  <span class="c1"># Take care of distributed/parallel training</span>
<span class="n">model_to_save</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>

<span class="c1"># Good practice: save your training arguments together with the trained model</span>
<span class="c1"># torch.save(args, os.path.join(output_dir, &#39;training_args.bin&#39;))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Saving model to ./model_save/
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;./model_save/vocab.txt&#39;,
 &#39;./model_save/special_tokens_map.json&#39;,
 &#39;./model_save/added_tokens.json&#39;)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's check out the file sizes, out of curiosity.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>ls -l --block-size<span class="o">=</span>K ./model_save/
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>total 427960K
-rw-r--r-- 1 root root      2K May  7 18:26 config.json
-rw-r--r-- 1 root root 427719K May  7 18:26 pytorch_model.bin
-rw-r--r-- 1 root root      1K May  7 18:26 special_tokens_map.json
-rw-r--r-- 1 root root      1K May  7 18:26 tokenizer_config.json
-rw-r--r-- 1 root root    227K May  7 18:26 vocab.txt
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The largest file is the model weights, at around 418 megabytes.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>ls -l --block-size<span class="o">=</span>M ./model_save/pytorch_model.bin
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>-rw-r--r-- 1 root root 418M May  7 18:26 ./model_save/pytorch_model.bin
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To save your model across Colab Notebook sessions, download it to your local machine, or ideally copy it to your Google Drive.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">drive</span>
<span class="n">drive</span><span class="o">.</span><span class="n">mount</span><span class="p">(</span><span class="s1">&#39;/content/drive&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&amp;response_type=code&amp;scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly

Enter your authorization code:
··········
Mounted at /content/drive
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>cp -r ./model_save/ <span class="s2">&quot;/content/drive/My Drive/pickles&quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The following functions will load the model back from disk.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">model_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer_class</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">output_dir</span><span class="p">)</span>

<span class="c1"># Copy the model to the GPU.</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="A.2.-Weight-Decay">A.2. Weight Decay<a class="anchor-link" href="#A.2.-Weight-Decay"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The huggingface example includes the following code block for enabling weight decay, but the default decay rate is "0.0", so I moved this to the appendix.</p>
<p>This block essentially tells the optimizer to not apply weight decay to the bias terms (e.g., $ b $ in the equation $ y = Wx + b $ ). Weight decay is a form of regularization--after calculating the gradients, we multiply them by, e.g., 0.99.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L102</span>

<span class="c1"># Don&#39;t apply weight decay to any parameters whose names include these tokens.</span>
<span class="c1"># (Here, the BERT doesn&#39;t have `gamma` or `beta` parameters, only `bias` terms)</span>
<span class="n">no_decay</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;bias&#39;</span><span class="p">,</span> <span class="s1">&#39;LayerNorm.weight&#39;</span><span class="p">]</span>

<span class="c1"># Separate the `weight` parameters from the `bias` parameters. </span>
<span class="c1"># - For the `weight` parameters, this specifies a &#39;weight_decay_rate&#39; of 0.01. </span>
<span class="c1"># - For the `bias` parameters, the &#39;weight_decay_rate&#39; is 0.0. </span>
<span class="n">optimizer_grouped_parameters</span> <span class="o">=</span> <span class="p">[</span>
    <span class="c1"># Filter for all parameters which *don&#39;t* include &#39;bias&#39;, &#39;gamma&#39;, &#39;beta&#39;.</span>
    <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">param_optimizer</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">(</span><span class="n">nd</span> <span class="ow">in</span> <span class="n">n</span> <span class="k">for</span> <span class="n">nd</span> <span class="ow">in</span> <span class="n">no_decay</span><span class="p">)],</span>
     <span class="s1">&#39;weight_decay_rate&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">},</span>
    
    <span class="c1"># Filter for parameters which *do* include those.</span>
    <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">param_optimizer</span> <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">nd</span> <span class="ow">in</span> <span class="n">n</span> <span class="k">for</span> <span class="n">nd</span> <span class="ow">in</span> <span class="n">no_decay</span><span class="p">)],</span>
     <span class="s1">&#39;weight_decay_rate&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">}</span>
<span class="p">]</span>

<span class="c1"># Note - `optimizer_grouped_parameters` only includes the parameter values, not </span>
<span class="c1"># the names.</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

</div>

<script type="application/vnd.jupyter.widget-state+json">
{"11a9b77b19f945dc82dd9f009dae040a": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "1d8d980718d14e06aef24aa8bdb78508": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "1f0dc332c051405faced0a9199615119": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "1ff432472b364a10bd2f33a79a51e1f1": {"model_module": "@jupyter-widgets/controls", "model_name": "IntProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "IntProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "Downloading: 100%", "description_tooltip": null, "layout": "IPY_MODEL_20aab88cae494faa880ffcab97f18d5c", "max": 433, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_5ef9effd20eb4ef78274776fd7cee7fe", "value": 433}}, "20aab88cae494faa880ffcab97f18d5c": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "251b35d145254da1aa8b9cab5f5d1f4f": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "321a9382437a41cdabb1705a2a63cea8": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "349628b1aba841228f3f81ba425cf400": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "3988f885fdf74f64b798b51140fec8a0": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_f9a685737dd241778db01ddbfd8e2aa7", "placeholder": "\u200b", "style": "IPY_MODEL_9bd5e7d2611847db84b475aad5a3a720", "value": " 433/433 [00:02&lt;00:00, 153B/s]"}}, "49e8d2b98491499597bd224fdd70b65a": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_349628b1aba841228f3f81ba425cf400", "placeholder": "\u200b", "style": "IPY_MODEL_321a9382437a41cdabb1705a2a63cea8", "value": " 440M/440M [01:02&lt;00:00, 7.08MB/s]"}}, "594fb08b42a443ccb9177657f6dc78d7": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_b2fdbde4c8bc481fb9fc86ec21c72703", "placeholder": "\u200b", "style": "IPY_MODEL_8374816f613a4381b22c315404d99fee", "value": " 232k/232k [00:00&lt;00:00, 304kB/s]"}}, "5ef9effd20eb4ef78274776fd7cee7fe": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": "initial"}}, "6da65d408b304fc58e3c6efb35c54de5": {"model_module": "@jupyter-widgets/controls", "model_name": "IntProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "IntProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "Downloading: 100%", "description_tooltip": null, "layout": "IPY_MODEL_11a9b77b19f945dc82dd9f009dae040a", "max": 231508, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_717beba549e244128cd797380bdd91b9", "value": 231508}}, "717beba549e244128cd797380bdd91b9": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": "initial"}}, "781ac31eb2bb459ebb63703681a59bf0": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "8374816f613a4381b22c315404d99fee": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "9bd5e7d2611847db84b475aad5a3a720": {"model_module": "@jupyter-widgets/controls", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "a9cd931a75b74e579bd20dd2d8947ba9": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_e7f38ec7c1ff4167a1d1708fe8cc667d", "IPY_MODEL_49e8d2b98491499597bd224fdd70b65a"], "layout": "IPY_MODEL_1d8d980718d14e06aef24aa8bdb78508"}}, "b2fdbde4c8bc481fb9fc86ec21c72703": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c3a5d9b5397540238c4a02b90421b735": {"model_module": "@jupyter-widgets/controls", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": "initial"}}, "cc8306ba5a75487c89029f85eae3ce16": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_6da65d408b304fc58e3c6efb35c54de5", "IPY_MODEL_594fb08b42a443ccb9177657f6dc78d7"], "layout": "IPY_MODEL_781ac31eb2bb459ebb63703681a59bf0"}}, "e7f38ec7c1ff4167a1d1708fe8cc667d": {"model_module": "@jupyter-widgets/controls", "model_name": "IntProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "IntProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "Downloading: 100%", "description_tooltip": null, "layout": "IPY_MODEL_251b35d145254da1aa8b9cab5f5d1f4f", "max": 440473133, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_c3a5d9b5397540238c4a02b90421b735", "value": 440473133}}, "f4bfd726d318479ba17f26a334131d48": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_1ff432472b364a10bd2f33a79a51e1f1", "IPY_MODEL_3988f885fdf74f64b798b51140fec8a0"], "layout": "IPY_MODEL_1f0dc332c051405faced0a9199615119"}}, "f9a685737dd241778db01ddbfd8e2aa7": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}}
</script>



  </div><a class="u-url" href="/2022/01/27/sentence-classification.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Jupyter notebook database.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/recohut" target="_blank" title="recohut"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
