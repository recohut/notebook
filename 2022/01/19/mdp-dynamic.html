<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>MDP with Dynamic Programming in PyTorch | reconb</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="MDP with Dynamic Programming in PyTorch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Jupyter notebook database." />
<meta property="og:description" content="Jupyter notebook database." />
<link rel="canonical" href="https://nb.recohut.com/2022/01/19/mdp-dynamic.html" />
<meta property="og:url" content="https://nb.recohut.com/2022/01/19/mdp-dynamic.html" />
<meta property="og:site_name" content="reconb" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-01-19T00:00:00-06:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="MDP with Dynamic Programming in PyTorch" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-01-19T00:00:00-06:00","datePublished":"2022-01-19T00:00:00-06:00","description":"Jupyter notebook database.","headline":"MDP with Dynamic Programming in PyTorch","mainEntityOfPage":{"@type":"WebPage","@id":"https://nb.recohut.com/2022/01/19/mdp-dynamic.html"},"url":"https://nb.recohut.com/2022/01/19/mdp-dynamic.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://nb.recohut.com/feed.xml" title="reconb" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">reconb</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">MDP with Dynamic Programming in PyTorch</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-01-19T00:00:00-06:00" itemprop="datePublished">
        Jan 19, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      34 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/recohut/notebook/tree/master/_notebooks/2022-01-19-mdp-dynamic.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/recohut/notebook/master?filepath=_notebooks%2F2022-01-19-mdp-dynamic.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/recohut/notebook/blob/master/_notebooks/2022-01-19-mdp-dynamic.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Frecohut%2Fnotebook%2Fblob%2Fmaster%2F_notebooks%2F2022-01-19-mdp-dynamic.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-01-19-mdp-dynamic.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We will evaluate and solve MDPs using dynamic programming (DP). It is worth to note that the Model-based methods such as DP have some drawbacks. They require the environment to be fully known, including the transition matrix and reward matrix. They also have limited scalability, especially for environments with plenty of states.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Simple-MDP">Simple MDP<a class="anchor-link" href="#Simple-MDP"> </a></h2>
</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our MDP has 3 state (sleep, study and play games), and 2 actions (word, slack). The 3 <em> 2 </em> 3 transition matrix T(s, a, s') is as follows:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">T</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
                   <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]],</span>
                  <span class="p">[[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
                   <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]],</span>
                  <span class="p">[[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
                   <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]]]</span>
                 <span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This means, for example, that when taking the a1 slack action from state s0 study, there is a 60% chance that it will become s1 sleep (maybe getting tired ) and a 30% chance that it will become s2 play games (maybe wanting to relax ), and that there is a 10% chance of keeping on studying (maybe a true workaholic ).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">R</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">])</span>

<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="n">action</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We define the reward function as [+1, 0, -1] for three states, to compensate for the hard work. Obviously, the optimal policy, in this case, is choosing a0 work for each step (keep on studying – no pain no gain, right?). Also, we choose 0.5 as the discount factor, to begin with.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this oversimplified study-sleep-game process, the optimal policy, that is, the policy that achieves the highest total reward, is choosing action a0 in all steps. However, it won't be that straightforward in most cases. Also, the actions taken in individual steps won't necessarily be the same. They are usually dependent on states. So, we will have to solve an MDP by finding the optimal policy in real-world cases.</p>
<p>The value function of a policy measures how good it is for an agent to be in each state, given the policy being followed. The greater the value, the better the state.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We calculate the value, V, of the optimal policy using the matrix inversion method in the following function:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">cal_value_matrix_inversion</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">trans_matrix</span><span class="p">,</span> <span class="n">rewards</span><span class="p">):</span>
    <span class="n">inv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">inverse</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">rewards</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">trans_matrix</span><span class="p">)</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">inv</span><span class="p">,</span> <span class="n">rewards</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">V</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Above, we calculated the value, V, of the optimal policy using matrix inversion. According to the Bellman Equation, the relationship between the value at step t+1 and that at step t can be expressed as follows:</p>
<p>
$$V_{t+1} = R + \gamma*T*V_t$$
</p>
<p>When the value converges, which means $V_{t+1} = V_t$, we can derive the value, $V$, as follows:</p>
<p>
$$V = R + \gamma*T*V \\ V = (I-\gamma*T)^{-1}*R$$
</p>
<p>Here, $I$ is the identity matrix with 1s on the main diagonal.</p>
<p>One advantage of solving an MDP with matrix inversion is that you always get an exact answer. But the downside is its scalability. As we need to compute the inversion of an m * m matrix (where m is the number of possible states), the computation will become costly if there is a large number of states.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We feed all variables we have to the function, including the transition probabilities associated with action a0:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">trans_matrix</span> <span class="o">=</span> <span class="n">T</span><span class="p">[:,</span> <span class="n">action</span><span class="p">]</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">cal_value_matrix_inversion</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">trans_matrix</span><span class="p">,</span> <span class="n">R</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The value function under the optimal policy is:</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">V</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>The value function under the optimal policy is:
tensor([[ 1.6787],
        [ 0.6260],
        [-0.4820]])
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We decide to experiment with different values for the discount factor. Let's start with 0, which means we only care about the immediate reward:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">gamma</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">cal_value_matrix_inversion</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">trans_matrix</span><span class="p">,</span> <span class="n">R</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The value function under the optimal policy is:</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">V</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>The value function under the optimal policy is:
tensor([[ 1.],
        [ 0.],
        [-1.]])
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is consistent with the reward function as we only look at the reward received in the next move.</p>
<p>As the discount factor increases toward 1, future rewards are considered. Let's take a look at $\gamma$=0.99:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">cal_value_matrix_inversion</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">trans_matrix</span><span class="p">,</span> <span class="n">R</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The value function under the optimal policy is:</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">V</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>The value function under the optimal policy is:
tensor([[65.8293],
        [64.7194],
        [63.4876]])
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Performing-policy-evaluation">Performing policy evaluation<a class="anchor-link" href="#Performing-policy-evaluation"> </a></h2><p>Policy evaluation is an iterative algorithm. It starts with arbitrary policy values and then iteratively updates the values based on the Bellman expectation equation until they converge. In each iteration, the value of a policy, π, for a state, s, is updated as follows:</p>
<p>
$$\mathcal{V}_{\pi}(s) = \sum_{a \in \mathcal{A}} \pi(a | s) (\mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a {V}_{\pi}(s'))$$
</p>
<p>There are two ways to terminate an iterative updating process. One is by setting a fixed number of iterations, such as 1,000 and 10,000, which might be difficult to control sometimes. Another one involves specifying a threshold (usually 0.0001, 0.00001, or something similar) and terminating the process only if the values of all states change to an extent that is lower than the threshold specified.</p>
<p>Next, we will perform policy evaluation on the study-sleep-game process under the optimal policy and a random policy.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">T</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
                   <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]],</span>
                  <span class="p">[[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span>
                   <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]],</span>
                  <span class="p">[[</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>
                   <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]]]</span>
                 <span class="p">)</span>

<span class="n">R</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">])</span>

<span class="n">gamma</span> <span class="o">=</span> <span class="o">.</span><span class="mi">5</span>

<span class="n">threshold</span> <span class="o">=</span> <span class="mf">0.0001</span>

<span class="n">policy_optimal</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
                               <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>
                               <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Develop a policy evaluation function that takes in a policy, transition matrix, rewards, discount factor, and a threshold and computes the value function:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">policy_evaluation</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">trans_matrix</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform policy evaluation</span>
<span class="sd">    @param policy: policy matrix containing actions and their probability in each state</span>
<span class="sd">    @param trans_matrix: transformation matrix</span>
<span class="sd">    @param rewards: rewards for each state</span>
<span class="sd">    @param gamma: discount factor</span>
<span class="sd">    @param threshold: the evaluation will stop once values for all states are less than the threshold</span>
<span class="sd">    @return: values of the given policy for all possible states</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_state</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_state</span><span class="p">)</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">V_temp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_state</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">state</span><span class="p">,</span> <span class="n">actions</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">policy</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">action</span><span class="p">,</span> <span class="n">action_prob</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">actions</span><span class="p">):</span>
                <span class="n">V_temp</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+=</span> <span class="n">action_prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">R</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">trans_matrix</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">],</span> <span class="n">V</span><span class="p">))</span>
        <span class="n">max_delta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="n">V_temp</span><span class="p">))</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">V_temp</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">max_delta</span> <span class="o">&lt;=</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">V</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The policy evaluation function does the following tasks:</p>
<ul>
<li>Initializes the policy values as all zeros.</li>
<li>Updates the values based on the Bellman expectation equation.</li>
<li>Computes the maximal change of the values across all states.</li>
<li>If the maximal change is greater than the threshold, it keeps updating the values. Otherwise, it terminates the evaluation process and returns the latest values.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's plug in the optimal policy and all other variables:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">V</span> <span class="o">=</span> <span class="n">policy_evaluation</span><span class="p">(</span><span class="n">policy_optimal</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The value function under the optimal policy is:</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">V</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>The value function under the optimal policy is:
tensor([ 1.6786,  0.6260, -0.4821])
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is almost the same as what we got using matrix inversion.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We now experiment with another policy, a random policy where actions are picked with the same probabilities:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Plug in the random policy and all other variables:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">policy_random</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
                              <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span>
                              <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]])</span>

<span class="n">V</span> <span class="o">=</span> <span class="n">policy_evaluation</span><span class="p">(</span><span class="n">policy_random</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The value function under the random policy is:</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">V</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>The value function under the random policy is:
tensor([ 1.2348,  0.2691, -0.9013])
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We have just seen how effective it is to compute the value of a policy using policy evaluation. It is a simple convergent iterative approach, in the dynamic programming family, or to be more specific, approximate dynamic programming. It starts with random guesses as to the values and then iteratively updates them according to the Bellman expectation equation until they converge.</p>
<p>Since policy evaluation uses iterative approximation, its result might not be exactly the same as the result of the matrix inversion method, which uses exact computation. In fact, we don't really need the value function to be that precise. Also, it can solve the curses of dimensionality problem, which can result in scaling up the computation to thousands of millions of states. Therefore, we usually prefer policy evaluation over the other.</p>
<p>One more thing to remember is that policy evaluation is used to predict how great a we will get from a given policy; it is not used for control problems.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To take a closer look, we also plot the policy values over the whole evaluation process.</p>
<p>We first need to record the value for each iteration in the policy_evaluation function:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">policy_evaluation_history</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">trans_matrix</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
    <span class="n">n_state</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_state</span><span class="p">)</span>
    <span class="n">V_his</span> <span class="o">=</span> <span class="p">[</span><span class="n">V</span><span class="p">]</span>
    <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">V_temp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_state</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">for</span> <span class="n">state</span><span class="p">,</span> <span class="n">actions</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">policy</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">action</span><span class="p">,</span> <span class="n">action_prob</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">actions</span><span class="p">):</span>
                <span class="n">V_temp</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+=</span> <span class="n">action_prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">R</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">trans_matrix</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">],</span> <span class="n">V</span><span class="p">))</span>
        <span class="n">max_delta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="n">V_temp</span><span class="p">))</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">V_temp</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">V_his</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">max_delta</span> <span class="o">&lt;=</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">V</span><span class="p">,</span> <span class="n">V_his</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we feed the policy_evaluation_history function with the optimal policy, a discount factor of 0.5, and other variables:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">V</span><span class="p">,</span> <span class="n">V_history</span> <span class="o">=</span> <span class="n">policy_evaluation_history</span><span class="p">(</span><span class="n">policy_optimal</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We then plot the resulting history of values using the following lines of code:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">s0</span><span class="p">,</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">V_history</span><span class="p">])</span>
<span class="n">s1</span><span class="p">,</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">V_history</span><span class="p">])</span>
<span class="n">s2</span><span class="p">,</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">v</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">V_history</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Optimal policy with gamma = </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">gamma</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Policy values&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="n">s0</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">],</span>
           <span class="p">[</span><span class="s2">&quot;State s0&quot;</span><span class="p">,</span>
            <span class="s2">&quot;State s1&quot;</span><span class="p">,</span>
            <span class="s2">&quot;State s2&quot;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3wU9bn48c+zmzsJCSEgkARBLiIKAk0REAIU7VHrrVrrtWqrP6qttT3W47H11EuP9gctnmOp/lqpVrFab62tV7RWLgFERVBELgJyS7iFayDkvvv8/pgJbEKyWchuZpM8b93XznznuzNPNmGeme/MfL+iqhhjjDHN8XkdgDHGmPhmicIYY0xYliiMMcaEZYnCGGNMWJYojDHGhGWJwhhjTFiWKMxxEZG+IlIuIv4YrPt+EXk2Buu9UUQWhcyXi8gp0d5OBHFMEJEvwizvJyIqIgltGZcxLbFE0cG5O8mVIlIhIjtF5PciknUcn98sIufUz6vqVlVNV9VAbCKOPTf+jR5sd6Gqnlo/3/i7NcdHRJJF5E8ictD9274jTN0bRSTgHiTUvya1YbjtmiWKDkxEfgpMB/4DyATGACcD74pIkpexGRMF9wODcP6mJwN3ich5YeovcQ8S6l/z2yDGDsESRQclIl2BB4AfqerbqlqrqpuBbwP9gOvceveLyF9F5EUROSQiy0XkTHfZn4G+wOvuEdhdjZtHRGS+iDwoIu+7dV4Xke4i8px7pLdURPqFxPVbESl2ly0TkQkR/jyTRKRERH4uInvco/FrQ5ZnisgzIrJbRLaIyH+JSJN/3278A93pVBF52P1MmYgscsveFJEfNfrcZyLyzSbWN9tNyohIrrv+H7rzA0Rkn4j46n+G5r7bkFVeKyJb3Z/znjDfSXf3+67/nh9s1MTW7Hft/t5fFpFn3d/7ShEZLCI/E5FS93NfD6nfJr/n43QD8N+qul9V1wB/BG6MwXY6PUsUHdc4IAV4JbRQVcuBt4BzQ4ovAV4GsoG/AP8QkURV/Q6wFbjIPQL7dTPbugr4DpALDACWAE+561sD3BdSdykwImRbL4tISoQ/Uy8gx93ODcAsEalvyvkdzlnTKcBE4HrguxGscwbwFZzvKxu4CwgCs3GTKYCbPHOBN5tYxwJgkjs9EdgIFIbML1TVYOgHWvhuxwOnAlOAe0XktGZifww4jPO93OC+QrX0XV8E/BnoBnwCvIOzT8gFfgk83mh9Mfk9i8jdInKguVczn+kG9AZWhBSvAE5vqr5rpJt814nIL8SuBUVOVe3VAV84O7mdzSybBrzrTt8PfBCyzAfsACa485uBc0KW9wMUSHDn5wP3hCx/GJgTMn8R8GmYOPcDZ4bE8mwz9SYBdUCXkLKXgF8AfqAGGBqy7PvAfHf6RmBRyDIFBro/a2X99httL8WNbZA7PwP4f83ENsCt6wP+4G67xF02G7gj5GcoCflcc99tXkjZR8BVTWzTD9QCp4aUPRj6c0bwXb/b6PdUDvjd+Qw3lqxo/56j9Ped78aXElJ2LrC5mfqnAP3d39EwYDXws1j+G+xILzuj6Lj2ADnNHDX1dpfXK66fUOfItwTocxzb2hUyXdnEfHr9jIjcKSJr3GaeAzhnATkRbme/qh4Omd/ixpkDJLrzoctyW1hfDk5C+LLxAlWtAl4ErnObsK7GOfo+hqp+iXNkPwKYALwBbHfPdibinHEcj50h0xWEfH8hegAJhPzuGk1H8l03/j3t0aM3KVS67+lh6sfq9xyJcve9a0hZV+BQU5VVdaOqblLVoKquxDlj+lYU4+nQLFF0XEuAauCy0EIRSQfOB94LKc4PWe4D8oDtblHUuhd226nvwrlO0k1Vs4AyQCJcRTcR6RIy39eNcw/O0fXJjZZta2F9e4AqnDOCpswGrsVpAqpQ1SVh1rUAZ8eTpKrb3PkbcJp1Pm3mM635bnfjnGHlhZSF/h5b+12fsOPdtnvdqby5V1OfUdX9OGe+Z4YUnwmsijBMbS4ecyxLFB2UqpbhXMz+nYicJyKJ7sXGl3DOGEKPjr8iIpe5Zx8/wUkwH7jLduGctkdDBs7ObTeQICL30vCIMBIPiEiSuzO6EHjZPQp+CXhIRDJE5GTgDiDsMxnu2dOfgP8RkT4i4heRsSKS7C5fgnO94mGaOZsIsQC4DShy5+e784u0+VuJT/i7ddf5CnC/iKSJyBCc6zL1ovFdn6jj2raq/kob3o3U4BVmO88A/yUi3dyf//8ATzdVUUTOF5GT3OkhOE2Wr57QT9cJWaLowNS5QPpznPb1g8CHOM0TU1S1OqTqq8CVOO3I3wEuU9Vad9n/xfnHeEBE7mxlSO8AbwPrcJqGqmjUXNKCnW6M24HngFtUda277Ec4zT8bgUU4F1D/FME67wRW4lx83YdzO3Hov4tncNq0W3oQcAHODrI+USwC0kLmm9La7/Y2nCadnTiJ7HmcJA+t/65bo622fR9Os+EWnO//N6r6NjR4MLSvW3cK8JmIHMa5meMV4FcxiKlDEvdCj+mkROR+YKCqXtdSXS+J83DUs6qa11LdKG/3emCqqo5vy+2eCBGZDvRS1cZ3PxnTKnZGYUwzRCQN+AEwy+tYmiIiQ0RkuDhGAzcBf/c6LtPxWKIwpgki8m84bey7cJqx4lEGThPKYZw7tB7G2t1NDFjTkzHGmLDsjMIYY0xYHe4R9pycHO3Xr5/XYRhjTLuybNmyParao6llHS5R9OvXj48//tjrMIwxpl0RkS3NLbOmJ2OMMWFZojDGGBOWJQpjjDFhdbhrFE2pra2lpKSEqqoqr0NpF1JSUsjLyyMxMdHrUIwxcaBTJIqSkhIyMjLo168fItZhZDiqyt69eykpKaF///5eh2OMiQOdoumpqqqK7t27W5KIgIjQvXt3O/syxhzRKRIFYEniONh3ZYwJ1Smanoxpz1SVQFCpq38FgtQFnbLaQJCAOx1Up25QIeh+Rt3pYEh5MKRuIKRcVQkGIVA/raAKytH1OPEcLXOm3SGVAerX1bie+8Ej5fXrOvIz0mj+aNdCR5fpMXUbL2v8mea+z+bqNv5Y2PW0MO5U+M8eh+PoZqlXZirXnNW35YrHyRJFG3nooYf4y1/+gt/vx+fz8fjjj3PWWWfxyCOPMHXqVNLS0sJ+PtJ6LVFVfvzjH/PWW2+RlpbG008/zahRo1q1zo6opi5IVV2A6togVbUBqusCVLnTR97rl4csq64NUFVXXy9AdV3Dz9Tv2GsD7o4+6MzXBZS6YKNlbt26oPXH1tlFepI/Ij/LEkV7tWTJEt544w2WL19OcnIye/bsoaamBnASwHXXXRdRooikXkvmzJnD+vXrWb9+PR9++CG33norH374YavWGa+CQeVQVR1llbUcqKzhQEUtByprKauspayi8XzDOjV1wRPebnKCj5REPymJ7nuCM52c6CctKYEEv5DgE/w+IcHvI8EnJPjcd3fZkXK/4Pf5SPQJ/vplPp9bz3dkPX6f4PMJPgGfOO8igl8En8+Zri/3i7jzuJ8J/ZwgAn6f8y447z4BqC9z1ue8H60Dzvoal7sfbTBf/3k4uhM8UiJHy4/WkdBFx3ymqR1p4zIJGfn02GWhn5Nml7W0jWOXd4xmXEsUbWDHjh3k5OSQnJwMQE6OM8b8zJkz2b59O5MnTyYnJ4d58+Zx6623snTpUiorK/nWt77FAw880GS9f/7zn9x3331UV1czYMAAnnrqKdLTG44aOXPmTP7whz+QkJDA0KFDeeGFF3j11Ve5/vrrERHGjBnDgQMH2LFjB717927z7+VEBIPKrkNVbNlbwZa9h9m2v5IDlbXHJoHKWg5W1hLuYDw10U9WWiKZqYlkpSXSP6cLWalJZKUlkp6ccGRnn5zod3f47vSRRNAwGSQn+khO8HWYnYMx9TpcN+MFBQXauK+nNWvWcNpppwHwwOurWL39YFS3ObRPV+676PRml5eXlzN+/HgqKio455xzuPLKK5k4cSJwtG+q+uSxb98+srOzCQQCTJkyhZkzZzJ8+PAG9fbs2cNll13GnDlz6NKlC9OnT6e6upp77723wXb79OnDpk2bSE5O5sCBA2RlZXHhhRdy9913M368M2DblClTmD59OgUFBQ0+G/qdtbW6QJBtByqPJIMteyvYvLeCrfuc6eqQo30R6Jri7OizUhPJTEtydvzuzj8ztT4RJIXUccqSE/ye/HzGxCMRWaaqBU0tszOKNpCens6yZctYuHAh8+bN48orr2TatGnceOONx9R96aWXmDVrFnV1dezYsYPVq1czfPjwBnU++OADVq9ezdlnnw1ATU0NY8eOPWZdw4cP59prr+XSSy/l0ksvjcnPdqKqagMU76twk8Bhtu5zk8Hew5Tsr2zQLp+c4OPk7mmc3L0LhYN6cHJOF07OTqNf9y70yUohwd9pbt4zxhOdLlGEO/KPJb/fz6RJk5g0aRLDhg1j9uzZxySKTZs2MWPGDJYuXUq3bt248cYbm3yeQVU599xzef7558Nu880336SoqIjXX3+dhx56iJUrV5Kbm0tx8dFx7ktKSsjNzY3Kz9iUYFBZveMg73+5h427DztJYW8FOw5WNbiZIyM5gZNz0jg9N5MLhvWmX/cu9O3uJIOeGcn4fNacY4xXOl2i8MIXX3yBz+dj0KBBAHz66aecfPLJAGRkZHDo0CFycnI4ePAgXbp0ITMzk127djFnzhwmTZp0TL0xY8bwwx/+kA0bNjBw4EAOHz7Mtm3bGDx48JFtBoNBiouLmTx5MuPHj+eFF16gvLyciy++mEcffZSrrrqKDz/8kMzMzKhfn9hTXs3C9btZ8MVuFm3Yw55y58J9TnoSfbPTGHNKd07u3sU9S3DOFLqlJVrbvjFxyhJFGygvL+dHP/oRBw4cICEhgYEDBzJr1iwApk6dynnnnUefPn2YN28eI0eOZMiQIeTn5x9pWmqq3tNPP83VV19NdXU1AA8++GCDRBEIBLjuuusoKytDVbn99tvJysriggsu4K233mLgwIGkpaXx1FNPtfrnq6kLsmzLforW76Zo3W5WudeAsrskMWFQDoWDejBhcA49M1JavS1jTNvrdBezTWRa+s627D1M0brdLFi3hyVf7uFwTYAEnzCqbzcKB+cwcXBPTu/T1ZqMjGkn7GK2abXy6jqWfLmXonW7KVq/my17KwDIz07l0pG5FA7uwbgB3clIsR5njeloPE0UIvIn4EKgVFXPaGL5JOBVYJNb9Iqq/rLtIuy8VOHzbWUUudcalm/dT21ASU30M3ZAd747rh8TT+1Jv+5pdm3BmA7O6zOKp4FHgWfC1Fmoqhe2TTidm+rRJ5l3Hqzke88sAuC03l353vj+TBzUg6/062bPHxjTyXiaKFS1SET6eRmDcVRU17G9rIqKmjoSfD6SE/w8fMWZTBiUQ8+udhHamM7M6zOKSIwVkRXAduBOVV3VuIKITAWmAvTtG/0OsTqymroAO8uqOVBZQ4LPR263VLLTklhblsTZp+V5HZ4xJg7Ee6JYDpysquUicgHwD2BQ40qqOguYBc5dT20bYvsUCAYpPVTNnvIaBOiZkUKPjGT8dpeSMaaRuO77QFUPqmq5O/0WkCgiOR6HdUIeeughTj/9dIYPH86IESOO9Nj6yCOPUFFR0eLnI63XkjVr1vDV0WNITU1lxm9mkJWayOCTMuiVmWJJwhjTpLhOFCLSS9xbakRkNE68e72N6viFdjP+2Wef8a9//Yv8/Hyg7RKFqnKwspb9gWT+/d5fcfMPbqd7ejL52WkkJcT1n4ExxmOe7iFE5HlgCXCqiJSIyE0icouI3OJW+RbwuXuNYiZwlbbDJwSb6ma8T58+DboPnzx5MgC33norBQUFnH766dx3330ATdb75z//ydixYxk1ahRXXHEF5eXlx2x35syZDB06lGHDhnPRZVewee9huuf04BtTJtCjqyUIY0xkOt+T2XPuhp0ro7vRXsPg/GnNLvaym/GFy1ZxOCAcPlTGoLxeZKcn4RPh/vvvJz09nTvvvLPJmO1pdmM6l3BPZtshZRuo72Z81qxZ9OjRgyuvvJKnn366ybovvfQSo0aNYuTIkaxatYrVq1cfUye0m/ERI0Ywe/ZstmzZcmR5MKjsOljFKYOHcsvNN7LgzVc4rU83cjKS8dnDccaY4xTvdz1FX5gj/1hqi27GVZUDFbXsPFhFbSDIMy+9wvoVH/HOnLcYO+Y3rFy5koSEzvcrN8a0jp1RtIEvvviC9evXH5lvqptxoMluxuuF1hszZgyLFy9mw4YNABw+fJhPVq5mQ2k5xfsrSPQL/bun4avYx7+dew7Tp0+nrKysyesYxhjTEju8bAOx7Ga8qqqa2mCQH/z055x7fh752WlkpSZSV1fXZDfjO3fupKCggIMHD+Lz+XjkkUdYvXo1Xbt29errMcbEuc53MbuDqAs4D8ztLa9BBHpmJJOTHr2R4Drid2aMaZ51M97BBILKpj2HqaoN0K1LEid1TSHRxo02xsSIJYp2RlUp2V9BVW2Ak7t3oWuqjf9gjIktOwxtZ3YerKKsspZemamWJIwxbcISRTuy73ANuw9Vk90liZz0JK/DMcZ0EpYo2onD1XVsO1BJenICfbJSbVQ5Y0ybsUTRDlTXBdiy9zBJfh99s9Ps6WpjTJuyRNFGTrSb8UAwyOY9FTzzx/9Hz1RIaOXdTc899xzDhw9n2LBhjBs3jhUrVrRqfcaYjs8SRRs40W7GVZUteyuoqQvy/J8eJ1Bb3epY+vfvz4IFC1i5ciW/+MUvmDp1aqvXaYzp2CxRtIET7Wb8jrvvoby6jjeef5IdO068m/Hhw4dz1VVXATBu3Di6desGOF2BlJSUtMVXYIxpxzrdk9nTP5rO2n1ro7rNIdlD+M/R/9ns8hPpZnxXWQUXnvd1pv3mf5gyfvQJdzO+adMmkpOTOXDgAFlZWQ2Wz5gxg7Vr1/LEE08cE7M9mW1M52LdjHvseLsZHzFyJBPGfJWN69dSWvzlMXVa6ma83vDhw7n22mt59tlnj+k1dt68eTz55JNMnz49Kj+jMabj6nRPZoc78o+lSLsZ/82MGfz51ffIycnmobt+RHX1sdclmutmvLE333yToqIiXn/9dR566KEj3Yx/9tln3HzzzcyZM4fu3btH88c0xnRAdkbRBiLtZnzf/gMkJqfSNTOT1Lpy3n478m7G161b12CbwWCQ4uJiJk+e3KCb8a1bt3LZZZfx5z//mcGDB8f05zbGdAyd7ozCC5F2M/7Ei69z6unD+Obk0fTtG1k34/VnHA8++GCDHX8gEGiym/E777yTvXv38oMf/ACAhIQEGl/TMcaYUJ3uYnY8cjr6q2R/RQ19s9PISvO+e454/86MMdFlF7Pj3O7yavZX1HBS15S4SBLGGBPKEoXHyipr2VlWRVZqEj0zkr0OxxhjjtFpEkU8NrFV1tRRvK+CtKQE8rrFT0d/8fhdGWO80ykSRUpKCnv37o2rHWBtIMjmvRUk+ISTu6dFbQjT1lJV9u7dS0pKitehGGPiRKe46ykvL4+SkhJ2797tdSiAszPefaiauqDSIyOZDQfiK1+npKSQl5fndRjGmDjRKRJFYmIi/fv39zoMAIJB5bbnlzPn8508cX0Bw087yeuQjDEmrPg6lO0E/ufddby1cif3XHAaUyxJGGPaAUsUbejvn5Tw6LwNXD06n5vGx8cZjjHGtMTTRCEifxKRUhH5vJnlIiIzRWSDiHwmIqPaOsZo+XjzPv7zrysZe0p3fnnJGXFzh5MxxrTE6zOKp4Hzwiw/HxjkvqYCv2+DmKKueF8F3//zMnK7pfL760aR2MpR6owxpi15usdS1SJgX5gqlwDPqOMDIEtEerdNdNFRXl3HTbOXUhdUnryhwJ68Nsa0O/F+aJsLFIfMl7hlDYjIVBH5WEQ+jpdbYOu9/HEx63aV89g1ozilR7rX4RhjzHGL90QREVWdpaoFqlrQo0cPr8NpYO7aUgb06ML4QTleh2KMMSck3hPFNiA/ZD7PLWsXDlfX8eHGfXxtSE+vQzHGmBMW74niNeB69+6nMUCZqu7wOqhILdqwh5pAkK8NsecljDHtl6dPZovI88AkIEdESoD7gEQAVf0D8BZwAbABqAC+602kJ2bumlIyUhIo6NfN61CMMeaEeZooVPXqFpYr8MM2CieqgkFl3helFA7qYbfDGmPaNduDxciq7QcpPVRt1yeMMe2eJYoYmbu2FBGYdGp83YVljDHHyxJFjMxdu4sR+Vl0T7dR64wx7ZslihjYfaiaFSVlfO1Ua3YyxrR/lihiYP4XpQBMtusTxpgOwBJFDMxdW8pJXZM5vU9Xr0MxxphWs0QRZTV1QRau38PXhvS0rsSNMR2CJYooW7p5H+XVdfY0tjGmw7BEEWVz15aSlODj7IHdvQ7FGGOiwhJFlM1bW8rYU7qTluTpQ+/GGBM1liiiaNOew2zcc9iexjbGdCiWKKJo7lrntlhLFMaYjsQSRRTNXbuLQT3Tyc9O8zoUY4yJmhYThYj8WkS6ikiiiLwnIrtF5Lq2CK49Ka+u46NNNkiRMabjieSM4uuqehC4ENgMDAT+I5ZBtUeL1u+mNqCWKIwxHU4kiaL+9p1vAC+ralkM42m33ltTSteUBL5ysg1SZIzpWCK5h/MNEVkLVAK3ikgPoCq2YbUvziBFuykc3IMEG6TIGNPBtLhXU9W7gXFAgarW4gxJekmsA2tPVm4rY095NVNOs2YnY0zHE8nF7DTgB8Dv3aI+QEEsg2pv6gcpmjjYEoUxpuOJpJ3kKaAG56wCYBvwYMwiaofmfVHKqL7dyO6S5HUoxhgTdZEkigGq+mugFkBVKwDrFtVVerCKz0rK7G4nY0yHFcnF7BoRSQUUQEQGANUxjaodmf/FbgAm22h2pjmqUFcFtZUQqIVgXcNXs2UBCIYsC9Qvr200XwcaAA26L3Xeg6FlzbyOqaPHrgttNB36TvPLNNhE/ZDP1X83zsSx8+GWHZlvvIyGdY7ZzvEuI4ywC5uI6Tg+e6JOOh0ufyLqq40kUdwHvA3ki8hzwNnAjVGPpJ16b+0uememcFrvDK9DMScqGICqMqjc774OQHUZ1FZBXaX7XnV0Z19X1WhZE+911Ufr1nl4k6D4QXwNXz4/iISUNVXHB8jRevXTjd+PLKPRMl/T9Y95dz93ZH2hsbvlR6ZpYb5RQ8cx48FI65c11poxZ2IxXk3X3OivkwgShaq+KyLLgTE439iPVXVPTKJpZ6rrAixav4dLRubaIEXxoLbS2ckf2eG7r6rGZQeOTQqREB8kpEJCMiSmQkJKw/e0HEhMceYbL6t/9yeCLwF87rs/oeG8z9+oTuh8yMsfUt+X4OzsfY13+PY3aaKjxUQhIoXu5CH3faiIoKpFsQurfVi6aT+HawJMsesTsVdXAwe2wv5NsG8j7HPfy4qP7vTDHbmLH1K7HX2l94QepzYsS8k6Op2c4e70U4+++xNt52s6pUiankK760gBRgPLgK/FJKJ25L21u0hO8DFuQI7XoXQMNYdh/+aGiaA+MZSVuG3ersQukH0KdOsPuV9puMNPzTo2ASRn2E7emBMUSdPTRaHzIpIPPBKziNoJVWXu2lLGDuhOapLf63Daj8r9jZLApqPz5Tsb1k3t5iSD/LNg+FXOdHZ/571LD9vxG9NGTmQYthLgtGgH0t5s3HOYLXsruHl8f69DiV+BWihZChveg01FsHe9kyhCZfR2zgoGngPZ/Y6eJWT3dxKFMcZzkVyj+B1H7+XyASOA5dHYuIicB/wW8ANPqOq0RstvBH6D85AfwKOqGv17v07APHeQosl2faKh/ZudxPDlXCc5VB90LqzmFsDp33STwCluQugHSTZ2hzHxLpIzio9DpuuA51V1cWs3LCJ+4DHgXJyzlKUi8pqqrm5U9UVVva2124u2uWtLOfWkDPK6dfIdXXU5bF4EX77nJIh9XzrlmflOYhg4BfpPdK4bGGPapUiuUcyO0bZHAxtUdSOAiLyA09lg40QRdw5W1fLRpn3cPOEUr0Npe6qw63P3rOE92LLEeQAsIRX6jYfR/wcGTIGcQXYNwZgOotlEISIrafrxQQFUVYe3ctu5QHHIfAlwVhP1Lndv0V0H/LuqFjeuICJTgakAffv2bWVYLVu0fg91Qe08vcUe3gNfznMSw5dzoXyXU97zdBhzi5MY+o51biM1xnQ44c4oLmyzKJr3Ok5TV7WIfB+YTRO35arqLGAWQEFBQYyejT/qvTWlZKYmMjK/gzanBGqh+KOjzUk7VgDqXFw+ZbLTnDTga9C1j9eRGmPaQLOJQlW3xHjb24D8kPk8jl60ro9hb8jsE8CvYxxTi4JBZf4XpUzsiIMU7dsE86fB2jeh5pDzkFreV2Hyz52zhj4jnKd/jTGdSiR3PY0BfodzS2wSzh1Kh1W1ayu3vRQYJCL9cRLEVcA1jbbdW1V3uLMXA2tauc1WW1FygL2HazpWs1PFPiiaAR/Ncp4+HnaFc7tq/0K7CG2Mieiup0dxduIv4wxYdD0wuLUbVtU6EbkNeAcn+fxJVVeJyC+Bj1X1NeB2EbkY526rfcRBZ4Tz1pbiE5g4uIfXobRebZWTHBbOgOpDMOJamHwPdO3tdWTGmDgS0QN3qrpBRPyqGgCeEpFPgJ+1duOq+hbwVqOye0OmfxaN7UTT3C9K+crJ3chKa8eDFAWDsOoVeO8Bp/+kgefAub90uig2xphGIkkUFSKSBHwqIr8GdhDZgEcdzq6DVXy+7SB3nXeq16GcuM2L4J//Bds/gV7D4Dv/gAGTvY7KGBPHIkkU38FJDLcB/45zAfryWAYVr+qfxm6Xo9ntXgf/ug++eMvps/7SP8DwK91xB4wxpnmRJIqvAG+q6kHggRjHE9feW1tKblYqp57UjgYpKi+F+f8Xls2GpC4w5T4Yc6szNoIxxkQgkkRxEfC/IlIEvAi8rap1sQ0r/lTXBVi8YQ+XjWongxTVVMCSx2DxI844DV+9CSb+J3SxLtGNMccnki48visiicD5wNXAYyLyrqreHPPo4siHG/dRURNgypCTvA4lvGAAPv0LzHsIDu2A06ZbW0QAABoYSURBVC6CKfdDzkCvIzPGtFOR3vVUKyJzcLr0SAUuBTpVopi7tpSURB9jB3T3OpSmqTpPUb97L5Such6Uu+Jp6DvG68iMMe1cJA/cnQ9cCUwC5uM8If3tmEYVZ1SV99buYtyAHFIS4/DJ5B2fwbu/gI3znW68r5gNQy+xTvmMMVERyRnF9TjXJr6vqtUxjicufbm7nOJ9lXy/cIDXoTRUtg3mPggrnneeoD5vGhTcBAnt+BkPY0zcieQaxdVtEUg8mxuPgxR99EfneQhVOPt2GH+HdbdhjImJExkKtdOZu7aUIb0yyM2Kk1tKP5wFc/4DBn0dvvEwZMW+a3VjTOdlT1u1oKyylqWb98fPQ3ZLn3SSxJAL4aq/WJIwxsRci4lCRC4SkU6bUBau300gqPGRKJbNhjfvgMHnwbeecnp6NcaYGIskAVwJrBeRX4vIkFgHFG/mriklKy2RkX27eRvIJ8/B6z+GgefCt5+xC9bGmDbTYqJQ1euAkcCXwNMiskREpopIO+rH4sQEgsr8dbuZNLgHfp+Ht5queBFe/SGcMgmufBYSkr2LxRjT6UTUpOT28/RX4AWgN/BNYLmI/CiGsXluRckB9h2u4Wunefg09sq/wj9ugX7jnWsSNi61MaaNRXKN4mIR+TvOw3aJwGhVPR84E/hpbMPz1tw1pfh9wsRBHg1StOof8MpU6DsWrnkRktK8icMY06lFcnvs5cD/qmpRaKGqVojITbEJKz7MXesMUpSZ5sFF4zVvwN9ucrriuOYlp+dXY4zxQCRNT/cDH9XPiEiqiPQDUNX3YhJVHNhRVsnqHQe9udvpi7fh5Ruhz0i49mVITm/7GIwxxhVJongZCIbMB9yyDm3e2t2AB4MUrX8XXvqOM/rcdX+DlK5tu31jjGkkkkSRoKo19TPudIe/N3Pu2lLyuqUyqGcbHs1veA9euBZ6ngbfeQVSMttu28YY04xIEsVuEbm4fkZELgH2xC4k71XVOoMUfW1Iz7YbpGjjAnjhGsgZ7IxjnerxcxvGGOOK5GL2LcBzIvIoIEAxTo+yHdYHG/dSWRtou2anzYvgL1dC9ilw/T8gLbtttmuMMRGIpPfYL4ExIpLuzpfHPCqPzV1bSmqinzGntMEgRVuWwHPfdvpsuv41G6rUGBN3mk0UInKdqj4rInc0KgdAVf8nxrF5QlWZu7aUswd2j/0gRcUfwXPfgq694YbXIN2j5zWMMSaMcNco6m/cz2jm1SGtLy2nZH8lX4v12Ngly+DZyyG9J9zwOmT0iu32jDHmBDV7RqGqj7vvD7RdON47OkhRDI/ut38Cz37TuWB9w+vQtU/stmWMMa0UrulpZrgPqurt0Q/He3PXljK0d1d6Z8ZokKIdn8Ezl0JyJtz4BmTmxWY7xhgTJeEuZi9rsyjiRFlFLcu27OfWiTEaG3vXKnjmEkhKd65J2KBDxph2IFzT0+zQ+Vjc9SQi5wG/BfzAE6o6rdHyZOAZ4CvAXuBKVd0cre03tsAdpCgmY2OXroXZFztdhN/wGmT3j/42jDEmBiLpPfYMEfkEWAWsFpFlInJ6azcsIn7gMeB8YChwtYgMbVTtJmC/qg4E/heY3trthjN3zS6yuyQxIj8ruivevQ5mXwQ+P9zwBnSP0RmLMcbEQCQP3M0C7lDVeQAiMgn4IzCuldseDWxQ1Y3uel8ALgFWh9S5BKdTQnDGw3hURERVtZXbPkbp/k2U7fkBP83Kxv+Pv0Z35RvnAeokiZyB0V23McbEWCSJokt9kgBQ1fkiEo0+r3NxnvKuVwKc1VwdVa0TkTKgO426EBGRqcBUgL59T6zd//DhIJ91OcSAykOwtbjlDxyPjN5w6e+hx6nRXa8xxrSBSBLFRhH5BfBnd/46YGPsQjp+qjoL58yHgoKCEzrb6J83gNH5E3j/4Bb0m2+2XR9PxhgT5yLpFPB7QA/gFeBvQI5b1lrbgPyQ+Ty3rMk6IpIAZOJc1I6JiXkTKT5UzOaDm2O1CWOMaXeaTRQikiIiPwH+G+dC9lmq+hVV/Ymq7o/CtpcCg0Skv4gkAVcBrzWq8xpwgzv9LWBuLK5P1CvMKwSgqKSohZrGGNN5hDujmA0UACtx7kz6TTQ3rKp1wG3AO8Aa4CVVXSUivwzp1vxJoLuIbADuAO6OZgyN9Unvw8CsgSwsWRjLzRhjTLsS7hrFUFUdBiAiTxIyHGq0qOpbwFuNyu4Nma4Croj2dsMpzCvkmVXPcKjmEBlJHbZLK2OMiVi4M4ra+gn36L9TmJg3kTqtY8n2JV6HYowxcSFcojhTRA66r0PA8PppETnYVgG2teE9htM1qSsLShZ4HYoxxsSFcF14xHgwhviU4Evg7NyzWbRtEUEN4pNIbgwzxpiOy/aCTZiYN5F9VftYtWeV16EYY4znLFE0YXzueHzis+YnY4zBEkWTMpMzGdFjhD1PYYwxWKJo1oS8CazZt4bSilKvQzHGGE9ZomhG/VPa9vCdMaazs0TRjEFZg+jdpbddpzDGdHqWKJohIhTmFfLBjg+oCdR4HY4xxnjGEkUYhXmFVNZV8vHOj70OxRhjPGOJIozRvUaT4k+x5idjTKdmiSKMlIQURvcezYKSBcSwd3NjjIlrlihaMDFvItvKt7GpbJPXoRhjjCcsUbRgQu4EwAYzMsZ0XpYoWtA7vTeDuw2maJslCmNM52SJIgKFeYUs37WcgzUdtnd1Y4xpliWKCBTmFRLQAO9vf9/rUIwxps1ZoojA8JzhZCZnUlRszU/GmM7HEkUE/D4/43PHs2jbIgLBgNfhGGNMm7JEEaGJeRPZX72fz/d+7nUoxhjTpixRRGhcn3H4xc+CYntK2xjTuViiiFBmciZn9jiThdus23FjTOdiieI4TMyfyNp9a9l5eKfXoRhjTJuxRHEcCnPdwYzsrMIY04lYojgOA7IGkJuea915GGM6FUsUx0FEmJA7gQ93fEh1oNrrcIwxpk1YojhO9YMZLd251OtQjDGmTSR4sVERyQZeBPoBm4Fvq+r+JuoFgJXu7FZVvbitYmzO6N6jSU1IZUHxAsbnjvc6HGM6FVVF0SPTAMrRsqMVG89qk9Oh62lp2XHH2jiIKK03HJ/4SEtMi/p6PUkUwN3Ae6o6TUTuduf/s4l6lao6om1DCy/Zn8xZvc5i4baFqCoi4nVIJk7UBmuprqumKlBFdaCa6rpq5z3gltVVUxuspU7rqAvWEQgGnHd13htMN1GnNlh7ZHkgGGhQJ6hBggQJahBVdeabK3PLVZWABo4ub6LsyH+qx86H7LCPq15oWf3OVDmmLLSeiczwnOE8943nor5erxLFJcAkd3o2MJ+mE0VcKswvZH7JfL488CUDuw30OhxzAmoCNZRVl1FWXcbBmoPOdM3R+fKa8iM7+epANVV1VQ12+PXTNYGaI8sCGt3uXXziwy9+EnwJJEgCfp8zfaTMnfb7/PjFj098+PDhEx8i4syHvI4pwynzi//oskZlAIIc+bzglrnTghxXPUFw/m9Yp/7z9ULLGi+vX0fjssYHbaHrC11nU5radnPriXRZS9uMhZzUnJis16tEcZKq7nCndwInNVMvRUQ+BuqAaar6j6YqichUYCpA3759ox3rMY4MZrStyBKFx8pryo/s4EN3+o3fQxPBoZpDVNZVNrtOn/joktiFFH8Kyf5kUhKc92R/Ml0SupCdkt3kstDp0PkUfwrJCc57/Q6+8Q6/PhH4xU+iLxG/z9nxGxMPYpYoRORfQK8mFt0TOqOqKiLNnV+erKrbROQUYK6IrFTVLxtXUtVZwCyAgoKCmJ+r9urSiyHZQygqKeJ7Z3wv1pvr1FSV3ZW7KT5UzNaDWyk+VHzktfXQVg7VHGr2syn+FLomd6VrUlcykzPJT8/njO5nkJmceaSsa3JXMpMyG5R1SexiO2ljQsQsUajqOc0tE5FdItJbVXeISG+gtJl1bHPfN4rIfGAkcEyi8MKE3An86fM/UVZdRmZyptfhtGt1wTp2Ht7ZMAkc3MrWQ1vZVr6twdG/X/z0Se9DfkY+F+RcQG56LlnJWWQmOzv7zKTMI8khJSHFw5/KmI7Dq6an14AbgGnu+6uNK4hIN6BCVatFJAc4G/h1m0YZRmFeIX9c+Ufe3/4+5/c/3+tw4l5NoIaS8hKKDx49G6hPCtvKt1EXrDtSN9mfTF56Hvld8xnbZyx9M/qSn5FP34y+9ErvRaIv0cOfxJjOx6tEMQ14SURuArYA3wYQkQLgFlW9GTgNeFxEgjjPe0xT1dUexXuMYTnD6JbcjQUlCyxRNCGoQdbuW8v729/n/e3v80npJw2SQXpiOvkZ+Zza7VTOPflc8jPyj7x6pvW0ph9j4ogniUJV9wJTmij/GLjZnX4fGNbGoUWsfjCjhdsWEggG8Pv8Xofkud0Vu1myYwmLty3mgx0fsK9qHwBDsofwndO+w+DswUfODLKSs+zWYmPaCa/OKDqEwvxCXt/4Oiv3rGREz7h63KNNVAeqWb5rOUu2L2Hx9sWs278OgOyUbMb1Gce4PuMY22dszG7ZM8a0DUsUrXBkMKOSBZ0iUagqm8o2sXj7YhZvX8yyncuoClSR6EtkVM9R/GTUTzg792wGdxtsTUfGdCCWKFqha1JXRvYcSVFJET8e9WOvw4mJsuoyPtjxwZFrDfVjcfTr2o/LB1/OuD7jKDipICbdBhhj4oMlilaamDeRh5c9zI7yHfRO7+11OK1WF6xj5Z6VLN62mCXbl/D53s8JapCMxAzG9BnD94d/n3F9xtEnvY/XoRpj2oglilYqzCvk4WUPs3DbQr596re9DueE1QZr+fv6v/P4iscprSzFJz6G5QzjluG3MLbPWM7IOYMEn/25GNMZ2b/8Vuqf2Z+89DyKSoraZaIIapA5m+bw2KePUXyomBE9RnDX6LsY03uMPUhojAEsUbSaiFCYV8gr61+hqq6q3TwNrKoUlRQx85OZrNu/jsHdBvPYlMeYkDvBbls1xjRgt6ZEQWFeIVWBKj7a+ZHXoURk6c6lXD/nem6bextVdVVMnzCdly96mcK8QksSxphj2BlFFBT0KiA1IZWikiIK8wq9DqdZq/au4nfLf8fi7YvpmdqTe8fey6UDL7UuMYwxYVmiiIJkfzJjeo+hqKQoLgcz2li2kUc/eZR3t7xLVnIWdxbcyZWnXtlumsmMMd6yRBElE/MmMq94HusPrGdwt8FehwPAjvId/H7F73n1y1dJ8adwy5m3cMPQG0hPSvc6NGNMO2KJIkom5LmDGZUUeZ4o9lbu5YmVT/DiFy8iCNeedi03D7uZ7JRsT+MyxrRPliiipGdaT07LPo2FJQu5edjNnsRwqOYQT696mj+v/jPVgWouHXgpt555K726NDV+lDHGRMYSRRTVj1FxoOoAWSlZbbbdqroqnl/7PE9+/iRl1WX8W79/44cjfkj/zP5tFoMxpuOyRBFFhXmFPP7Z4yzevphvnPKNmG+v8dPUZ+eeze0jb2do96Ex37YxpvOwRBFFZ+ScQXZKNgtKFsQ8UczdOpcZH8+g+FAxI3uOZHrhdAp6FcR0m8aYzskSRRT5xMf43PHML55PXbAuJn0jlVWXMe2jabyx8Q0GZg20p6mNMTFnT2ZHWWFeIQdrDvLZ7s+ivu5F2xZx2auXMWfTHG4981Zeuugle5raGBNzdkYRZeP6jCNBElhQsoBRJ42KyjoP1x5mxscz+Ou6vzIgcwAzp8zk9O6nR2XdxhjTEjujiLKMpAxGnTSKopKiqKxv6c6lXP7a5fxt3d/47hnf5cWLXrQkYYxpU5YoYqAwr5ANBzawvXz7Ca+jsq6S6R9N53vvfA+/+Jl9/mzu+ModJPuToxipMca0zBJFDNR3DHiiZxUrdq/g269/m2fXPMvVQ67m5YteZmTPkdEM0RhjImaJIgb6de1H34y+x50oagI1/Hb5b7l+zvVUB6r549f/yM/P+rmNR22M8ZRdzI6B+sGMXl73MpV1laQmpLb4mTV713DP4ntYv389lw26jP8o+A/rvM8YExfsjCJGJuRNoDpQzUc7wg9mVBus5Q8r/sA1b17DgaoDPDblMR4Y94AlCWNM3LAzihgpOKmAtIQ0FpQsYGL+xCbrfHngS+5ZdA+r9q7igv4X8POzfm7jVBtj4o4lihhJ8icxts/YJgczCgQDPLvmWWYun0mXxC48PPFhvt7v6x5Ga4wxzbOmpxiamDeRXRW7WLd/3ZGyrQe38t13vsuMj2cwPnc8r1zyiiUJY0xc8yRRiMgVIrJKRIIi0mxPdiJynoh8ISIbROTutowxGkIHMwpqkBfWvsC3Xv8WG/Zv4Ffjf8Ujkx8hJzXH4yiNMSY8r5qePgcuAx5vroKI+IHHgHOBEmCpiLymqqvbJsTWy0nNYWj3oby9+W0+2vkRH+z4gLP7nM394+63wYSMMe2GJ4lCVdcALXVmNxrYoKob3bovAJcA7SZRgNP89PsVvyctIY37xt7H5YMut078jDHtSjxfzM4FikPmS4CzmqooIlOBqQB9+/aNfWTH4YrBV1BeW841Q64hLyPP63CMMea4xSxRiMi/gKbaV+5R1VejuS1VnQXMAigoKNBorru1eqT14K6v3uV1GMYYc8JilihU9ZxWrmIbkB8yn+eWGWOMaUPxfHvsUmCQiPQXkSTgKuA1j2MyxphOx6vbY78pIiXAWOBNEXnHLe8jIm8BqGodcBvwDrAGeElVV3kRrzHGdGZe3fX0d+DvTZRvBy4ImX8LeKsNQzPGGNNIPDc9GWOMiQOWKIwxxoRlicIYY0xYliiMMcaEJapx9Xxaq4nIbmBLK1aRA+yJUjix1p5ihfYVb3uKFdpXvO0pVmhf8bYm1pNVtUdTCzpcomgtEflYVZvt0TaetKdYoX3F255ihfYVb3uKFdpXvLGK1ZqejDHGhGWJwhhjTFiWKI41y+sAjkN7ihXaV7ztKVZoX/G2p1ihfcUbk1jtGoUxxpiw7IzCGGNMWJYojDHGhGWJwiUi54nIFyKyQUTu9jqecEQkX0TmichqEVklIj/2OqaWiIhfRD4RkTe8jqUlIpIlIn8VkbUiskZExnodU3NE5N/dv4HPReR5EUnxOqZQIvInESkVkc9DyrJF5F0RWe++d/MyxnrNxPob9+/gMxH5u4hkeRljqKbiDVn2UxFREcmJxrYsUeDsxIDHgPOBocDVIjLU26jCqgN+qqpDgTHAD+M8XoAf43QX3x78FnhbVYcAZxKncYtILnA7UKCqZwB+nHFb4snTwHmNyu4G3lPVQcB77nw8eJpjY30XOENVhwPrgJ+1dVBhPM2x8SIi+cDXga3R2pAlCsdoYIOqblTVGuAF4BKPY2qWqu5Q1eXu9CGcHVmut1E1T0TygG8AT3gdS0tEJBMoBJ4EUNUaVT3gbVRhJQCpIpIApAHbPY6nAVUtAvY1Kr4EmO1OzwYubdOgmtFUrKr6T3dsHIAPcEbajAvNfLcA/wvcBUTtTiVLFI5coDhkvoQ43vGGEpF+wEjgQ28jCesRnD/coNeBRKA/sBt4ym0qe0JEungdVFNUdRswA+fIcQdQpqr/9DaqiJykqjvc6Z3ASV4Gcxy+B8zxOohwROQSYJuqrojmei1RtGMikg78DfiJqh70Op6miMiFQKmqLvM6lgglAKOA36vqSOAw8dM00oDbtn8JTnLrA3QRkeu8jer4qHN/ftzfoy8i9+A0+T7ndSzNEZE04OfAvdFetyUKxzYgP2Q+zy2LWyKSiJMknlPVV7yOJ4yzgYtFZDNOk97XRORZb0MKqwQoUdX6M7S/4iSOeHQOsElVd6tqLfAKMM7jmCKxS0R6A7jvpR7HE5aI3AhcCFyr8f3g2QCcg4YV7r+3PGC5iPRq7YotUTiWAoNEpL+IJOFcEHzN45iaJSKC04a+RlX/x+t4wlHVn6lqnqr2w/le56pq3B71qupOoFhETnWLpgCrPQwpnK3AGBFJc/8mphCnF94beQ24wZ2+AXjVw1jCEpHzcJpNL1bVCq/jCUdVV6pqT1Xt5/57KwFGuX/TrWKJAnAvVt0GvIPzD+0lVV3lbVRhnQ18B+fo/FP3dUFLHzIR+xHwnIh8BowAfuVxPE1yz3r+CiwHVuL8e46r7iZE5HlgCXCqiJSIyE3ANOBcEVmPc1Y0zcsY6zUT66NABvCu++/sD54GGaKZeGOzrfg+kzLGGOM1O6MwxhgTliUKY4wxYVmiMMYYE5YlCmOMMWFZojDGGBOWJQpjwhCRcve9n4hcE+V1/7zR/PvRXL8x0WKJwpjI9AOOK1G4HfWF0yBRqGp7eKradEKWKIyJzDRggvvQ1b+742v8RkSWumMVfB9ARCaJyEIReQ33iW4R+YeILHPHjZjqlk3D6fX1UxF5zi2rP3sRd92fi8hKEbkyZN3zQ8bKeM59ItuYmGrpiMcY47gbuFNVLwRwd/hlqvpVEUkGFotIfc+to3DGMNjkzn9PVfeJSCqwVET+pqp3i8htqjqiiW1dhvNE+JlAjvuZInfZSOB0nO7EF+M8pb8o+j+uMUfZGYUxJ+brwPUi8ilOF+/dgUHuso9CkgTA7SKyAmc8g/yQes0ZDzyvqgFV3QUsAL4asu4SVQ0Cn+I0iRkTU3ZGYcyJEeBHqvpOg0KRSThdk4fOnwOMVdUKEZkPtGa40uqQ6QD2b9i0ATujMCYyh3A6h6v3DnCr2907IjK4mQGOMoH9bpIYgjN0bb3a+s83shC40r0O0gNnxL2PovJTGHMC7GjEmMh8BgTcJqSnccbV7ofT37/gjIrX1JCebwO3iMga4Auc5qd6s4DPRGS5ql4bUv53YCywAmdQn7tUdaebaIxpc9Z7rDHGmLCs6ckYY0xYliiMMcaEZYnCGGNMWJYojDHGhGWJwhhjTFiWKIwxxoRlicIYY0xY/x/mJgYYnZbh5AAAAABJRU5ErkJggg==
" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we run the same code but with two different discount factors, 0.2 and 0.99.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">V</span><span class="p">,</span> <span class="n">V_history</span> <span class="o">=</span> <span class="n">policy_evaluation_history</span><span class="p">(</span><span class="n">policy_optimal</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>

<span class="n">s0</span><span class="p">,</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">V_history</span><span class="p">])</span>
<span class="n">s1</span><span class="p">,</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">V_history</span><span class="p">])</span>
<span class="n">s2</span><span class="p">,</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">v</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">V_history</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Optimal policy with gamma = </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">gamma</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Policy values&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="n">s0</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">],</span>
           <span class="p">[</span><span class="s2">&quot;State s0&quot;</span><span class="p">,</span>
            <span class="s2">&quot;State s1&quot;</span><span class="p">,</span>
            <span class="s2">&quot;State s2&quot;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Comparing the plot with a discount factor of 0.5 with this one, we can see that the smaller the factor, the faster the policy values converge.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>
<span class="n">V</span><span class="p">,</span> <span class="n">V_history</span> <span class="o">=</span> <span class="n">policy_evaluation_history</span><span class="p">(</span><span class="n">policy_optimal</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>

<span class="n">s0</span><span class="p">,</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">V_history</span><span class="p">])</span>
<span class="n">s1</span><span class="p">,</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">V_history</span><span class="p">])</span>
<span class="n">s2</span><span class="p">,</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">v</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">V_history</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Optimal policy with gamma = </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">gamma</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Policy values&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="n">s0</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">],</span>
           <span class="p">[</span><span class="s2">&quot;State s0&quot;</span><span class="p">,</span>
            <span class="s2">&quot;State s1&quot;</span><span class="p">,</span>
            <span class="s2">&quot;State s2&quot;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By comparing the plot with a discount factor of 0.5 to the plot with a discount factor of 0.99, we can see that the larger the factor, the longer it takes for policy values to converge. The discount factor is a tradeoff between rewards now and rewards in the future.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Simulating-the-FrozenLake-environment">Simulating the FrozenLake environment<a class="anchor-link" href="#Simulating-the-FrozenLake-environment"> </a></h2><p>The optimal policies for the MDPs we have dealt with so far are pretty intuitive. However, it won't be that straightforward in most cases, such as the FrozenLake environment.</p>
<p>FrozenLake is a typical Gym environment with a discrete state space. It is about moving an agent from the starting location to the goal location in a grid world, and at the same time avoiding traps. The grid is either four by four (<a href="https://gym.openai.com/envs/FrozenLake-v0/">https://gym.openai.com/envs/FrozenLake-v0/</a>) or eight by eight.</p>
<p>The grid is made up of the following four types of tiles:</p>
<ul>
<li>S: The starting location</li>
<li>G: The goal location, which terminates an episode</li>
<li>F: The frozen tile, which is a walkable location</li>
<li>H: The hole location, which terminates an episode</li>
</ul>
<p>There are four actions, obviously: moving left (0), moving down (1), moving right (2), and moving up (3). The reward is +1 if the agent successfully reaches the goal location, and 0 otherwise. Also, the observation space is represented in a 16-dimensional integer array, and there are 4 possible actions (which makes sense).</p>
<p>What is tricky in this environment is that, as the ice surface is slippery, the agent won't always move in the direction it intends. For example, it may move to the left or to the right when it intends to move down.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">torch</span>


<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;FrozenLake-v0&quot;</span><span class="p">)</span>

<span class="n">n_state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span>
<span class="nb">print</span><span class="p">(</span><span class="n">n_state</span><span class="p">)</span>
<span class="n">n_action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
<span class="nb">print</span><span class="p">(</span><span class="n">n_action</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>16
4
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

<span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>

<span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">is_done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>
<span class="ansi-red-bg">S</span>FFF
FHFH
FFFH
HFFG
  (Down)
SFFF
<span class="ansi-red-bg">F</span>HFH
FFFH
HFFG
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">new_state</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">is_done</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">info</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>4
0.0
False
{&#39;prob&#39;: 0.3333333333333333}
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To demonstrate how difficult it is to walk on the frozen lake, implement a random policy and calculate the average total reward over 1,000 episodes. First, define a function that simulates a FrozenLake episode given a policy and returns the total reward (we know it is either 0 or 1):</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">run_episode</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">is_done</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">is_done</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">policy</span><span class="p">[</span><span class="n">state</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">is_done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="k">if</span> <span class="n">is_done</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">total_reward</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now run 1000 episodes, and a policy will be randomly generated and will be used in each episode:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">n_episode</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">total_rewards</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_episode</span><span class="p">):</span>
    <span class="n">random_policy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">high</span><span class="o">=</span><span class="n">n_action</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_state</span><span class="p">,))</span>
    <span class="n">total_reward</span> <span class="o">=</span> <span class="n">run_episode</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">random_policy</span><span class="p">)</span>
    <span class="n">total_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_reward</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Average total reward under random policy: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_episode</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Average total reward under random policy: 0.014
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This basically means there is only a 1.4% chance on average that the agent can reach the goal if we randomize the actions.</p>
<p>Next, we experiment with a random search policy. In the training phase, we randomly generate a bunch of policies and record the first one that reaches the goal:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">random_policy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">high</span><span class="o">=</span><span class="n">n_action</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_state</span><span class="p">,))</span>
    <span class="n">total_reward</span> <span class="o">=</span> <span class="n">run_episode</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">random_policy</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">total_reward</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">best_policy</span> <span class="o">=</span> <span class="n">random_policy</span>
        <span class="k">break</span>

<span class="nb">print</span><span class="p">(</span><span class="n">best_policy</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor([3, 3, 0, 3, 2, 0, 1, 3, 1, 2, 0, 1, 1, 1, 1, 0])
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now run 1,000 episodes with the policy we just cherry-picked:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">total_rewards</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_episode</span><span class="p">):</span>
    <span class="n">total_reward</span> <span class="o">=</span> <span class="n">run_episode</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">best_policy</span><span class="p">)</span>
    <span class="n">total_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_reward</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Average total reward under random search policy: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_episode</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Average total reward under random search policy: 0.111
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Using the random search algorithm, the goal will be reached 11.1% of the time on average.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can look into the details of the FrozenLake environment, including the transformation matrix and rewards for each state and action, by using the P attribute. For example, for state 6, we can do the following:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">P</span><span class="p">[</span><span class="mi">6</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>{0: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False)], 1: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True)], 2: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False)], 3: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True)]}
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This returns a dictionary with keys 0, 1, 2, and 3, representing four possible actions. The value is a list of movements after taking an action. The movement list is in the following format: (transformation probability, new state, reward received, is done). For instance, if the agent resides in state 6 and intends to take action 1 (down), there is a 33.33% chance that it will land in state 5, receiving a reward of 0 and terminating the episode; there is a 33.33% chance that it will land in state 10 and receive a reward of 0; and there is a 33.33% chance that it will land in state 7, receiving a reward of 0 and terminating the episode.</p>
<p>For state 11, we can do the following:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">P</span><span class="p">[</span><span class="mi">11</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>{0: [(1.0, 11, 0, True)], 1: [(1.0, 11, 0, True)], 2: [(1.0, 11, 0, True)], 3: [(1.0, 11, 0, True)]}
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>As stepping on a hole will terminate an episode, it won’t make any movement afterward.</p>
<p>Feel free to check out the other states.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Solving-an-MDP-with-a-value-iteration-algorithm">Solving an MDP with a value iteration algorithm<a class="anchor-link" href="#Solving-an-MDP-with-a-value-iteration-algorithm"> </a></h2><p>An MDP is considered solved if its optimal policy is found. In this recipe, we will figure out the optimal policy for the FrozenLake environment using a value iteration algorithm.</p>
<p>The idea behind value iteration is quite similar to that of policy evaluation. It is also an iterative algorithm. It starts with arbitrary policy values and then iteratively updates the values based on the Bellman optimality equation until they converge. So in each iteration, instead of taking the expectation (average) of values across all actions, it picks the action that achieves the maximal policy values:</p>
<p>
$$\mathcal{V}_*(s) = \max_{a \in \mathcal{A}} (\mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a {V}_{*}(s')))$$
</p>
<p>Once the optimal values are computed, we can easily obtain the optimal policy accordingly.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">gym</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;FrozenLake-v0&#39;</span><span class="p">)</span>

<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>

<span class="n">threshold</span> <span class="o">=</span> <span class="mf">0.0001</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now define the function that computes optimal values based on the value iteration algorithm:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">value_iteration</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Solve a given environment with value iteration algorithm</span>
<span class="sd">    @param env: OpenAI Gym environment</span>
<span class="sd">    @param gamma: discount factor</span>
<span class="sd">    @param threshold: the evaluation will stop once values for all states are less than the threshold</span>
<span class="sd">    @return: values of the optimal policy for the given environment</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span>
    <span class="n">n_action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_state</span><span class="p">)</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">V_temp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_state</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_state</span><span class="p">):</span>
            <span class="n">v_actions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_action</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_action</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">trans_prob</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">P</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]:</span>
                    <span class="n">v_actions</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">trans_prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">new_state</span><span class="p">])</span>
            <span class="n">V_temp</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">v_actions</span><span class="p">)</span>
        <span class="n">max_delta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="n">V_temp</span><span class="p">))</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">V_temp</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">max_delta</span> <span class="o">&lt;=</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">V</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Plug in the environment, discount factor, and convergence threshold, then print the optimal values:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">V_optimal</span> <span class="o">=</span> <span class="n">value_iteration</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Optimal values:</span><span class="se">\n</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">V_optimal</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Optimal values:
tensor([0.5404, 0.4966, 0.4681, 0.4541, 0.5569, 0.0000, 0.3572, 0.0000, 0.5905,
        0.6421, 0.6144, 0.0000, 0.0000, 0.7410, 0.8625, 0.0000])
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have the optimal values, we develop the function that extracts the optimal policy out of them:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We developed our value_iteration function according to the Bellamn Optimality Equation. We perform the following tasks:</p>
<ul>
<li>Initialize the policy values as all zeros.</li>
<li>Update the values based on the Bellman optimality equation.</li>
<li>Compute the maximal change of the values across all states.</li>
<li>If the maximal change is greater than the threshold, we keep updating the values. Otherwise, we terminate the evaluation process and return the latest values as the optimal values.</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">extract_optimal_policy</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">V_optimal</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Obtain the optimal policy based on the optimal values</span>
<span class="sd">    @param env: OpenAI Gym environment</span>
<span class="sd">    @param V_optimal: optimal values</span>
<span class="sd">    @param gamma: discount factor</span>
<span class="sd">    @return: optimal policy</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span>
    <span class="n">n_action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
    <span class="n">optimal_policy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_state</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_state</span><span class="p">):</span>
        <span class="n">v_actions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_action</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_action</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">trans_prob</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">P</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]:</span>
                <span class="n">v_actions</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">trans_prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V_optimal</span><span class="p">[</span><span class="n">new_state</span><span class="p">])</span>
        <span class="n">optimal_policy</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">v_actions</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">optimal_policy</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Plug in the environment, discount factor, and optimal values, then print the optimal policy:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">optimal_policy</span> <span class="o">=</span> <span class="n">extract_optimal_policy</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">V_optimal</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Optimal policy:</span><span class="se">\n</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">optimal_policy</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Optimal policy:
tensor([0., 3., 3., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.])
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We want to gauge how good the optimal policy is. So, let's run 1,000 episodes with the optimal policy and check the average reward. Here, we will reuse the run_episode function we defined in the previous recipe:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">run_episode</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">is_done</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">is_done</span><span class="p">:</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">policy</span><span class="p">[</span><span class="n">state</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">is_done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="k">if</span> <span class="n">is_done</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">total_reward</span>


<span class="n">n_episode</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">total_rewards</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_episode</span><span class="p">):</span>
    <span class="n">total_reward</span> <span class="o">=</span> <span class="n">run_episode</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">optimal_policy</span><span class="p">)</span>
    <span class="n">total_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_reward</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Average total reward under the optimal policy: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_episode</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Average total reward under the optimal policy: 0.737
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Under the optimal policy, the agent will reach the goal 74% of the time, on average. This is the best we are able to get since the ice is slippery.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We obtained a success rate of 74% with a discount factor of 0.99. How does the discount factor affect the performance? Let's do some experiments with different factors, including 0, 0.2, 0.4, 0.6, 0.8, 0.99, and 1.:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">gammas</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="o">.</span><span class="mi">99</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]</span>
<span class="n">n_episode</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">avg_reward_gamma</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">gamma</span> <span class="ow">in</span> <span class="n">gammas</span><span class="p">:</span>
    <span class="n">V_optimal</span> <span class="o">=</span> <span class="n">value_iteration</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
    <span class="n">optimal_policy</span> <span class="o">=</span> <span class="n">extract_optimal_policy</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">V_optimal</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
    <span class="n">total_rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_episode</span><span class="p">):</span>
        <span class="n">total_reward</span> <span class="o">=</span> <span class="n">run_episode</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">optimal_policy</span><span class="p">)</span>
        <span class="n">total_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_reward</span><span class="p">)</span>
    <span class="n">avg_reward_gamma</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_episode</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">gammas</span><span class="p">,</span> <span class="n">avg_reward_gamma</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Success rate vs discount factor&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Discount factor&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Average success rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The result shows that the performance improves when there is an increase in the discount factor. This verifies the fact that a small discount factor values the reward now and a large discount factor values a better reward in the future.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Solving-an-MDP-with-a-policy-iteration-algorithm">Solving an MDP with a policy iteration algorithm<a class="anchor-link" href="#Solving-an-MDP-with-a-policy-iteration-algorithm"> </a></h2><p>Another approach to solving an MDP is by using a policy iteration algorithm, which we will discuss in this section.</p>
<p>A policy iteration algorithm can be subdivided into two components: policy evaluation and policy improvement. It starts with an arbitrary policy. And in each iteration, it first computes the policy values given the latest policy, based on the Bellman expectation equation; it then extracts an improved policy out of the resulting policy values, based on the Bellman optimality equation. It iteratively evaluates the policy and generates an improved version until the policy doesn't change any more.</p>
<p>Let's develop a policy iteration algorithm and use it to solve the FrozenLake environment.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">gym</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;FrozenLake-v0&#39;</span><span class="p">)</span>

<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>

<span class="n">threshold</span> <span class="o">=</span> <span class="mf">0.0001</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we define the policy_evaluation function that computes the values given a policy:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">policy_evaluation</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform policy evaluation</span>
<span class="sd">    @param env: OpenAI Gym environment</span>
<span class="sd">    @param policy: policy matrix containing actions and their probability in each state</span>
<span class="sd">    @param gamma: discount factor</span>
<span class="sd">    @param threshold: the evaluation will stop once values for all states are less than the threshold</span>
<span class="sd">    @return: values of the given policy</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_state</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_state</span><span class="p">)</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">V_temp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_state</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_state</span><span class="p">):</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">policy</span><span class="p">[</span><span class="n">state</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">trans_prob</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">P</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]:</span>
                <span class="n">V_temp</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+=</span> <span class="n">trans_prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">new_state</span><span class="p">])</span>
        <span class="n">max_delta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="n">V_temp</span><span class="p">))</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">V_temp</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">max_delta</span> <span class="o">&lt;=</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">V</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we develop the second main component of the policy iteration algorithm, the policy improvement part:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">policy_improvement</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Obtain an improved policy based on the values</span>
<span class="sd">    @param env: OpenAI Gym environment</span>
<span class="sd">    @param V: policy values</span>
<span class="sd">    @param gamma: discount factor</span>
<span class="sd">    @return: the policy</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span>
    <span class="n">n_action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
    <span class="n">policy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_state</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_state</span><span class="p">):</span>
        <span class="n">v_actions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_action</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_action</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">trans_prob</span><span class="p">,</span> <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">env</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">P</span><span class="p">[</span><span class="n">state</span><span class="p">][</span><span class="n">action</span><span class="p">]:</span>
                <span class="n">v_actions</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">trans_prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">new_state</span><span class="p">])</span>
        <span class="n">policy</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">v_actions</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">policy</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This extracts an improved policy from the given policy values, based on the Bellman optimality equation.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have both components ready, we develop the policy iteration algorithm as follows:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">policy_iteration</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Solve a given environment with policy iteration algorithm</span>
<span class="sd">    @param env: OpenAI Gym environment</span>
<span class="sd">    @param gamma: discount factor</span>
<span class="sd">    @param threshold: the evaluation will stop once values for all states are less than the threshold</span>
<span class="sd">    @return: optimal values and the optimal policy for the given environment</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span>
    <span class="n">n_action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
    <span class="n">policy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">high</span><span class="o">=</span><span class="n">n_action</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_state</span><span class="p">,))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">policy_evaluation</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
        <span class="n">policy_improved</span> <span class="o">=</span> <span class="n">policy_improvement</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">policy_improved</span><span class="p">,</span> <span class="n">policy</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">V</span><span class="p">,</span> <span class="n">policy_improved</span>
        <span class="n">policy</span> <span class="o">=</span> <span class="n">policy_improved</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The policy_iteration function does the following tasks:</p>
<ul>
<li>Initializes a random policy.</li>
<li>Computes the values of the policy with the policy evaluation algorithm.</li>
<li>Obtains an improved policy based on the policy values.</li>
<li>If the new policy is different from the old one, it updates the policy and runs another iteration. Otherwise, it terminates the iteration process and returns the policy values and the policy.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Plug in the environment, discount factor, and convergence threshold:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">V_optimal</span><span class="p">,</span> <span class="n">optimal_policy</span> <span class="o">=</span> <span class="n">policy_iteration</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Optimal values:</span><span class="se">\n</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">V_optimal</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Optimal policy:</span><span class="se">\n</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">optimal_policy</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Optimal values:
tensor([0.5404, 0.4966, 0.4681, 0.4541, 0.5569, 0.0000, 0.3572, 0.0000, 0.5905,
        0.6421, 0.6144, 0.0000, 0.0000, 0.7410, 0.8625, 0.0000])
Optimal policy:
tensor([0., 3., 3., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.])
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>They are exactly the same as what we got using the value iteration algorithm.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We have just solved the FrozenLake environment with a policy iteration algorithm. So, you may wonder when it is better to use policy iteration over value iteration and vice versa. There are basically three scenarios where one has the edge over the other:</p>
<ul>
<li>If there is a large number of actions, use policy iteration, as it can converge faster.</li>
<li>If there is a small number of actions, use value iteration.</li>
<li>If there is already a viable policy (obtained either by intuition or domain knowledge), use policy iteration.</li>
</ul>
<p>Outside those scenarios, policy iteration and value iteration are generally comparable.</p>
<p>In the next section, we will apply each algorithm to solve the coin-flipping-gamble problem. We will see which algorithm converges faster.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Solving-the-coin-flipping-gamble-problem">Solving the coin-flipping gamble problem<a class="anchor-link" href="#Solving-the-coin-flipping-gamble-problem"> </a></h2><p>Gambling on coin flipping should sound familiar to everyone. In each round of the game, the gambler can make a bet on whether a coin flip will show heads. If it turns out heads, the gambler will win the same amount they bet; otherwise, they will lose this amount. The game continues until the gambler loses (ends up with nothing) or wins (wins more than 100 dollars, let's say). Let's say the coin is unfair and it lands on heads 40% of the time. In order to maximize the chance of winning, how much should the gambler bet based on their current capital in each round? This will definitely be an interesting problem to solve.</p>
<p>If the coin lands on heads more than 50% of the time, there is nothing to discuss. The gambler can just keep betting one dollar each round and should win the game most of the time. If it is a fair coin, the gambler could bet one dollar each round and end up winning around 50% of the time. It gets tricky when the probability of heads is lower than 50%; the safe-bet strategy wouldn't work anymore. Nor would a random strategy, either. We need to resort to the reinforcement learning techniques we've learned in this tutorial to make smart bets.</p>
<p>Let's get started by formulating the coin-flipping gamble problem as an MDP. It is basically an undiscounted, episodic, and finite MDP with the following properties:</p>
<ul>
<li>The state is the gambler's capital in dollars. There are 101 states: 0, 1, 2, …, 98, 99, and 100+.</li>
<li>The reward is 1 if the state 100+ is reached; otherwise, the reward is 0.</li>
<li>The action is the possible amount the gambler bets in a round. Given state s, the possible actions include 1, 2, …, and min(s, 100 - s). For example, when the gambler has 60 dollars, they can bet any amount from 1 to 40. Any amount above 40 doesn't make any sense as it increases the loss and doesn't increase the chance of winning.</li>
<li>The next state after taking an action depends on the probability of the coin coming up heads. Let's say it is 40%. So, the next state of state s after taking action a will be s+a by 40%, s-a by 60%.</li>
<li>The process terminates at state 0 and state 100+.</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">capital_max</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_state</span> <span class="o">=</span> <span class="n">capital_max</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">rewards</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_state</span><span class="p">)</span>
<span class="n">rewards</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="nb">print</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>

<span class="n">gamma</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="mf">1e-10</span>

<span class="n">head_prob</span> <span class="o">=</span> <span class="mf">0.4</span>

<span class="n">env</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;capital_max&#39;</span><span class="p">:</span> <span class="n">capital_max</span><span class="p">,</span>
       <span class="s1">&#39;head_prob&#39;</span><span class="p">:</span> <span class="n">head_prob</span><span class="p">,</span>
       <span class="s1">&#39;rewards&#39;</span><span class="p">:</span> <span class="n">rewards</span><span class="p">,</span>
       <span class="s1">&#39;n_state&#39;</span><span class="p">:</span> <span class="n">n_state</span><span class="p">}</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we develop a function that computes optimal values based on the value iteration algorithm:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">value_iteration</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Solve the coin flipping gamble problem with value iteration algorithm</span>
<span class="sd">    @param env: the coin flipping gamble environment</span>
<span class="sd">    @param gamma: discount factor</span>
<span class="sd">    @param threshold: the evaluation will stop once values for all states are less than the threshold</span>
<span class="sd">    @return: values of the optimal policy for the given environment</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">head_prob</span> <span class="o">=</span> <span class="n">env</span><span class="p">[</span><span class="s1">&#39;head_prob&#39;</span><span class="p">]</span>
    <span class="n">n_state</span> <span class="o">=</span> <span class="n">env</span><span class="p">[</span><span class="s1">&#39;n_state&#39;</span><span class="p">]</span>
    <span class="n">capital_max</span> <span class="o">=</span> <span class="n">env</span><span class="p">[</span><span class="s1">&#39;capital_max&#39;</span><span class="p">]</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_state</span><span class="p">)</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">V_temp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_state</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">capital_max</span><span class="p">):</span>
            <span class="n">v_actions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">capital_max</span> <span class="o">-</span> <span class="n">state</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">capital_max</span> <span class="o">-</span> <span class="n">state</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">v_actions</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">head_prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">rewards</span><span class="p">[</span><span class="n">state</span> <span class="o">+</span> <span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">state</span> <span class="o">+</span> <span class="n">action</span><span class="p">])</span>
                <span class="n">v_actions</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">head_prob</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">rewards</span><span class="p">[</span><span class="n">state</span> <span class="o">-</span> <span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">state</span> <span class="o">-</span> <span class="n">action</span><span class="p">])</span>
            <span class="n">V_temp</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">v_actions</span><span class="p">)</span>
        <span class="n">max_delta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="n">V_temp</span><span class="p">))</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">V_temp</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">max_delta</span> <span class="o">&lt;=</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">V</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We only need to compute the values for states 1 to 99, as the values for state 0 and state 100+ are 0. And given state s, the possible actions can be anything from 1 up to min(s, 100 - s). We should keep this in mind while computing the Bellman optimality equation.</p>
<p>Next, we develop a function that extracts the optimal policy based on the optimal values:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">extract_optimal_policy</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">V_optimal</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Obtain the optimal policy based on the optimal values</span>
<span class="sd">    @param env: the coin flipping gamble environment</span>
<span class="sd">    @param V_optimal: optimal values</span>
<span class="sd">    @param gamma: discount factor</span>
<span class="sd">    @return: optimal policy</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">head_prob</span> <span class="o">=</span> <span class="n">env</span><span class="p">[</span><span class="s1">&#39;head_prob&#39;</span><span class="p">]</span>
    <span class="n">n_state</span> <span class="o">=</span> <span class="n">env</span><span class="p">[</span><span class="s1">&#39;n_state&#39;</span><span class="p">]</span>
    <span class="n">capital_max</span> <span class="o">=</span> <span class="n">env</span><span class="p">[</span><span class="s1">&#39;capital_max&#39;</span><span class="p">]</span>
    <span class="n">optimal_policy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">capital_max</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">capital_max</span><span class="p">):</span>
        <span class="n">v_actions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_state</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">capital_max</span> <span class="o">-</span> <span class="n">state</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">v_actions</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">head_prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">rewards</span><span class="p">[</span><span class="n">state</span> <span class="o">+</span> <span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V_optimal</span><span class="p">[</span><span class="n">state</span> <span class="o">+</span> <span class="n">action</span><span class="p">])</span>
            <span class="n">v_actions</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">head_prob</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">rewards</span><span class="p">[</span><span class="n">state</span> <span class="o">-</span> <span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V_optimal</span><span class="p">[</span><span class="n">state</span> <span class="o">-</span> <span class="n">action</span><span class="p">])</span>
        <span class="n">optimal_policy</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">v_actions</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">optimal_policy</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, we can plug in the environment, discount factor, and convergence threshold to compute the optimal values and optimal policy after . Also, we time how long it takes to solve the gamble MDP with value iteration; we will compare this with the time it takes for policy iteration to complete:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">V_optimal</span> <span class="o">=</span> <span class="n">value_iteration</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
<span class="n">optimal_policy</span> <span class="o">=</span> <span class="n">extract_optimal_policy</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">V_optimal</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;It takes </span><span class="si">{:.3f}</span><span class="s2">s to solve with value iteration&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>It takes 3.126s to solve with value iteration
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We solved the gamble problem with value iteration in 3.126 seconds.</p>
<p>Take a look at the optimal policy values and the optimal policy we got:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Optimal values:</span><span class="se">\n</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">V_optimal</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Optimal policy:</span><span class="se">\n</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">optimal_policy</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Optimal values:
tensor([0.0000, 0.0021, 0.0052, 0.0092, 0.0129, 0.0174, 0.0231, 0.0278, 0.0323,
        0.0377, 0.0435, 0.0504, 0.0577, 0.0652, 0.0695, 0.0744, 0.0807, 0.0866,
        0.0942, 0.1031, 0.1087, 0.1160, 0.1259, 0.1336, 0.1441, 0.1600, 0.1631,
        0.1677, 0.1738, 0.1794, 0.1861, 0.1946, 0.2017, 0.2084, 0.2165, 0.2252,
        0.2355, 0.2465, 0.2579, 0.2643, 0.2716, 0.2810, 0.2899, 0.3013, 0.3147,
        0.3230, 0.3339, 0.3488, 0.3604, 0.3762, 0.4000, 0.4031, 0.4077, 0.4138,
        0.4194, 0.4261, 0.4346, 0.4417, 0.4484, 0.4565, 0.4652, 0.4755, 0.4865,
        0.4979, 0.5043, 0.5116, 0.5210, 0.5299, 0.5413, 0.5547, 0.5630, 0.5740,
        0.5888, 0.6004, 0.6162, 0.6400, 0.6446, 0.6516, 0.6608, 0.6690, 0.6791,
        0.6919, 0.7026, 0.7126, 0.7248, 0.7378, 0.7533, 0.7697, 0.7868, 0.7965,
        0.8075, 0.8215, 0.8349, 0.8520, 0.8721, 0.8845, 0.9009, 0.9232, 0.9406,
        0.9643, 0.0000])
Optimal policy:
tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,  9, 17,
        18,  6,  5, 21,  3,  2,  1, 25,  1,  2,  3, 29,  5,  6,  7,  8,  9, 35,
        36, 12, 12, 11, 10,  9,  8,  7, 44,  5,  4,  3,  2,  1, 50,  1,  2,  3,
         4,  5,  6,  7,  8,  9, 10, 11, 12, 12, 11, 10,  9,  8,  7,  6,  5,  4,
         3,  2,  1, 25,  1,  2,  3, 21,  5, 19,  7,  8, 16, 15, 14, 12, 12, 11,
        10,  9,  8,  7,  6,  5,  4,  3,  2,  1], dtype=torch.int32)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can plot the policy value versus state as follows:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">V_optimal</span><span class="p">[:</span><span class="mi">100</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Optimal policy values&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Capital&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Policy value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">capital_max</span><span class="p">),</span> <span class="n">optimal_policy</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">capital_max</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Optimal policy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Capital&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Optimal action&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we've solved the gamble problem with value iteration, how about policy iteration? Let's see.</p>
<p>We start by developing the policy_evaluation function that computes the values given a policy:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">policy_evaluation</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Perform policy evaluation</span>
<span class="sd">    @param env: the coin flipping gamble environment</span>
<span class="sd">    @param policy: policy tensor containing actions taken for individual state</span>
<span class="sd">    @param gamma: discount factor</span>
<span class="sd">    @param threshold: the evaluation will stop once values for all states are less than the threshold</span>
<span class="sd">    @return: values of the given policy</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">head_prob</span> <span class="o">=</span> <span class="n">env</span><span class="p">[</span><span class="s1">&#39;head_prob&#39;</span><span class="p">]</span>
    <span class="n">n_state</span> <span class="o">=</span> <span class="n">env</span><span class="p">[</span><span class="s1">&#39;n_state&#39;</span><span class="p">]</span>
    <span class="n">capital_max</span> <span class="o">=</span> <span class="n">env</span><span class="p">[</span><span class="s1">&#39;capital_max&#39;</span><span class="p">]</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_state</span><span class="p">)</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">V_temp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_state</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">capital_max</span><span class="p">):</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">policy</span><span class="p">[</span><span class="n">state</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">V_temp</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+=</span> <span class="n">head_prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">rewards</span><span class="p">[</span><span class="n">state</span> <span class="o">+</span> <span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">state</span> <span class="o">+</span> <span class="n">action</span><span class="p">])</span>
            <span class="n">V_temp</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">head_prob</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">rewards</span><span class="p">[</span><span class="n">state</span> <span class="o">-</span> <span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">state</span> <span class="o">-</span> <span class="n">action</span><span class="p">])</span>
        <span class="n">max_delta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">V</span> <span class="o">-</span> <span class="n">V_temp</span><span class="p">))</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">V_temp</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">max_delta</span> <span class="o">&lt;=</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">V</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next, we develop another main component of the policy iteration algorithm, the policy improvement part:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">policy_improvement</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Obtain an improved policy based on the values</span>
<span class="sd">    @param env: the coin flipping gamble environment</span>
<span class="sd">    @param V: policy values</span>
<span class="sd">    @param gamma: discount factor</span>
<span class="sd">    @return: the policy</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">head_prob</span> <span class="o">=</span> <span class="n">env</span><span class="p">[</span><span class="s1">&#39;head_prob&#39;</span><span class="p">]</span>
    <span class="n">n_state</span> <span class="o">=</span> <span class="n">env</span><span class="p">[</span><span class="s1">&#39;n_state&#39;</span><span class="p">]</span>
    <span class="n">capital_max</span> <span class="o">=</span> <span class="n">env</span><span class="p">[</span><span class="s1">&#39;capital_max&#39;</span><span class="p">]</span>
    <span class="n">policy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_state</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">capital_max</span><span class="p">):</span>
        <span class="n">v_actions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">capital_max</span> <span class="o">-</span> <span class="n">state</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">capital_max</span> <span class="o">-</span> <span class="n">state</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">v_actions</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">head_prob</span> <span class="o">*</span> <span class="p">(</span><span class="n">rewards</span><span class="p">[</span><span class="n">state</span> <span class="o">+</span> <span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">state</span> <span class="o">+</span> <span class="n">action</span><span class="p">])</span>
            <span class="n">v_actions</span><span class="p">[</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">head_prob</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">rewards</span><span class="p">[</span><span class="n">state</span> <span class="o">-</span> <span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">state</span> <span class="o">-</span> <span class="n">action</span><span class="p">])</span>
        <span class="n">policy</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">v_actions</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">policy</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With both components ready, we can develop the main entry to the policy iteration algorithm as follows:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">policy_iteration</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Solve the coin flipping gamble problem with policy iteration algorithm</span>
<span class="sd">    @param env: the coin flipping gamble environment</span>
<span class="sd">    @param gamma: discount factor</span>
<span class="sd">    @param threshold: the evaluation will stop once values for all states are less than the threshold</span>
<span class="sd">    @return: optimal values and the optimal policy for the given environment</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">n_state</span> <span class="o">=</span> <span class="n">env</span><span class="p">[</span><span class="s1">&#39;n_state&#39;</span><span class="p">]</span>
    <span class="n">policy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_state</span><span class="p">)</span><span class="o">.</span><span class="n">int</span><span class="p">()</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">V</span> <span class="o">=</span> <span class="n">policy_evaluation</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">policy</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
        <span class="n">policy_improved</span> <span class="o">=</span> <span class="n">policy_improvement</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">policy_improved</span><span class="p">,</span> <span class="n">policy</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">V</span><span class="p">,</span> <span class="n">policy_improved</span>
        <span class="n">policy</span> <span class="o">=</span> <span class="n">policy_improved</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, we plug in the environment, discount factor, and convergence threshold to compute the optimal values and the optimal policy. We record the time spent solving the MDP as well:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">V_optimal</span><span class="p">,</span> <span class="n">optimal_policy</span> <span class="o">=</span> <span class="n">policy_iteration</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;It takes </span><span class="si">{:.3f}</span><span class="s2">s to solve with policy iteration&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>It takes 2.951s to solve with policy iteration
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Check out the optimal values and optimal policy we just obtained:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Optimal values:</span><span class="se">\n</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">V_optimal</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Optimal policy:</span><span class="se">\n</span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">optimal_policy</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Optimal values:
tensor([0.0000, 0.0021, 0.0052, 0.0092, 0.0129, 0.0174, 0.0231, 0.0278, 0.0323,
        0.0377, 0.0435, 0.0504, 0.0577, 0.0652, 0.0695, 0.0744, 0.0807, 0.0866,
        0.0942, 0.1031, 0.1087, 0.1160, 0.1259, 0.1336, 0.1441, 0.1600, 0.1631,
        0.1677, 0.1738, 0.1794, 0.1861, 0.1946, 0.2017, 0.2084, 0.2165, 0.2252,
        0.2355, 0.2465, 0.2579, 0.2643, 0.2716, 0.2810, 0.2899, 0.3013, 0.3147,
        0.3230, 0.3339, 0.3488, 0.3604, 0.3762, 0.4000, 0.4031, 0.4077, 0.4138,
        0.4194, 0.4261, 0.4346, 0.4417, 0.4484, 0.4565, 0.4652, 0.4755, 0.4865,
        0.4979, 0.5043, 0.5116, 0.5210, 0.5299, 0.5413, 0.5547, 0.5630, 0.5740,
        0.5888, 0.6004, 0.6162, 0.6400, 0.6446, 0.6516, 0.6608, 0.6690, 0.6791,
        0.6919, 0.7026, 0.7126, 0.7248, 0.7378, 0.7533, 0.7697, 0.7868, 0.7965,
        0.8075, 0.8215, 0.8349, 0.8520, 0.8721, 0.8845, 0.9009, 0.9232, 0.9406,
        0.9643, 0.0000])
Optimal policy:
tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,  9, 17,
        18,  6,  5, 21,  3,  2,  1, 25,  1,  2,  3, 29,  5,  6,  7,  8,  9, 35,
        36, 12, 12, 11, 10,  9,  8,  7, 44,  5,  4,  3,  2,  1, 50,  1,  2,  3,
         4,  5,  6,  7,  8,  9, 10, 11, 12, 12, 11, 10,  9,  8,  7,  6,  5,  4,
         3,  2,  1, 25,  1,  2,  3, 21,  5, 19,  7,  8, 16, 15, 14, 12, 12, 11,
        10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0], dtype=torch.int32)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The results from the two approaches, value iteration and policy iteration, are consistent.</p>
<p>We have solved the gamble problem by using value iteration and policy iteration. To deal with a reinforcement learning problem, one of the trickiest tasks is to formulate the process into an MDP. In our case, the policy is transformed from the current capital (states) to the new capital (new states) by betting certain stakes (actions). The optimal policy maximizes the probability of winning the game (+1 reward), and evaluates the probability of winning under the optimal policy.</p>
<p>Another interesting thing to note is how the transformation probabilities and new states are determined in the Bellman equation in our example. Taking action a in state s (having capital s and making a bet of 1 dollar) will have two possible outcomes:</p>
<ul>
<li>Moving to new state s+a, if the coin lands on heads. Hence, the transformation probability is equal to the probability of heads.</li>
<li>Moving to new state s-a, if the coin lands on tails. Therefore, the transformation probability is equal to the probability of tails.
This is quite similar to the FrozenLake environment, where the agent lands on the intended tile only by a certain probability.</li>
</ul>
<p>We also verified that policy iteration converges faster than value iteration in this case. This is because there are up to 50 possible actions, which is more than the 4 actions in FrozenLake. For MDPs with a large number of actions, solving with policy iteration is more efficient than doing so with value iteration.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>You may want to know whether the optimal policy really works. Let's act like smart gamblers and play 10,000 episodes of the game. We are going to compare the optimal policy with two other strategies: conservative (betting one dollar each round) and random (betting a random amount):</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="k">def</span> <span class="nf">run_random_episode</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">capital</span><span class="p">):</span>
    <span class="k">while</span> <span class="n">capital</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># print(capital)</span>
        <span class="c1"># bet = torch.randint(1, capital + 1, (1,)).item()</span>
        <span class="n">bet</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">head</span><span class="p">:</span>
            <span class="n">capital</span> <span class="o">+=</span> <span class="n">bet</span>
            <span class="k">if</span> <span class="n">capital</span> <span class="o">&gt;=</span> <span class="mi">100</span><span class="p">:</span>
                <span class="k">return</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">capital</span> <span class="o">-=</span> <span class="n">bet</span>
    <span class="k">return</span> <span class="mi">0</span>


<span class="k">def</span> <span class="nf">run_optimal_episode</span><span class="p">(</span><span class="n">head</span><span class="p">,</span> <span class="n">capital</span><span class="p">,</span> <span class="n">optimal_policy</span><span class="p">):</span>
    <span class="k">while</span> <span class="n">capital</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">bet</span> <span class="o">=</span> <span class="n">optimal_policy</span><span class="p">[</span><span class="n">capital</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">head</span><span class="p">:</span>
            <span class="n">capital</span> <span class="o">+=</span> <span class="n">bet</span>
            <span class="k">if</span> <span class="n">capital</span> <span class="o">&gt;=</span> <span class="mi">100</span><span class="p">:</span>
                <span class="k">return</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">capital</span> <span class="o">-=</span> <span class="n">bet</span>
    <span class="k">return</span> <span class="mi">0</span>


<span class="n">capital</span> <span class="o">=</span> <span class="mi">50</span>

<span class="n">n_episode</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">total_rewards_random</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">total_rewards_opt</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_episode</span><span class="p">):</span>
    <span class="n">total_reward_random</span> <span class="o">=</span> <span class="n">run_random_episode</span><span class="p">(</span><span class="mf">0.48</span><span class="p">,</span> <span class="n">capital</span><span class="p">)</span>
    <span class="n">total_reward_opt</span> <span class="o">=</span> <span class="n">run_optimal_episode</span><span class="p">(</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">capital</span><span class="p">,</span> <span class="n">optimal_policy</span><span class="p">)</span>
    <span class="n">total_rewards_random</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_reward_random</span><span class="p">)</span>
    <span class="n">total_rewards_opt</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_reward_opt</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-python"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Average total reward under the random policy: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">total_rewards_random</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_episode</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Average total reward under the optimal policy: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">total_rewards_opt</span><span class="p">)</span> <span class="o">/</span> <span class="n">n_episode</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Average total reward under the random policy: 0.0152
Average total reward under the optimal policy: 0.4014
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Our optimal policy is clearly the winner!</p>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/2022/01/19/mdp-dynamic.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Jupyter notebook database.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/recohut" target="_blank" title="recohut"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
