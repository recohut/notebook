{
  
    
        "post0": {
            "title": "Vehicle Detection",
            "content": "!pip install sk-video&gt;=1.1.8 import os if not os.path.exists(&#39;road.mp4&#39;): !wget https://learnml.s3.eu-north-1.amazonaws.com/road.mp4 . --2020-01-10 13:06:41-- https://learnml.s3.eu-north-1.amazonaws.com/road.mp4 Resolving learnml.s3.eu-north-1.amazonaws.com (learnml.s3.eu-north-1.amazonaws.com)... 52.95.169.4 Connecting to learnml.s3.eu-north-1.amazonaws.com (learnml.s3.eu-north-1.amazonaws.com)|52.95.169.4|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 191816449 (183M) [video/mp4] Saving to: ‘road.mp4’ road.mp4 100%[===================&gt;] 182.93M 18.9MB/s in 11s 2020-01-10 13:06:53 (16.6 MB/s) - ‘road.mp4’ saved [191816449/191816449] . Tutorial: Making Road Traffic Counting App based on Computer Vision and OpenCV . Today we will learn how to count road traffic based on computer vision and without heavy deep learning algorithms. . For this tutorial, we will use only Python and OpenCV with the pretty simple idea of motion detection with help of background subtraction algorithm. . Here is our plan: . Understand the main idea of background subtraction algorithms that used for foreground detection. | OpenCV image filters. | Object detection by contours. | Building processing pipeline for further data manipulation. | import os import csv import numpy as np import logging import logging.handlers import math import sys import random import numpy as np import skvideo.io import cv2 import matplotlib.pyplot as plt from IPython.display import HTML from base64 import b64encode # without this some strange errors happen cv2.ocl.setUseOpenCL(False) random.seed(123) # setup logging def init_logging(level=logging.INFO): main_logger = logging.getLogger() for hnd in main_logger.handlers: main_logger.removeHandler(hnd) formatter = logging.Formatter( fmt=&#39;%(asctime)s.%(msecs)03d %(levelname)-8s [%(name)s] %(message)s&#39;, datefmt=&#39;%Y-%m-%d %H:%M:%S&#39;) handler_stream = logging.StreamHandler(sys.stdout) handler_stream.setFormatter(formatter) main_logger.addHandler(handler_stream) main_logger.setLevel(level) return main_logger . Background subtraction algorithms . There are many different algorithms for background subtraction, but the main idea of them is very simple. . Let’s assume that you have a video of your room, and on some of the frames of this video there is no humans &amp; pets, so basically it’s static, let’s call it background_layer. So to get objects that are moving on the video we just need to: . foreground_objects = current_frame - background_layer . But in some cases, we cant get static frame because lighting can change, or some objects will be moved by someone, or always exist movement, etc. In such cases we are saving some number of frames and trying to figure out which of the pixels are the same for most of them, then this pixels becoming part of background_layer. Difference generally in how we get this background_layer and additional filtering that we use to make selection more accurate. . In this lesson, we will use MOG algorithm for background subtraction and after processing, it looks like this: . . def train_bg_subtractor(inst, cap, num=500): &#39;&#39;&#39; BG substractor need process some amount of frames to start giving result &#39;&#39;&#39; print (&#39;Training BG Subtractor...&#39;) i = 0 for frame in cap: inst.apply(frame, None, 0.001) i += 1 if i &gt;= num: return cap VIDEO_SOURCE = &quot;road.mp4&quot; bg_subtractor = cv2.createBackgroundSubtractorMOG2( history=500, detectShadows=True) # Set up image source cap = skvideo.io.vreader(VIDEO_SOURCE) # skipping 500 frames to train bg subtractor train_bg_subtractor(bg_subtractor, cap, num=500) frame = next(cap) fg_mask = bg_subtractor.apply(frame, None, 0.001) plt.figure(figsize=(12,12)) plt.imshow(fg_mask) plt.show() . Training BG Subtractor... . As you can see there is some noise on the foreground mask which we will try to remove with some standard filtering technic. . Filtering . For our case we will need this filters: Threshold, Erode, Dilate, Opening, Closing. Please go by links and read about each of them and look how they work (to not make copy/paste) . So now we will use them to remove some noise on foreground mask. First, we will use Closing to remove gaps in areas, then Opening to remove 1–2 px points, and after that dilation to make object bolder. . def filter_mask(img): &#39;&#39;&#39; This filters are hand-picked just based on visual tests &#39;&#39;&#39; kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2)) # Fill any small holes closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel) # Remove noise opening = cv2.morphologyEx(closing, cv2.MORPH_OPEN, kernel) # Dilate to merge adjacent blobs dilation = cv2.dilate(opening, kernel, iterations=2) return dilation bg_subtractor = cv2.createBackgroundSubtractorMOG2( history=500, detectShadows=True) # Set up image source cap = skvideo.io.vreader(VIDEO_SOURCE) # skipping 500 frames to train bg subtractor train_bg_subtractor(bg_subtractor, cap, num=500) frame = next(cap) fg_mask = bg_subtractor.apply(frame, None, 0.001) fg_mask[fg_mask &lt; 240] = 0 fg_mask = filter_mask(fg_mask) plt.figure(figsize=(12,12)) plt.imshow(fg_mask) plt.show() . Training BG Subtractor... . Object detection by contours . For this purpose we will use the standard cv2.findContours method with params: . cv2.CV_RETR_EXTERNAL — get only outer contours. cv2.CV_CHAIN_APPROX_TC89_L1 - use Teh-Chin chain approximation algorithm (faster) . def get_centroid(x, y, w, h): x1 = int(w / 2) y1 = int(h / 2) cx = x + x1 cy = y + y1 return (cx, cy) class ContourDetection: &#39;&#39;&#39; Detecting moving objects. Purpose of this processor is to subtrac background, get moving objects and detect them with a cv2.findContours method, and then filter off-by width and height. bg_subtractor - background subtractor isinstance. min_contour_width - min bounding rectangle width. min_contour_height - min bounding rectangle height. save_image - if True will save detected objects mask to file. image_dir - where to save images(must exist). &#39;&#39;&#39; def __init__(self, bg_subtractor, min_contour_width=35, min_contour_height=35, save_image=False, image_dir=&#39;images&#39;): super(ContourDetection, self).__init__() self.bg_subtractor = bg_subtractor self.min_contour_width = min_contour_width self.min_contour_height = min_contour_height self.save_image = save_image self.image_dir = image_dir def filter_mask(self, img, a=None): &#39;&#39;&#39; This filters are hand-picked just based on visual tests &#39;&#39;&#39; kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2)) # Fill any small holes closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel) # Remove noise opening = cv2.morphologyEx(closing, cv2.MORPH_OPEN, kernel) # Dilate to merge adjacent blobs dilation = cv2.dilate(opening, kernel, iterations=2) return dilation def detect_vehicles(self, fg_mask): matches = [] # finding external contours contours, hierarchy = cv2.findContours( fg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_L1) for (i, contour) in enumerate(contours): (x, y, w, h) = cv2.boundingRect(contour) # On the exit, we add some filtering by height, width and add centroid. contour_valid = (w &gt;= self.min_contour_width) and ( h &gt;= self.min_contour_height) if not contour_valid: continue centroid = get_centroid(x, y, w, h) matches.append(((x, y, w, h), centroid)) return matches def __call__(self, frame): frame = frame.copy() fg_mask = self.bg_subtractor.apply(frame, None, 0.001) # just thresholding values fg_mask[fg_mask &lt; 240] = 0 fg_mask = self.filter_mask(fg_mask, 0) return self.detect_vehicles(fg_mask) cd = ContourDetection(bg_subtractor) bg_subtractor = cv2.createBackgroundSubtractorMOG2( history=500, detectShadows=True) # Set up image source cap = skvideo.io.vreader(VIDEO_SOURCE) # skipping 500 frames to train bg subtractor train_bg_subtractor(bg_subtractor, cap, num=500) frame = next(cap) objects = cd(frame) print(&#39;Getting list of [((x,y,w,h), (xc,yc)), ...]&#39;) print(objects) . Training BG Subtractor... Getting list of [((x,y,w,h), (xc,yc)), ...] [((344, 650, 120, 70), (404, 685)), ((1009, 598, 189, 122), (1103, 659)), ((1171, 581, 109, 139), (1225, 650)), ((194, 532, 131, 103), (259, 583)), ((395, 406, 61, 51), (425, 431)), ((228, 393, 142, 104), (299, 445)), ((494, 387, 68, 59), (528, 416)), ((856, 364, 55, 47), (883, 387)), ((460, 356, 49, 35), (484, 373)), ((752, 317, 79, 64), (791, 349)), ((508, 283, 54, 41), (535, 303)), ((594, 239, 52, 50), (620, 264))] . Building processing pipeline . You must understand that in ML and CV there is no one magic algorithm that making altogether, even if we imagine that such algorithm exists, we still wouldn’t use it because it would be not effective at scale. For example a few years ago Netflix created competition with the prize 3 million dollars for the best movie recommendation algorithm. And one of the team created such, problem was that it just couldn’t work at scale and thus was useless for the company. But still, Netflix paid 1 million to them :) . So now we will build simple processing pipeline, it not for scale just for convenient but the idea the same. . class PipelineRunner(object): &#39;&#39;&#39; Very simple pipline. Just run passed processors in order with passing context from one to another. You can also set log level for processors. &#39;&#39;&#39; def __init__(self, pipeline=None, log_level=logging.INFO): self.pipeline = pipeline or [] self.context = {} self.log = logging.getLogger(self.__class__.__name__) self.log.setLevel(log_level) self.log_level = log_level self.set_log_level() def set_context(self, data): self.context = data def add(self, processor): if not isinstance(processor, PipelineProcessor): raise Exception( &#39;Processor should be an isinstance of PipelineProcessor.&#39;) processor.log.setLevel(self.log_level) self.pipeline.append(processor) def remove(self, name): for i, p in enumerate(self.pipeline): if p.__class__.__name__ == name: del self.pipeline[i] return True return False def set_log_level(self): for p in self.pipeline: p.log.setLevel(self.log_level) def run(self): for p in self.pipeline: self.context = p(self.context) self.log.debug(&quot;Frame #%d processed.&quot;, self.context[&#39;frame_number&#39;]) return self.context class PipelineProcessor(object): &#39;&#39;&#39; Base class for processors. &#39;&#39;&#39; def __init__(self): self.log = logging.getLogger(self.__class__.__name__) . As input constructor will take a list of processors that will be run in order. Each processor making part of the job. . We already have Countour Detection class, just need slightly udate it to use context . def save_frame(frame, file_name, flip=True): # flip BGR to RGB if flip: cv2.imwrite(file_name, np.flip(frame, 2)) else: cv2.imwrite(file_name, frame) class ContourDetection(PipelineProcessor): &#39;&#39;&#39; Detecting moving objects. Purpose of this processor is to subtrac background, get moving objects and detect them with a cv2.findContours method, and then filter off-by width and height. bg_subtractor - background subtractor isinstance. min_contour_width - min bounding rectangle width. min_contour_height - min bounding rectangle height. save_image - if True will save detected objects mask to file. image_dir - where to save images(must exist). &#39;&#39;&#39; def __init__(self, bg_subtractor, min_contour_width=35, min_contour_height=35, save_image=False, image_dir=&#39;images&#39;): super(ContourDetection, self).__init__() self.bg_subtractor = bg_subtractor self.min_contour_width = min_contour_width self.min_contour_height = min_contour_height self.save_image = save_image self.image_dir = image_dir def filter_mask(self, img, a=None): &#39;&#39;&#39; This filters are hand-picked just based on visual tests &#39;&#39;&#39; kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2, 2)) # Fill any small holes closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel) # Remove noise opening = cv2.morphologyEx(closing, cv2.MORPH_OPEN, kernel) # Dilate to merge adjacent blobs dilation = cv2.dilate(opening, kernel, iterations=2) return dilation def detect_vehicles(self, fg_mask, context): matches = [] # finding external contours contours, hierarchy = cv2.findContours( fg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_L1) for (i, contour) in enumerate(contours): (x, y, w, h) = cv2.boundingRect(contour) contour_valid = (w &gt;= self.min_contour_width) and ( h &gt;= self.min_contour_height) if not contour_valid: continue centroid = get_centroid(x, y, w, h) matches.append(((x, y, w, h), centroid)) return matches def __call__(self, context): frame = context[&#39;frame&#39;].copy() frame_number = context[&#39;frame_number&#39;] fg_mask = self.bg_subtractor.apply(frame, None, 0.001) # just thresholding values fg_mask[fg_mask &lt; 240] = 0 fg_mask = self.filter_mask(fg_mask, frame_number) if self.save_image: save_frame(fg_mask, self.image_dir + &quot;/mask_%04d.png&quot; % frame_number, flip=False) context[&#39;objects&#39;] = self.detect_vehicles(fg_mask, context) context[&#39;fg_mask&#39;] = fg_mask return context . Now let’s create a processor that will link detected objects on different frames and will create paths, and also will count vehicles that got to the exit zone. . def distance(x, y, type=&#39;euclidian&#39;, x_weight=1.0, y_weight=1.0): if type == &#39;euclidian&#39;: return math.sqrt(float((x[0] - y[0])**2) / x_weight + float((x[1] - y[1])**2) / y_weight) class VehicleCounter(PipelineProcessor): &#39;&#39;&#39; Counting vehicles that entered in exit zone. Purpose of this class based on detected object and local cache create objects pathes and count that entered in exit zone defined by exit masks. exit_masks - list of the exit masks. path_size - max number of points in a path. max_dst - max distance between two points. &#39;&#39;&#39; def __init__(self, exit_masks=[], path_size=10, max_dst=30, x_weight=1.0, y_weight=1.0): super(VehicleCounter, self).__init__() self.exit_masks = exit_masks self.vehicle_count = 0 self.path_size = path_size self.pathes = [] self.max_dst = max_dst self.x_weight = x_weight self.y_weight = y_weight def check_exit(self, point): for exit_mask in self.exit_masks: try: if exit_mask[point[1]][point[0]] == 255: return True except: return True return False def __call__(self, context): objects = context[&#39;objects&#39;] context[&#39;exit_masks&#39;] = self.exit_masks context[&#39;pathes&#39;] = self.pathes context[&#39;vehicle_count&#39;] = self.vehicle_count if not objects: return context points = np.array(objects)[:, 0:2] points = points.tolist() # add new points if pathes is empty if not self.pathes: for match in points: self.pathes.append([match]) else: # link new points with old pathes based on minimum distance between # points new_pathes = [] for path in self.pathes: _min = 999999 _match = None for p in points: if len(path) == 1: # distance from last point to current d = distance(p[0], path[-1][0]) else: # based on 2 prev points predict next point and calculate # distance from predicted next point to current xn = 2 * path[-1][0][0] - path[-2][0][0] yn = 2 * path[-1][0][1] - path[-2][0][1] d = distance( p[0], (xn, yn), x_weight=self.x_weight, y_weight=self.y_weight ) if d &lt; _min: _min = d _match = p if _match and _min &lt;= self.max_dst: points.remove(_match) path.append(_match) new_pathes.append(path) # do not drop path if current frame has no matches if _match is None: new_pathes.append(path) self.pathes = new_pathes # add new pathes if len(points): for p in points: # do not add points that already should be counted if self.check_exit(p[1]): continue self.pathes.append([p]) # save only last N points in path for i, _ in enumerate(self.pathes): self.pathes[i] = self.pathes[i][self.path_size * -1:] # count vehicles and drop counted pathes: new_pathes = [] for i, path in enumerate(self.pathes): d = path[-2:] if ( # need at list two points to count len(d) &gt;= 2 and # prev point not in exit zone not self.check_exit(d[0][1]) and # current point in exit zone self.check_exit(d[1][1]) and # path len is bigger then min self.path_size &lt;= len(path) ): self.vehicle_count += 1 else: # prevent linking with path that already in exit zone add = True for p in path: if self.check_exit(p[1]): add = False break if add: new_pathes.append(path) self.pathes = new_pathes context[&#39;pathes&#39;] = self.pathes context[&#39;objects&#39;] = objects context[&#39;vehicle_count&#39;] = self.vehicle_count self.log.debug(&#39;#VEHICLES FOUND: %s&#39; % self.vehicle_count) return context . This class a bit complicated so let’s walk through it by parts. . . This green mask on the image is exit zone, is where we counting our vehicles. For example, we will count only paths that have length more than 3 points(to remove some noise) and the 4th in the green zone. We use masks cause it’s many operation effective and simpler than using vector algorithms. . Just use “binary and” operation to check that point in the area, and that’s all. And here is how we set it: . EXIT_PTS = np.array([ [[732, 720], [732, 590], [1280, 500], [1280, 720]], [[0, 400], [645, 400], [645, 0], [0, 0]] ]) SHAPE = (720,1280) base = np.zeros(SHAPE + (3,), dtype=&#39;uint8&#39;) exit_mask = cv2.fillPoly(base, EXIT_PTS, (255, 255, 255))[:, :, 0] plt.imshow(base) plt.show() . Now let’s link points in paths at line 55 . On first frame. we just add all points as new paths. . Next if len(path) == 1, for each path in the cache we are trying to find the point(centroid) from newly detected objects which will have the smallest Euclidean distance to the last point of the path. . If len(path) &gt; 1, then with the last two points in the path we are predicting new point on the same line, and finding min distance between it and the current point. . The point with minimal distance added to the end of the current path and removed from the list. . If some points left after this we add them as new paths. . And also we limit the number of points in the path at line 101 . Now we will try to count vehicles that entering in the exit zone. To do this we just take 2 last points in the path and checking that last of them in exit zone, and previous not, and also checking that len(path) should be bigger than limit. The part after else is preventing of back-linking new points to the points in exit zone. . And the last two processor is CSV writer to create report CSV file, and visualization for debugging and nice pictures/videos. . class CsvWriter(PipelineProcessor): def __init__(self, path, name, start_time=0, fps=15): super(CsvWriter, self).__init__() self.fp = open(os.path.join(path, name), &#39;w&#39;) self.writer = csv.DictWriter(self.fp, fieldnames=[&#39;time&#39;, &#39;vehicles&#39;]) self.writer.writeheader() self.start_time = start_time self.fps = fps self.path = path self.name = name self.prev = None def __call__(self, context): frame_number = context[&#39;frame_number&#39;] count = _count = context[&#39;vehicle_count&#39;] if self.prev: _count = count - self.prev time = ((self.start_time + int(frame_number / self.fps)) * 100 + int(100.0 / self.fps) * (frame_number % self.fps)) self.writer.writerow({&#39;time&#39;: time, &#39;vehicles&#39;: _count}) self.prev = count return context BOUNDING_BOX_COLOUR = (255, 192, 0) CENTROID_COLOUR = (255, 192, 0) CAR_COLOURS = [(255, 192, 0)] EXIT_COLOR = (66, 183, 42) class Visualizer(PipelineProcessor): def __init__(self, save_image=True, image_dir=&#39;images&#39;): super(Visualizer, self).__init__() self.save_image = save_image self.image_dir = image_dir def check_exit(self, point, exit_masks=[]): for exit_mask in exit_masks: if exit_mask[point[1]][point[0]] == 255: return True return False def draw_pathes(self, img, pathes): if not img.any(): return for i, path in enumerate(pathes): path = np.array(path)[:, 1].tolist() for point in path: cv2.circle(img, point, 2, CAR_COLOURS[0], -1) cv2.polylines(img, [np.int32(path)], False, CAR_COLOURS[0], 1) return img def draw_boxes(self, img, pathes, exit_masks=[]): for (i, match) in enumerate(pathes): contour, centroid = match[-1][:2] if self.check_exit(centroid, exit_masks): continue x, y, w, h = contour cv2.rectangle(img, (x, y), (x + w - 1, y + h - 1), BOUNDING_BOX_COLOUR, 1) cv2.circle(img, centroid, 2, CENTROID_COLOUR, -1) return img def draw_ui(self, img, vehicle_count, exit_masks=[]): # this just add green mask with opacity to the image for exit_mask in exit_masks: _img = np.zeros(img.shape, img.dtype) _img[:, :] = EXIT_COLOR mask = cv2.bitwise_and(_img, _img, mask=exit_mask) cv2.addWeighted(mask, 1, img, 1, 0, img) # drawing top block with counts cv2.rectangle(img, (0, 0), (img.shape[1], 50), (0, 0, 0), cv2.FILLED) cv2.putText(img, (&quot;Vehicles passed: {total} &quot;.format(total=vehicle_count)), (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 1) return img def __call__(self, context): frame = context[&#39;frame&#39;].copy() frame = np.ascontiguousarray(np.flip(frame, 2)) frame_number = context[&#39;frame_number&#39;] pathes = context[&#39;pathes&#39;] exit_masks = context[&#39;exit_masks&#39;] vehicle_count = context[&#39;vehicle_count&#39;] frame = self.draw_ui(frame, vehicle_count, exit_masks) frame = self.draw_pathes(frame, pathes) frame = self.draw_boxes(frame, pathes, exit_masks) if self.save_image: save_frame(frame, self.image_dir + &quot;/processed_%04d.png&quot; % frame_number) context[&#39;frame&#39;] = frame return context . CSV writer is saving data by time, cause we need it for further analytics. So i use this formula to add additional frame timing to the unixtimestamp: . time = ((self.start_time + int(frame_number / self.fps)) * 100 + int(100.0 / self.fps) * (frame_number % self.fps)) . so with start time=1 000 000 000 and fps=10 i will get results like this frame 1 = 1 000 000 000 010 frame 1 = 1 000 000 000 020 … . Then after you get full csv report you can aggregate this data as you want. . Conclusion . So as you see it was not so hard as many people think. . But if you run the script you will see that this solution is not ideal, and having a problem with foreground objects overlapping, also it doesn’t have vehicles classification by types(that you will definitely need for real analytics). But still, with good camera position(above the road), it gives pretty good accuracy. And that tells us that even small &amp; simple algorithms used in a right way can give good results. . So what we can do to fix current issues? . One way is to try adding some additional filtration trying to separate objects for better detection. Another is to use more complex algorithms like deep convolution networks, which u can study on our course http://learnml.today . def main(): log = logging.getLogger(&quot;main&quot;) # creating exit mask from points, where we will be counting our vehicles base = np.zeros(SHAPE + (3,), dtype=&#39;uint8&#39;) exit_mask = cv2.fillPoly(base, EXIT_PTS, (255, 255, 255))[:, :, 0] # there is also bgslibrary, that seems to give better BG substruction, but # not tested it yet bg_subtractor = cv2.createBackgroundSubtractorMOG2( history=500, detectShadows=True) # processing pipline for programming conviniance pipeline = PipelineRunner(pipeline=[ ContourDetection(bg_subtractor=bg_subtractor, save_image=True, image_dir=IMAGE_DIR), # we use y_weight == 2.0 because traffic are moving vertically on video # use x_weight == 2.0 for horizontal. VehicleCounter(exit_masks=[exit_mask], y_weight=2.0), Visualizer(image_dir=IMAGE_DIR,save_image=False), CsvWriter(path=&#39;./&#39;, name=&#39;report.csv&#39;) ], log_level=logging.INFO) # Set up image source cap = skvideo.io.vreader(VIDEO_SOURCE) # skipping 500 frames to train bg subtractor train_bg_subtractor(bg_subtractor, cap, num=500) fourcc = cv2.VideoWriter_fourcc(*&quot;MP4V&quot;) writer = cv2.VideoWriter(VIDEO_OUT, fourcc, 25, (SHAPE[1], SHAPE[0]), True) frame_number = -1 for frame in cap: if not frame.any(): log.error(&quot;Frame capture failed, stopping...&quot;) break frame_number += 1 log.info(&quot;Frame #%s&quot; % frame_number) pipeline.set_context({ &#39;frame&#39;: frame, &#39;frame_number&#39;: frame_number, }) ctx = pipeline.run() writer.write(ctx[&#39;frame&#39;]) if frame_number &gt; PARSE_FRAMES: break writer.release() . # ============================================================================ IMAGE_DIR = &quot;./out&quot; VIDEO_SOURCE = &quot;road.mp4&quot; VIDEO_OUT = &quot;road_parsed.mp4&quot; PARSE_FRAMES = 15*25 SHAPE = (720, 1280) # HxW EXIT_PTS = np.array([ [[732, 720], [732, 590], [1280, 500], [1280, 720]], [[0, 400], [645, 400], [645, 0], [0, 0]] ]) # ============================================================================ log = init_logging() main() . Training BG Subtractor... 2020-01-10 13:12:10.439 INFO [main] Frame #0 2020-01-10 13:12:10.549 INFO [main] Frame #1 2020-01-10 13:12:10.628 INFO [main] Frame #2 2020-01-10 13:12:10.702 INFO [main] Frame #3 2020-01-10 13:12:10.773 INFO [main] Frame #4 2020-01-10 13:12:10.844 INFO [main] Frame #5 2020-01-10 13:12:10.915 INFO [main] Frame #6 2020-01-10 13:12:10.991 INFO [main] Frame #7 2020-01-10 13:12:11.068 INFO [main] Frame #8 2020-01-10 13:12:11.141 INFO [main] Frame #9 2020-01-10 13:12:11.212 INFO [main] Frame #10 2020-01-10 13:12:11.284 INFO [main] Frame #11 2020-01-10 13:12:11.354 INFO [main] Frame #12 2020-01-10 13:12:11.427 INFO [main] Frame #13 2020-01-10 13:12:11.499 INFO [main] Frame #14 2020-01-10 13:12:11.571 INFO [main] Frame #15 2020-01-10 13:12:11.640 INFO [main] Frame #16 2020-01-10 13:12:11.711 INFO [main] Frame #17 2020-01-10 13:12:11.783 INFO [main] Frame #18 2020-01-10 13:12:11.854 INFO [main] Frame #19 2020-01-10 13:12:11.926 INFO [main] Frame #20 2020-01-10 13:12:12.001 INFO [main] Frame #21 2020-01-10 13:12:12.076 INFO [main] Frame #22 2020-01-10 13:12:12.147 INFO [main] Frame #23 2020-01-10 13:12:12.216 INFO [main] Frame #24 2020-01-10 13:12:12.288 INFO [main] Frame #25 2020-01-10 13:12:12.361 INFO [main] Frame #26 2020-01-10 13:12:12.431 INFO [main] Frame #27 2020-01-10 13:12:12.503 INFO [main] Frame #28 2020-01-10 13:12:12.575 INFO [main] Frame #29 2020-01-10 13:12:12.643 INFO [main] Frame #30 2020-01-10 13:12:12.713 INFO [main] Frame #31 2020-01-10 13:12:12.786 INFO [main] Frame #32 2020-01-10 13:12:12.853 INFO [main] Frame #33 2020-01-10 13:12:12.926 INFO [main] Frame #34 2020-01-10 13:12:12.999 INFO [main] Frame #35 2020-01-10 13:12:13.065 INFO [main] Frame #36 2020-01-10 13:12:13.137 INFO [main] Frame #37 2020-01-10 13:12:13.211 INFO [main] Frame #38 2020-01-10 13:12:13.279 INFO [main] Frame #39 2020-01-10 13:12:13.349 INFO [main] Frame #40 2020-01-10 13:12:13.422 INFO [main] Frame #41 2020-01-10 13:12:13.491 INFO [main] Frame #42 2020-01-10 13:12:13.567 INFO [main] Frame #43 2020-01-10 13:12:13.639 INFO [main] Frame #44 2020-01-10 13:12:13.706 INFO [main] Frame #45 2020-01-10 13:12:13.775 INFO [main] Frame #46 2020-01-10 13:12:13.851 INFO [main] Frame #47 2020-01-10 13:12:13.920 INFO [main] Frame #48 2020-01-10 13:12:13.992 INFO [main] Frame #49 2020-01-10 13:12:14.072 INFO [main] Frame #50 2020-01-10 13:12:14.142 INFO [main] Frame #51 2020-01-10 13:12:14.212 INFO [main] Frame #52 2020-01-10 13:12:14.285 INFO [main] Frame #53 2020-01-10 13:12:14.364 INFO [main] Frame #54 2020-01-10 13:12:14.439 INFO [main] Frame #55 2020-01-10 13:12:14.509 INFO [main] Frame #56 2020-01-10 13:12:14.579 INFO [main] Frame #57 2020-01-10 13:12:14.651 INFO [main] Frame #58 2020-01-10 13:12:14.724 INFO [main] Frame #59 2020-01-10 13:12:14.795 INFO [main] Frame #60 2020-01-10 13:12:14.869 INFO [main] Frame #61 2020-01-10 13:12:14.939 INFO [main] Frame #62 2020-01-10 13:12:15.012 INFO [main] Frame #63 2020-01-10 13:12:15.085 INFO [main] Frame #64 2020-01-10 13:12:15.156 INFO [main] Frame #65 2020-01-10 13:12:15.231 INFO [main] Frame #66 2020-01-10 13:12:15.306 INFO [main] Frame #67 2020-01-10 13:12:15.377 INFO [main] Frame #68 2020-01-10 13:12:15.456 INFO [main] Frame #69 2020-01-10 13:12:15.530 INFO [main] Frame #70 2020-01-10 13:12:15.602 INFO [main] Frame #71 2020-01-10 13:12:15.676 INFO [main] Frame #72 2020-01-10 13:12:15.755 INFO [main] Frame #73 2020-01-10 13:12:15.826 INFO [main] Frame #74 2020-01-10 13:12:15.900 INFO [main] Frame #75 2020-01-10 13:12:15.971 INFO [main] Frame #76 2020-01-10 13:12:16.046 INFO [main] Frame #77 2020-01-10 13:12:16.121 INFO [main] Frame #78 2020-01-10 13:12:16.192 INFO [main] Frame #79 2020-01-10 13:12:16.264 INFO [main] Frame #80 2020-01-10 13:12:16.335 INFO [main] Frame #81 2020-01-10 13:12:16.411 INFO [main] Frame #82 2020-01-10 13:12:16.482 INFO [main] Frame #83 2020-01-10 13:12:16.556 INFO [main] Frame #84 2020-01-10 13:12:16.631 INFO [main] Frame #85 2020-01-10 13:12:16.703 INFO [main] Frame #86 2020-01-10 13:12:16.776 INFO [main] Frame #87 2020-01-10 13:12:16.846 INFO [main] Frame #88 2020-01-10 13:12:16.918 INFO [main] Frame #89 2020-01-10 13:12:16.993 INFO [main] Frame #90 2020-01-10 13:12:17.063 INFO [main] Frame #91 2020-01-10 13:12:17.150 INFO [main] Frame #92 2020-01-10 13:12:17.224 INFO [main] Frame #93 2020-01-10 13:12:17.297 INFO [main] Frame #94 2020-01-10 13:12:17.370 INFO [main] Frame #95 2020-01-10 13:12:17.446 INFO [main] Frame #96 2020-01-10 13:12:17.520 INFO [main] Frame #97 2020-01-10 13:12:17.593 INFO [main] Frame #98 2020-01-10 13:12:17.672 INFO [main] Frame #99 2020-01-10 13:12:17.746 INFO [main] Frame #100 2020-01-10 13:12:17.819 INFO [main] Frame #101 2020-01-10 13:12:17.892 INFO [main] Frame #102 2020-01-10 13:12:17.968 INFO [main] Frame #103 2020-01-10 13:12:18.041 INFO [main] Frame #104 2020-01-10 13:12:18.114 INFO [main] Frame #105 2020-01-10 13:12:18.190 INFO [main] Frame #106 2020-01-10 13:12:18.263 INFO [main] Frame #107 2020-01-10 13:12:18.337 INFO [main] Frame #108 2020-01-10 13:12:18.412 INFO [main] Frame #109 2020-01-10 13:12:18.486 INFO [main] Frame #110 2020-01-10 13:12:18.559 INFO [main] Frame #111 2020-01-10 13:12:18.636 INFO [main] Frame #112 2020-01-10 13:12:18.710 INFO [main] Frame #113 2020-01-10 13:12:18.783 INFO [main] Frame #114 2020-01-10 13:12:18.856 INFO [main] Frame #115 2020-01-10 13:12:18.929 INFO [main] Frame #116 2020-01-10 13:12:19.002 INFO [main] Frame #117 2020-01-10 13:12:19.074 INFO [main] Frame #118 2020-01-10 13:12:19.146 INFO [main] Frame #119 2020-01-10 13:12:19.218 INFO [main] Frame #120 2020-01-10 13:12:19.298 INFO [main] Frame #121 2020-01-10 13:12:19.372 INFO [main] Frame #122 2020-01-10 13:12:19.445 INFO [main] Frame #123 2020-01-10 13:12:19.517 INFO [main] Frame #124 2020-01-10 13:12:19.596 INFO [main] Frame #125 2020-01-10 13:12:19.669 INFO [main] Frame #126 2020-01-10 13:12:19.741 INFO [main] Frame #127 2020-01-10 13:12:19.812 INFO [main] Frame #128 2020-01-10 13:12:19.885 INFO [main] Frame #129 2020-01-10 13:12:19.958 INFO [main] Frame #130 2020-01-10 13:12:20.030 INFO [main] Frame #131 2020-01-10 13:12:20.108 INFO [main] Frame #132 2020-01-10 13:12:20.181 INFO [main] Frame #133 2020-01-10 13:12:20.253 INFO [main] Frame #134 2020-01-10 13:12:20.323 INFO [main] Frame #135 2020-01-10 13:12:20.396 INFO [main] Frame #136 2020-01-10 13:12:20.470 INFO [main] Frame #137 2020-01-10 13:12:20.541 INFO [main] Frame #138 2020-01-10 13:12:20.611 INFO [main] Frame #139 2020-01-10 13:12:20.685 INFO [main] Frame #140 2020-01-10 13:12:20.758 INFO [main] Frame #141 2020-01-10 13:12:20.830 INFO [main] Frame #142 2020-01-10 13:12:20.901 INFO [main] Frame #143 2020-01-10 13:12:20.974 INFO [main] Frame #144 2020-01-10 13:12:21.049 INFO [main] Frame #145 2020-01-10 13:12:21.120 INFO [main] Frame #146 2020-01-10 13:12:21.189 INFO [main] Frame #147 2020-01-10 13:12:21.261 INFO [main] Frame #148 2020-01-10 13:12:21.336 INFO [main] Frame #149 2020-01-10 13:12:21.408 INFO [main] Frame #150 2020-01-10 13:12:21.480 INFO [main] Frame #151 2020-01-10 13:12:21.558 INFO [main] Frame #152 2020-01-10 13:12:21.631 INFO [main] Frame #153 2020-01-10 13:12:21.703 INFO [main] Frame #154 2020-01-10 13:12:21.774 INFO [main] Frame #155 2020-01-10 13:12:21.847 INFO [main] Frame #156 2020-01-10 13:12:21.920 INFO [main] Frame #157 2020-01-10 13:12:21.991 INFO [main] Frame #158 2020-01-10 13:12:22.061 INFO [main] Frame #159 2020-01-10 13:12:22.134 INFO [main] Frame #160 2020-01-10 13:12:22.204 INFO [main] Frame #161 2020-01-10 13:12:22.279 INFO [main] Frame #162 2020-01-10 13:12:22.351 INFO [main] Frame #163 2020-01-10 13:12:22.422 INFO [main] Frame #164 2020-01-10 13:12:22.493 INFO [main] Frame #165 2020-01-10 13:12:22.564 INFO [main] Frame #166 2020-01-10 13:12:22.636 INFO [main] Frame #167 2020-01-10 13:12:22.709 INFO [main] Frame #168 2020-01-10 13:12:22.782 INFO [main] Frame #169 2020-01-10 13:12:22.855 INFO [main] Frame #170 2020-01-10 13:12:22.927 INFO [main] Frame #171 2020-01-10 13:12:22.997 INFO [main] Frame #172 2020-01-10 13:12:23.069 INFO [main] Frame #173 2020-01-10 13:12:23.144 INFO [main] Frame #174 2020-01-10 13:12:23.216 INFO [main] Frame #175 2020-01-10 13:12:23.288 INFO [main] Frame #176 2020-01-10 13:12:23.359 INFO [main] Frame #177 2020-01-10 13:12:23.430 INFO [main] Frame #178 2020-01-10 13:12:23.506 INFO [main] Frame #179 2020-01-10 13:12:23.577 INFO [main] Frame #180 2020-01-10 13:12:23.652 INFO [main] Frame #181 2020-01-10 13:12:23.723 INFO [main] Frame #182 2020-01-10 13:12:23.797 INFO [main] Frame #183 2020-01-10 13:12:23.870 INFO [main] Frame #184 2020-01-10 13:12:23.942 INFO [main] Frame #185 2020-01-10 13:12:24.013 INFO [main] Frame #186 2020-01-10 13:12:24.087 INFO [main] Frame #187 2020-01-10 13:12:24.160 INFO [main] Frame #188 2020-01-10 13:12:24.232 INFO [main] Frame #189 2020-01-10 13:12:24.309 INFO [main] Frame #190 2020-01-10 13:12:24.385 INFO [main] Frame #191 2020-01-10 13:12:24.462 INFO [main] Frame #192 2020-01-10 13:12:24.536 INFO [main] Frame #193 2020-01-10 13:12:24.611 INFO [main] Frame #194 2020-01-10 13:12:24.687 INFO [main] Frame #195 2020-01-10 13:12:24.762 INFO [main] Frame #196 2020-01-10 13:12:24.835 INFO [main] Frame #197 2020-01-10 13:12:24.909 INFO [main] Frame #198 2020-01-10 13:12:24.984 INFO [main] Frame #199 2020-01-10 13:12:25.058 INFO [main] Frame #200 2020-01-10 13:12:25.133 INFO [main] Frame #201 2020-01-10 13:12:25.208 INFO [main] Frame #202 2020-01-10 13:12:25.283 INFO [main] Frame #203 2020-01-10 13:12:25.361 INFO [main] Frame #204 2020-01-10 13:12:25.436 INFO [main] Frame #205 2020-01-10 13:12:25.508 INFO [main] Frame #206 2020-01-10 13:12:25.586 INFO [main] Frame #207 2020-01-10 13:12:25.659 INFO [main] Frame #208 2020-01-10 13:12:25.731 INFO [main] Frame #209 2020-01-10 13:12:25.805 INFO [main] Frame #210 2020-01-10 13:12:25.877 INFO [main] Frame #211 2020-01-10 13:12:25.949 INFO [main] Frame #212 2020-01-10 13:12:26.024 INFO [main] Frame #213 2020-01-10 13:12:26.100 INFO [main] Frame #214 2020-01-10 13:12:26.173 INFO [main] Frame #215 2020-01-10 13:12:26.249 INFO [main] Frame #216 2020-01-10 13:12:26.324 INFO [main] Frame #217 2020-01-10 13:12:26.399 INFO [main] Frame #218 2020-01-10 13:12:26.472 INFO [main] Frame #219 2020-01-10 13:12:26.547 INFO [main] Frame #220 2020-01-10 13:12:26.620 INFO [main] Frame #221 2020-01-10 13:12:26.693 INFO [main] Frame #222 2020-01-10 13:12:26.767 INFO [main] Frame #223 2020-01-10 13:12:26.840 INFO [main] Frame #224 2020-01-10 13:12:26.914 INFO [main] Frame #225 2020-01-10 13:12:26.987 INFO [main] Frame #226 2020-01-10 13:12:27.066 INFO [main] Frame #227 2020-01-10 13:12:27.140 INFO [main] Frame #228 2020-01-10 13:12:27.214 INFO [main] Frame #229 2020-01-10 13:12:27.287 INFO [main] Frame #230 2020-01-10 13:12:27.362 INFO [main] Frame #231 2020-01-10 13:12:27.434 INFO [main] Frame #232 2020-01-10 13:12:27.524 INFO [main] Frame #233 2020-01-10 13:12:27.598 INFO [main] Frame #234 2020-01-10 13:12:27.671 INFO [main] Frame #235 2020-01-10 13:12:27.742 INFO [main] Frame #236 2020-01-10 13:12:27.813 INFO [main] Frame #237 2020-01-10 13:12:27.884 INFO [main] Frame #238 2020-01-10 13:12:27.956 INFO [main] Frame #239 2020-01-10 13:12:28.026 INFO [main] Frame #240 2020-01-10 13:12:28.099 INFO [main] Frame #241 2020-01-10 13:12:28.173 INFO [main] Frame #242 2020-01-10 13:12:28.248 INFO [main] Frame #243 2020-01-10 13:12:28.317 INFO [main] Frame #244 2020-01-10 13:12:28.387 INFO [main] Frame #245 2020-01-10 13:12:28.463 INFO [main] Frame #246 2020-01-10 13:12:28.532 INFO [main] Frame #247 2020-01-10 13:12:28.601 INFO [main] Frame #248 2020-01-10 13:12:28.672 INFO [main] Frame #249 2020-01-10 13:12:28.742 INFO [main] Frame #250 2020-01-10 13:12:28.813 INFO [main] Frame #251 2020-01-10 13:12:28.886 INFO [main] Frame #252 2020-01-10 13:12:28.965 INFO [main] Frame #253 2020-01-10 13:12:29.037 INFO [main] Frame #254 2020-01-10 13:12:29.109 INFO [main] Frame #255 2020-01-10 13:12:29.180 INFO [main] Frame #256 2020-01-10 13:12:29.254 INFO [main] Frame #257 2020-01-10 13:12:29.326 INFO [main] Frame #258 2020-01-10 13:12:29.396 INFO [main] Frame #259 2020-01-10 13:12:29.468 INFO [main] Frame #260 2020-01-10 13:12:29.538 INFO [main] Frame #261 2020-01-10 13:12:29.609 INFO [main] Frame #262 2020-01-10 13:12:29.679 INFO [main] Frame #263 2020-01-10 13:12:29.755 INFO [main] Frame #264 2020-01-10 13:12:29.830 INFO [main] Frame #265 2020-01-10 13:12:29.901 INFO [main] Frame #266 2020-01-10 13:12:29.972 INFO [main] Frame #267 2020-01-10 13:12:30.045 INFO [main] Frame #268 2020-01-10 13:12:30.118 INFO [main] Frame #269 2020-01-10 13:12:30.194 INFO [main] Frame #270 2020-01-10 13:12:30.268 INFO [main] Frame #271 2020-01-10 13:12:30.340 INFO [main] Frame #272 2020-01-10 13:12:30.411 INFO [main] Frame #273 2020-01-10 13:12:30.485 INFO [main] Frame #274 2020-01-10 13:12:30.562 INFO [main] Frame #275 2020-01-10 13:12:30.633 INFO [main] Frame #276 2020-01-10 13:12:30.706 INFO [main] Frame #277 2020-01-10 13:12:30.776 INFO [main] Frame #278 2020-01-10 13:12:30.850 INFO [main] Frame #279 2020-01-10 13:12:30.922 INFO [main] Frame #280 2020-01-10 13:12:30.992 INFO [main] Frame #281 2020-01-10 13:12:31.067 INFO [main] Frame #282 2020-01-10 13:12:31.137 INFO [main] Frame #283 2020-01-10 13:12:31.206 INFO [main] Frame #284 2020-01-10 13:12:31.276 INFO [main] Frame #285 2020-01-10 13:12:31.346 INFO [main] Frame #286 2020-01-10 13:12:31.415 INFO [main] Frame #287 2020-01-10 13:12:31.485 INFO [main] Frame #288 2020-01-10 13:12:31.560 INFO [main] Frame #289 2020-01-10 13:12:31.630 INFO [main] Frame #290 2020-01-10 13:12:31.700 INFO [main] Frame #291 2020-01-10 13:12:31.771 INFO [main] Frame #292 2020-01-10 13:12:31.843 INFO [main] Frame #293 2020-01-10 13:12:31.915 INFO [main] Frame #294 2020-01-10 13:12:31.984 INFO [main] Frame #295 2020-01-10 13:12:32.054 INFO [main] Frame #296 2020-01-10 13:12:32.127 INFO [main] Frame #297 2020-01-10 13:12:32.196 INFO [main] Frame #298 2020-01-10 13:12:32.266 INFO [main] Frame #299 2020-01-10 13:12:32.339 INFO [main] Frame #300 2020-01-10 13:12:32.410 INFO [main] Frame #301 2020-01-10 13:12:32.482 INFO [main] Frame #302 2020-01-10 13:12:32.560 INFO [main] Frame #303 2020-01-10 13:12:32.632 INFO [main] Frame #304 2020-01-10 13:12:32.703 INFO [main] Frame #305 2020-01-10 13:12:32.776 INFO [main] Frame #306 2020-01-10 13:12:32.849 INFO [main] Frame #307 2020-01-10 13:12:32.920 INFO [main] Frame #308 2020-01-10 13:12:32.992 INFO [main] Frame #309 2020-01-10 13:12:33.063 INFO [main] Frame #310 2020-01-10 13:12:33.134 INFO [main] Frame #311 2020-01-10 13:12:33.207 INFO [main] Frame #312 2020-01-10 13:12:33.284 INFO [main] Frame #313 2020-01-10 13:12:33.356 INFO [main] Frame #314 2020-01-10 13:12:33.428 INFO [main] Frame #315 2020-01-10 13:12:33.503 INFO [main] Frame #316 2020-01-10 13:12:33.577 INFO [main] Frame #317 2020-01-10 13:12:33.650 INFO [main] Frame #318 2020-01-10 13:12:33.724 INFO [main] Frame #319 2020-01-10 13:12:33.799 INFO [main] Frame #320 2020-01-10 13:12:33.871 INFO [main] Frame #321 2020-01-10 13:12:33.948 INFO [main] Frame #322 2020-01-10 13:12:34.022 INFO [main] Frame #323 2020-01-10 13:12:34.096 INFO [main] Frame #324 2020-01-10 13:12:34.170 INFO [main] Frame #325 2020-01-10 13:12:34.240 INFO [main] Frame #326 2020-01-10 13:12:34.312 INFO [main] Frame #327 2020-01-10 13:12:34.386 INFO [main] Frame #328 2020-01-10 13:12:34.456 INFO [main] Frame #329 2020-01-10 13:12:34.525 INFO [main] Frame #330 2020-01-10 13:12:34.599 INFO [main] Frame #331 2020-01-10 13:12:34.674 INFO [main] Frame #332 2020-01-10 13:12:34.743 INFO [main] Frame #333 2020-01-10 13:12:34.813 INFO [main] Frame #334 2020-01-10 13:12:34.882 INFO [main] Frame #335 2020-01-10 13:12:34.955 INFO [main] Frame #336 2020-01-10 13:12:35.028 INFO [main] Frame #337 2020-01-10 13:12:35.100 INFO [main] Frame #338 2020-01-10 13:12:35.170 INFO [main] Frame #339 2020-01-10 13:12:35.239 INFO [main] Frame #340 2020-01-10 13:12:35.311 INFO [main] Frame #341 2020-01-10 13:12:35.380 INFO [main] Frame #342 2020-01-10 13:12:35.449 INFO [main] Frame #343 2020-01-10 13:12:35.521 INFO [main] Frame #344 2020-01-10 13:12:35.591 INFO [main] Frame #345 2020-01-10 13:12:35.660 INFO [main] Frame #346 2020-01-10 13:12:35.732 INFO [main] Frame #347 2020-01-10 13:12:35.803 INFO [main] Frame #348 2020-01-10 13:12:35.874 INFO [main] Frame #349 2020-01-10 13:12:35.943 INFO [main] Frame #350 2020-01-10 13:12:36.013 INFO [main] Frame #351 2020-01-10 13:12:36.083 INFO [main] Frame #352 2020-01-10 13:12:36.151 INFO [main] Frame #353 2020-01-10 13:12:36.219 INFO [main] Frame #354 2020-01-10 13:12:36.288 INFO [main] Frame #355 2020-01-10 13:12:36.360 INFO [main] Frame #356 2020-01-10 13:12:36.428 INFO [main] Frame #357 2020-01-10 13:12:36.496 INFO [main] Frame #358 2020-01-10 13:12:36.565 INFO [main] Frame #359 2020-01-10 13:12:36.634 INFO [main] Frame #360 2020-01-10 13:12:36.707 INFO [main] Frame #361 2020-01-10 13:12:36.776 INFO [main] Frame #362 2020-01-10 13:12:36.845 INFO [main] Frame #363 2020-01-10 13:12:36.914 INFO [main] Frame #364 2020-01-10 13:12:36.983 INFO [main] Frame #365 2020-01-10 13:12:37.052 INFO [main] Frame #366 2020-01-10 13:12:37.126 INFO [main] Frame #367 2020-01-10 13:12:37.195 INFO [main] Frame #368 2020-01-10 13:12:37.265 INFO [main] Frame #369 2020-01-10 13:12:37.339 INFO [main] Frame #370 2020-01-10 13:12:37.412 INFO [main] Frame #371 2020-01-10 13:12:37.484 INFO [main] Frame #372 2020-01-10 13:12:37.554 INFO [main] Frame #373 2020-01-10 13:12:37.627 INFO [main] Frame #374 2020-01-10 13:12:37.697 INFO [main] Frame #375 2020-01-10 13:12:37.766 INFO [main] Frame #376 . from google.colab import files files.download(&#39;road_parsed.mp4&#39;) .",
            "url": "https://nb.recohut.com/2022/01/01/vehicle-detection.html",
            "relUrl": "/2022/01/01/vehicle-detection.html",
            "date": " • Jan 1, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Tokenization doesn't have to be slow !",
            "content": "Introduction . Before going deep into any Machine Learning or Deep Learning Natural Language Processing models, every practitioner should find a way to map raw input strings to a representation understandable by a trainable model. . One very simple approach would be to split inputs over every space and assign an identifier to each word. This approach would look similar to the code below in python . s = &quot;very long corpus...&quot; words = s.split(&quot; &quot;) # Split over space vocabulary = dict(enumerate(set(words))) # Map storing the word to it&#39;s corresponding id . This approach might work well if your vocabulary remains small as it would store every word (or token) present in your original input. Moreover, word variations like &quot;cat&quot; and &quot;cats&quot; would not share the same identifiers even if their meaning is quite close. . . Subtoken Tokenization . To overcome the issues described above, recent works have been done on tokenization, leveraging &quot;subtoken&quot; tokenization. Subtokens extends the previous splitting strategy to furthermore explode a word into grammatically logicial sub-components learned from the data. . Taking our previous example of the words cat and cats, a sub-tokenization of the word cats would be [cat, ##s]. Where the prefix &quot;##&quot; indicates a subtoken of the initial input. Such training algorithms might extract sub-tokens such as &quot;##ing&quot;, &quot;##ed&quot; over English corpus. . As you might think of, this kind of sub-tokens construction leveraging compositions of &quot;pieces&quot; overall reduces the size of the vocabulary you have to carry to train a Machine Learning model. On the other side, as one token might be exploded into multiple subtokens, the input of your model might increase and become an issue on model with non-linear complexity over the input sequence&#39;s length. . . Among all the tokenization algorithms, we can highlight a few subtokens algorithms used in Transformers-based SoTA models : . Byte Pair Encoding (BPE) - Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015) | Word Piece - Japanese and Korean voice search (Schuster, M., and Nakajima, K., 2015) | Unigram Language Model - Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo, T., 2018) | Sentence Piece - A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Taku Kudo and John Richardson, 2018) | . Going through all of them is out of the scope of this notebook, so we will just highlight how you can use them. . @huggingface/tokenizers library . Along with the transformers library, we @huggingface provide a blazing fast tokenization library able to train, tokenize and decode dozens of Gb/s of text on a common multi-core machine. . The library is written in Rust allowing us to take full advantage of multi-core parallel computations in a native and memory-aware way, on-top of which we provide bindings for Python and NodeJS (more bindings may be added in the future). . We designed the library so that it provides all the required blocks to create end-to-end tokenizers in an interchangeable way. In that sense, we provide these various components: . Normalizer: Executes all the initial transformations over the initial input string. For example when you need to lowercase some text, maybe strip it, or even apply one of the common unicode normalization process, you will add a Normalizer. | PreTokenizer: In charge of splitting the initial input string. That&#39;s the component that decides where and how to pre-segment the origin string. The simplest example would be like we saw before, to simply split on spaces. | Model: Handles all the sub-token discovery and generation, this part is trainable and really dependant of your input data. | Post-Processor: Provides advanced construction features to be compatible with some of the Transformers-based SoTA models. For instance, for BERT it would wrap the tokenized sentence around [CLS] and [SEP] tokens. | Decoder: In charge of mapping back a tokenized input to the original string. The decoder is usually chosen according to the PreTokenizer we used previously. | Trainer: Provides training capabilities to each model. | . For each of the components above we provide multiple implementations: . Normalizer: Lowercase, Unicode (NFD, NFKD, NFC, NFKC), Bert, Strip, ... | PreTokenizer: ByteLevel, WhitespaceSplit, CharDelimiterSplit, Metaspace, ... | Model: WordLevel, BPE, WordPiece | Post-Processor: BertProcessor, ... | Decoder: WordLevel, BPE, WordPiece, ... | . All of these building blocks can be combined to create working tokenization pipelines. In the next section we will go over our first pipeline. . Alright, now we are ready to implement our first tokenization pipeline through tokenizers. . For this, we will train a Byte-Pair Encoding (BPE) tokenizer on a quite small input for the purpose of this notebook. We will work with the file from Peter Norving. This file contains around 130.000 lines of raw text that will be processed by the library to generate a working tokenizer. . !pip install tokenizers . BIG_FILE_URL = &#39;https://raw.githubusercontent.com/dscape/spell/master/test/resources/big.txt&#39; # Let&#39;s download the file and save it somewhere from requests import get with open(&#39;big.txt&#39;, &#39;wb&#39;) as big_f: response = get(BIG_FILE_URL, ) if response.status_code == 200: big_f.write(response.content) else: print(&quot;Unable to get the file: {}&quot;.format(response.reason)) . Now that we have our training data we need to create the overall pipeline for the tokenizer . # the overall pipeline for various well-known tokenization algorithm. # Everything described below can be replaced by the ByteLevelBPETokenizer class. from tokenizers import Tokenizer from tokenizers.decoders import ByteLevel as ByteLevelDecoder from tokenizers.models import BPE from tokenizers.normalizers import Lowercase, NFKC, Sequence from tokenizers.pre_tokenizers import ByteLevel # First we create an empty Byte-Pair Encoding model (i.e. not trained model) tokenizer = Tokenizer(BPE()) # Then we enable lower-casing and unicode-normalization # The Sequence normalizer allows us to combine multiple Normalizer that will be # executed in order. tokenizer.normalizer = Sequence([ NFKC(), Lowercase() ]) # Our tokenizer also needs a pre-tokenizer responsible for converting the input to a ByteLevel representation. tokenizer.pre_tokenizer = ByteLevel() # And finally, let&#39;s plug a decoder so we can recover from a tokenized input to the original one tokenizer.decoder = ByteLevelDecoder() . The overall pipeline is now ready to be trained on the corpus we downloaded earlier in this notebook. . from tokenizers.trainers import BpeTrainer # We initialize our trainer, giving him the details about the vocabulary we want to generate trainer = BpeTrainer(vocab_size=25000, show_progress=True, initial_alphabet=ByteLevel.alphabet()) tokenizer.train(trainer, [&quot;big.txt&quot;]) print(&quot;Trained vocab size: {}&quot;.format(tokenizer.get_vocab_size())) . Trained vocab size: 25000 . Et voilà ! You trained your very first tokenizer from scratch using tokenizers. Of course, this covers only the basics, and you may want to have a look at the add_special_tokens or special_tokens parameters on the Trainer class, but the overall process should be very similar. . We can save the content of the model to reuse it later. . tokenizer.model.save(&#39;.&#39;) . [&#39;./vocab.json&#39;, &#39;./merges.txt&#39;] . Now, let load the trained model and start using out newly trained tokenizer . tokenizer.model = BPE(&#39;vocab.json&#39;, &#39;merges.txt&#39;) encoding = tokenizer.encode(&quot;This is a simple input to be tokenized&quot;) print(&quot;Encoded string: {}&quot;.format(encoding.tokens)) decoded = tokenizer.decode(encoding.ids) print(&quot;Decoded string: {}&quot;.format(decoded)) . Encoded string: [&#39;Ġthis&#39;, &#39;Ġis&#39;, &#39;Ġa&#39;, &#39;Ġsimple&#39;, &#39;Ġin&#39;, &#39;put&#39;, &#39;Ġto&#39;, &#39;Ġbe&#39;, &#39;Ġtoken&#39;, &#39;ized&#39;] Decoded string: this is a simple input to be tokenized . The Encoding structure exposes multiple properties which are useful when working with transformers models . normalized_str: The input string after normalization (lower-casing, unicode, stripping, etc.) | original_str: The input string as it was provided | tokens: The generated tokens with their string representation | input_ids: The generated tokens with their integer representation | attention_mask: If your input has been padded by the tokenizer, then this would be a vector of 1 for any non padded token and 0 for padded ones. | special_token_mask: If your input contains special tokens such as [CLS], [SEP], [MASK], [PAD], then this would be a vector with 1 in places where a special token has been added. | type_ids: If your input was made of multiple &quot;parts&quot; such as (question, context), then this would be a vector with for each token the segment it belongs to. | overflowing: If your input has been truncated into multiple subparts because of a length limit (for BERT for example the sequence length is limited to 512), this will contain all the remaining overflowing parts. | .",
            "url": "https://nb.recohut.com/nlp/tokenization/2021/01/22/training-tokenizers.html",
            "relUrl": "/nlp/tokenization/2021/01/22/training-tokenizers.html",
            "date": " • Jan 22, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://nb.recohut.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://nb.recohut.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}