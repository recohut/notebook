<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Best Ads detection using bandit methods | reconb</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Best Ads detection using bandit methods" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Multi-armed Bandit for Banner Ad and 4 Exploration Strategies" />
<meta property="og:description" content="Multi-armed Bandit for Banner Ad and 4 Exploration Strategies" />
<link rel="canonical" href="https://nb.recohut.com/bandit/rl/mab/2021/07/02/ads-selection-using-bandits.html" />
<meta property="og:url" content="https://nb.recohut.com/bandit/rl/mab/2021/07/02/ads-selection-using-bandits.html" />
<meta property="og:site_name" content="reconb" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-07-02T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Best Ads detection using bandit methods" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-07-02T00:00:00-05:00","datePublished":"2021-07-02T00:00:00-05:00","description":"Multi-armed Bandit for Banner Ad and 4 Exploration Strategies","headline":"Best Ads detection using bandit methods","mainEntityOfPage":{"@type":"WebPage","@id":"https://nb.recohut.com/bandit/rl/mab/2021/07/02/ads-selection-using-bandits.html"},"url":"https://nb.recohut.com/bandit/rl/mab/2021/07/02/ads-selection-using-bandits.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://nb.recohut.com/feed.xml" title="reconb" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">reconb</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Best Ads detection using bandit methods</h1><p class="page-description">Multi-armed Bandit for Banner Ad and 4 Exploration Strategies</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-07-02T00:00:00-05:00" itemprop="datePublished">
        Jul 2, 2021
      </time>
       â€¢ <span class="read-time" title="Estimated read time">
    
    
      9 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#Bandit">Bandit</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#RL">RL</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#MAB">MAB</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/recohut/notebook/tree/master/_notebooks/2021-07-02-ads-selection-using-bandits.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/recohut/notebook/master?filepath=_notebooks%2F2021-07-02-ads-selection-using-bandits.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/recohut/notebook/blob/master/_notebooks/2021-07-02-ads-selection-using-bandits.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Frecohut%2Fnotebook%2Fblob%2Fmaster%2F_notebooks%2F2021-07-02-ads-selection-using-bandits.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-07-02-ads-selection-using-bandits.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;ggplot&#39;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span> git clone https://github.com/JKCooper2/gym-bandits

<span class="c1"># Next, we can install it using pip:</span>
<span class="o">%</span><span class="k">cd</span> gym-bandits
<span class="o">!</span> pip install -e .

<span class="c1"># After installation, we import gym_bandits and also the gym library:</span>
<span class="kn">import</span> <span class="nn">gym_bandits</span>
<span class="kn">import</span> <span class="nn">gym</span>

<span class="c1"># gym_bandits provides several versions of the bandit environment. </span>
<span class="c1"># We can examine the different bandit versions at https://github.com/JKCooper2/gym-bandits</span>

<span class="c1">#Let&#39;s just create a simple 2-armed bandit whose environment ID is BanditTwoArmedHighLowFixed-v0:</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;BanditTwoArmedHighLowFixed-v0&quot;</span><span class="p">)</span>

<span class="c1"># Since we created a 2-armed bandit, our action space will be 2 (as there are two arms), as shown here:</span>
<span class="nb">print</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">)</span>    <span class="c1"># The preceding code will print: 2</span>

<span class="c1"># We can also check the probability distribution of the arm with:</span>
<span class="nb">print</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">p_dist</span><span class="p">)</span>    <span class="c1"># The preceding code will print: [0.8, 0.2]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>fatal: destination path &#39;gym-bandits&#39; already exists and is not an empty directory.
/content/gym-bandits
Obtaining file:///content/gym-bandits
Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym-bandits==0.0.2) (0.17.3)
Requirement already satisfied: pyglet&lt;=1.5.0,&gt;=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym-&gt;gym-bandits==0.0.2) (1.5.0)
Requirement already satisfied: cloudpickle&lt;1.7.0,&gt;=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym-&gt;gym-bandits==0.0.2) (1.3.0)
Requirement already satisfied: numpy&gt;=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym-&gt;gym-bandits==0.0.2) (1.18.5)
Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym-&gt;gym-bandits==0.0.2) (1.4.1)
Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet&lt;=1.5.0,&gt;=1.4.0-&gt;gym-&gt;gym-bandits==0.0.2) (0.16.0)
Installing collected packages: gym-bandits
  Running setup.py develop for gym-bandits
Successfully installed gym-bandits
2
[0.8, 0.2]
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Initialize the count for storing the number of times an arm is pulled:</span>
<span class="n">count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># Initialize sum_rewards for storing the sum of rewards of each arm:</span>
<span class="n">sum_rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># Initialize Q for storing the average reward of each arm:</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># Set the number of rounds (iterations):</span>
<span class="n">num_rounds</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Now, let&#39;s define the epsilon_greedy function.</span>
<span class="k">def</span> <span class="nf">epsilon_greedy</span><span class="p">(</span><span class="n">epsilon</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;First, we generate a random number from a uniform distribution. If the random </span>
<span class="sd">    number is less than epsilon, then we pull the random arm; else, we pull </span>
<span class="sd">    the best arm that has the maximum average reward&quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>

<span class="c1"># Now, let&#39;s play the game and try to find the best arm using the epsilon-greedy method.</span>
<span class="c1"># For each round:</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_rounds</span><span class="p">):</span>
  <span class="c1"># Select the arm based on the epsilon-greedy method:</span>
  <span class="n">arm</span> <span class="o">=</span> <span class="n">epsilon_greedy</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
  <span class="c1"># Pull the arm and store the reward and next state information:</span>
  <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">arm</span><span class="p">)</span>
  <span class="c1"># Increment the count of the arm by 1:</span>
  <span class="n">count</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="c1"># Update the sum of rewards of the arm:</span>
  <span class="n">sum_rewards</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span><span class="o">+=</span><span class="n">reward</span>
  <span class="c1"># Update the average reward of the arm:</span>
  <span class="n">Q</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum_rewards</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span><span class="o">/</span><span class="n">count</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
  
<span class="c1"># After all the rounds, we look at the average reward obtained from each of the arms:</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>    <span class="c1"># The preceding code will print something like this: [0.xx 0.yy]</span>

<span class="c1"># Now, we can select the optimal arm as the one that has the maximum average reward:</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The optimal arm is arm </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[0.81012658 0.0952381 ]
The optimal arm is arm 1
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Initialize the count for storing the number of times an arm is pulled:</span>
<span class="n">count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># Initialize sum_rewards for storing the sum of rewards of each arm:</span>
<span class="n">sum_rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># Initialize Q for storing the average reward of each arm:</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># Set the number of rounds (iterations):</span>
<span class="n">num_rounds</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Now, we define the softmax function with the temperature T:</span>
<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
  <span class="c1"># Compute the probability of each arm based on the temperature equation:</span>
  <span class="n">denom</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">i</span><span class="o">/</span><span class="n">T</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">Q</span><span class="p">])</span>
  <span class="n">probs</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">i</span><span class="o">/</span><span class="n">T</span><span class="p">)</span><span class="o">/</span><span class="n">denom</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">Q</span><span class="p">]</span>
  <span class="c1"># Select the arm based on the computed probability distribution of arms:</span>
  <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">probs</span><span class="p">)</span>  
  <span class="k">return</span> <span class="n">arm</span>

<span class="c1"># Now, let&#39;s play the game and try to find the best arm using the softmax exploration method.</span>
<span class="c1"># Let&#39;s begin by setting the temperature T to a high number, say, 50:</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">50</span>
<span class="c1"># For each round:</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_rounds</span><span class="p">):</span>
  <span class="c1"># Select the arm based on the softmax exploration method:</span>
  <span class="n">arm</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
  <span class="c1"># Pull the arm and store the reward and next state information:</span>
  <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">arm</span><span class="p">)</span>
  <span class="c1"># Increment the count of the arm by 1:</span>
  <span class="n">count</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="c1"># Update the sum of rewards of the arm:</span>
  <span class="n">sum_rewards</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span><span class="o">+=</span><span class="n">reward</span>
  <span class="c1"># Update the average reward of the arm:</span>
  <span class="n">Q</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum_rewards</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span><span class="o">/</span><span class="n">count</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
  <span class="c1"># Reduce the temperature T:</span>
  <span class="n">T</span> <span class="o">=</span> <span class="n">T</span><span class="o">*</span><span class="mf">0.99</span>

<span class="c1"># After all the rounds, we check the Q value, that is, the average reward of all the arms:</span>
<span class="nb">print</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>    <span class="c1"># The preceding code will print something like this: [0.xx 0.yy]</span>

<span class="c1"># Now, we can select the optimal arm as the one that has the maximum average reward:</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The optimal arm is arm </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[0.73214286 0.11363636]
The optimal arm is arm 1
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Initialize the count for storing the number of times an arm is pulled:</span>
<span class="n">count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># Initialize sum_rewards for storing the sum of rewards of each arm:</span>
<span class="n">sum_rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># Initialize Q for storing the average reward of each arm:</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># Set the number of rounds (iterations):</span>
<span class="n">num_rounds</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Now, we define the UCB function, which returns the best arm as the </span>
<span class="c1"># one that has the highest UCB:</span>
<span class="k">def</span> <span class="nf">UCB</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
  <span class="c1"># Initialize the numpy array for storing the UCB of all the arms:</span>
  <span class="n">ucb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
  <span class="c1"># Before computing the UCB, we explore all the arms at least once, so for the </span>
  <span class="c1"># first 2 rounds, we directly select the arm corresponding to the round number:</span>
  <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">i</span>
  <span class="c1"># If the round is greater than 2, then we compute the UCB of all the arms as </span>
  <span class="c1"># specified in the UCB equation and return the arm that has the highest UCB:</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">arm</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
      <span class="n">ucb</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">count</span><span class="p">)))</span> <span class="o">/</span> <span class="n">count</span><span class="p">[</span><span class="n">arm</span><span class="p">])</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">ucb</span><span class="p">))</span>

<span class="c1"># Now, let&#39;s play the game and try to find the best arm using the UCB method.</span>
<span class="c1"># For each round:</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_rounds</span><span class="p">):</span>
  <span class="c1"># Select the arm based on the UCB method:</span>
  <span class="n">arm</span> <span class="o">=</span> <span class="n">UCB</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
  <span class="c1"># Pull the arm and store the reward and next state information:</span>
  <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">arm</span><span class="p">)</span>
  <span class="c1"># Increment the count of the arm by 1:</span>
  <span class="n">count</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="c1"># Update the sum of rewards of the arm:</span>
  <span class="n">sum_rewards</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span><span class="o">+=</span><span class="n">reward</span>
  <span class="c1"># Update the average reward of the arm:</span>
  <span class="n">Q</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum_rewards</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span><span class="o">/</span><span class="n">count</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>

<span class="c1"># Now, we can select the optimal arm as the one that has the maximum average reward:</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The optimal arm is arm </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>The optimal arm is arm 1
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Initialize the count for storing the number of times an arm is pulled:</span>
<span class="n">count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># Initialize sum_rewards for storing the sum of rewards of each arm:</span>
<span class="n">sum_rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># Initialize Q for storing the average reward of each arm:</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># Initialize the alpha value as 1 for both arms:</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># Initialize the beta value as 1 for both arms:</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># Set the number of rounds (iterations):</span>
<span class="n">num_rounds</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Now, let&#39;s define the thompson_sampling function</span>
<span class="k">def</span> <span class="nf">thompson_sampling</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">):</span>
  <span class="sd">&quot;&quot;&quot;we randomly sample values from the beta distributions of both arms and </span>
<span class="sd">  return the arm that has the maximum sampled value&quot;&quot;&quot;</span>
  <span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">alpha</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="n">beta</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">)]</span>
  <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="c1"># Now, let&#39;s play the game and try to find the best arm using the Thompson sampling method.</span>
<span class="c1"># For each round:</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_rounds</span><span class="p">):</span>
  <span class="c1"># Select the arm based on the Thompson sampling method:</span>
  <span class="n">arm</span> <span class="o">=</span> <span class="n">thompson_sampling</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span><span class="n">beta</span><span class="p">)</span>
  <span class="c1"># Pull the arm and store the reward and next state information:</span>
  <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">arm</span><span class="p">)</span> 
  <span class="c1"># Increment the count of the arm by 1:</span>
  <span class="n">count</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="c1"># Update the sum of rewards of the arm:</span>
  <span class="n">sum_rewards</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span><span class="o">+=</span><span class="n">reward</span>
  <span class="c1"># Update the average reward of the arm:</span>
  <span class="n">Q</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum_rewards</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span><span class="o">/</span><span class="n">count</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
  <span class="c1"># If we win the game, that is, if the reward is equal to 1, then we update </span>
  <span class="c1"># the value of alpha to alpha+1, else we update the value of beta to beta+1:</span>
  <span class="k">if</span> <span class="n">reward</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span>
    <span class="n">alpha</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">=</span> <span class="n">alpha</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">beta</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>

<span class="c1"># After all the rounds, we can select the optimal arm as the one that has the highest average reward:</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The optimal arm is arm </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>The optimal arm is arm 1
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># We generate a dataset with five columns denoting the five advertisement banners, </span>
<span class="c1"># where the values in the rows will be either 0 or 1, indicating whether the </span>
<span class="c1"># advertisement banner has been clicked (1) or not clicked (0) by the user:</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
  <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Banner_type_&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">100000</span><span class="p">)</span>

<span class="c1"># Now, let&#39;s initialize some of the important variables.</span>
<span class="c1"># Set the number of iterations:</span>
<span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">100000</span>
<span class="c1"># Define the number of banners:</span>
<span class="n">num_banner</span> <span class="o">=</span> <span class="mi">5</span>
<span class="c1"># Initialize count for storing the number of times the banner was clicked:</span>
<span class="n">count</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_banner</span><span class="p">)</span>
<span class="c1"># Initialize sum_rewards for storing the sum of rewards obtained from each banner:</span>
<span class="n">sum_rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_banner</span><span class="p">)</span>
<span class="c1"># Initialize Q for storing the mean reward of each banner:</span>
<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_banner</span><span class="p">)</span>
<span class="c1"># Define a list for storing the selected banners:</span>
<span class="n">banner_selected</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Now, let&#39;s define the epsilon-greedy method:</span>
<span class="k">def</span> <span class="nf">epsilon_greedy_policy</span><span class="p">(</span><span class="n">epsilon</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">num_banner</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>

<span class="c1"># Now, we run the epsilon-greedy policy to find out which advertisement banner is the best.</span>
<span class="c1"># For each iteration:</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
  <span class="c1"># Select the banner using the epsilon-greedy policy:</span>
  <span class="n">banner</span> <span class="o">=</span> <span class="n">epsilon_greedy_policy</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
  <span class="c1"># Get the reward of the banner:</span>
  <span class="n">reward</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">banner</span><span class="p">]</span>
  <span class="c1"># Increment the counter:</span>
  <span class="n">count</span><span class="p">[</span><span class="n">banner</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="c1"># Store the sum of rewards:</span>
  <span class="n">sum_rewards</span><span class="p">[</span><span class="n">banner</span><span class="p">]</span><span class="o">+=</span><span class="n">reward</span>
  <span class="c1"># Compute the average reward:</span>
  <span class="n">Q</span><span class="p">[</span><span class="n">banner</span><span class="p">]</span> <span class="o">=</span> <span class="n">sum_rewards</span><span class="p">[</span><span class="n">banner</span><span class="p">]</span><span class="o">/</span><span class="n">count</span><span class="p">[</span><span class="n">banner</span><span class="p">]</span>
  <span class="c1"># Store the banner to the banner selected list:</span>
  <span class="n">banner_selected</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">banner</span><span class="p">)</span>

<span class="c1"># After all the rounds, we can select the best banner as the one that has the maximum average reward:</span>
<span class="nb">print</span><span class="p">(</span> <span class="s1">&#39;The best banner is banner </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">)))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>The best banner is banner 0
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">countplot</span><span class="p">(</span><span class="n">banner_selected</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Banner&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Count&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
" />
</div>

</div>

</div>
</div>

</div>
    

</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="recohut/notebook"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/bandit/rl/mab/2021/07/02/ads-selection-using-bandits.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Jupyter notebook database.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/recohut" target="_blank" title="recohut"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
