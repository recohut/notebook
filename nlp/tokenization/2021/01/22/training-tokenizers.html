<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Tokenization doesn’t have to be slow ! | reconb</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Tokenization doesn’t have to be slow !" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Before going deep into any Machine Learning or Deep Learning Natural Language Processing models, every practitioner should find a way to map raw input strings to a representation understandable by a trainable model." />
<meta property="og:description" content="Before going deep into any Machine Learning or Deep Learning Natural Language Processing models, every practitioner should find a way to map raw input strings to a representation understandable by a trainable model." />
<link rel="canonical" href="https://nb.recohut.com/nlp/tokenization/2021/01/22/training-tokenizers.html" />
<meta property="og:url" content="https://nb.recohut.com/nlp/tokenization/2021/01/22/training-tokenizers.html" />
<meta property="og:site_name" content="reconb" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-22T00:00:00-06:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Tokenization doesn’t have to be slow !" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-01-22T00:00:00-06:00","datePublished":"2021-01-22T00:00:00-06:00","description":"Before going deep into any Machine Learning or Deep Learning Natural Language Processing models, every practitioner should find a way to map raw input strings to a representation understandable by a trainable model.","headline":"Tokenization doesn’t have to be slow !","mainEntityOfPage":{"@type":"WebPage","@id":"https://nb.recohut.com/nlp/tokenization/2021/01/22/training-tokenizers.html"},"url":"https://nb.recohut.com/nlp/tokenization/2021/01/22/training-tokenizers.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://nb.recohut.com/feed.xml" title="reconb" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">reconb</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Tokenization doesn&#39;t have to be slow !</h1><p class="page-description">Before going deep into any Machine Learning or Deep Learning Natural Language Processing models, every practitioner should find a way to map raw input strings to a representation understandable by a trainable model.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-01-22T00:00:00-06:00" itemprop="datePublished">
        Jan 22, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#nlp">nlp</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#tokenization">tokenization</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/recohut/notebook/tree/main/_notebooks/2021-01-22-training-tokenizers.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/recohut/notebook/main?filepath=_notebooks%2F2021-01-22-training-tokenizers.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/recohut/notebook/blob/main/_notebooks/2021-01-22-training-tokenizers.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
          <div class="px-2">
  <a href="https://deepnote.com/launch?url=https%3A%2F%2Fgithub.com%2Frecohut%2Fnotebook%2Fblob%2Fmain%2F_notebooks%2F2021-01-22-training-tokenizers.ipynb" target="_blank">
      <img class="notebook-badge-image" src="/assets/badges/deepnote.svg" alt="Launch in Deepnote"/>
  </a>
</div>

        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-01-22-training-tokenizers.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Introduction">Introduction<a class="anchor-link" href="#Introduction"> </a></h3><p>Before going deep into any Machine Learning or Deep Learning Natural Language Processing models, every practitioner
should find a way to map raw input strings to a representation understandable by a trainable model.</p>
<p>One very simple approach would be to split inputs over every space and assign an identifier to each word. This approach
would look similar to the code below in python</p>
<div class="highlight"><pre><span></span><span class="n">s</span> <span class="o">=</span> <span class="s2">&quot;very long corpus...&quot;</span>
<span class="n">words</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span>  <span class="c1"># Split over space</span>
<span class="n">vocabulary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">words</span><span class="p">)))</span>  <span class="c1"># Map storing the word to it&#39;s corresponding id</span>
</pre></div>
<p>This approach might work well if your vocabulary remains small as it would store every word (or <strong>token</strong>) present in your original
input. Moreover, word variations like "cat" and "cats" would not share the same identifiers even if their meaning is 
quite close.</p>
<p><img src="https://cdn.analyticsvidhya.com/wp-content/uploads/2019/11/tokenization.png" alt="tokenization_simple" /></p>
<h3 id="Subtoken-Tokenization">Subtoken Tokenization<a class="anchor-link" href="#Subtoken-Tokenization"> </a></h3><p>To overcome the issues described above, recent works have been done on tokenization, leveraging "subtoken" tokenization.
<strong>Subtokens</strong> extends the previous splitting strategy to furthermore explode a word into grammatically logicial sub-components learned
from the data.</p>
<p>Taking our previous example of the words <strong>cat</strong> and <strong>cats</strong>, a sub-tokenization of the word <strong>cats</strong> would be [cat, ##s]. Where the prefix <em>"##"</em> indicates a subtoken of the initial input. 
Such training algorithms might extract sub-tokens such as <em>"##ing"</em>, <em>"##ed"</em> over English corpus.</p>
<p>As you might think of, this kind of sub-tokens construction leveraging compositions of <em>"pieces"</em> overall reduces the size
of the vocabulary you have to carry to train a Machine Learning model. On the other side, as one token might be exploded
into multiple subtokens, the input of your model might increase and become an issue on model with non-linear complexity over the input sequence's length.</p>
<p><img src="https://nlp.fast.ai/images/multifit_vocabularies.png" alt="subtokenization" /></p>
<p>Among all the tokenization algorithms, we can highlight a few subtokens algorithms used in Transformers-based SoTA models :</p>
<ul>
<li><a href="https://arxiv.org/abs/1508.07909">Byte Pair Encoding (BPE) - Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015)</a></li>
<li><a href="https://research.google/pubs/pub37842/">Word Piece - Japanese and Korean voice search (Schuster, M., and Nakajima, K., 2015)</a></li>
<li><a href="https://arxiv.org/abs/1804.10959">Unigram Language Model - Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo, T., 2018)</a></li>
<li><a href="https://arxiv.org/abs/1808.06226">Sentence Piece - A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Taku Kudo and John Richardson, 2018)</a></li>
</ul>
<p>Going through all of them is out of the scope of this notebook, so we will just highlight how you can use them.</p>
<h3 id="@huggingface/tokenizers-library">@huggingface/tokenizers library<a class="anchor-link" href="#@huggingface/tokenizers-library"> </a></h3><p>Along with the transformers library, we @huggingface provide a blazing fast tokenization library
able to train, tokenize and decode dozens of Gb/s of text on a common multi-core machine.</p>
<p>The library is written in Rust allowing us to take full advantage of multi-core parallel computations in a native and memory-aware way, on-top of which 
we provide bindings for Python and NodeJS (more bindings may be added in the future).</p>
<p>We designed the library so that it provides all the required blocks to create end-to-end tokenizers in an interchangeable way. In that sense, we provide
these various components:</p>
<ul>
<li><strong>Normalizer</strong>: Executes all the initial transformations over the initial input string. For example when you need to
lowercase some text, maybe strip it, or even apply one of the common unicode normalization process, you will add a Normalizer. </li>
<li><strong>PreTokenizer</strong>: In charge of splitting the initial input string. That's the component that decides where and how to
pre-segment the origin string. The simplest example would be like we saw before, to simply split on spaces.</li>
<li><strong>Model</strong>: Handles all the sub-token discovery and generation, this part is trainable and really dependant
of your input data.</li>
<li><strong>Post-Processor</strong>: Provides advanced construction features to be compatible with some of the Transformers-based SoTA
models. For instance, for BERT it would wrap the tokenized sentence around [CLS] and [SEP] tokens.</li>
<li><strong>Decoder</strong>: In charge of mapping back a tokenized input to the original string. The decoder is usually chosen according
to the <code>PreTokenizer</code> we used previously.</li>
<li><strong>Trainer</strong>: Provides training capabilities to each model.</li>
</ul>
<p>For each of the components above we provide multiple implementations:</p>
<ul>
<li><strong>Normalizer</strong>: Lowercase, Unicode (NFD, NFKD, NFC, NFKC), Bert, Strip, ...</li>
<li><strong>PreTokenizer</strong>: ByteLevel, WhitespaceSplit, CharDelimiterSplit, Metaspace, ...</li>
<li><strong>Model</strong>: WordLevel, BPE, WordPiece</li>
<li><strong>Post-Processor</strong>: BertProcessor, ...</li>
<li><strong>Decoder</strong>: WordLevel, BPE, WordPiece, ...</li>
</ul>
<p>All of these building blocks can be combined to create working tokenization pipelines. 
In the next section we will go over our first pipeline.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Alright, now we are ready to implement our first tokenization pipeline through <code>tokenizers</code>.</p>
<p>For this, we will train a Byte-Pair Encoding (BPE) tokenizer on a quite small input for the purpose of this notebook.
We will work with <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjYp9Ppru_nAhUBzIUKHfbUAG8QFjAAegQIBhAB&amp;url=https%3A%2F%2Fnorvig.com%2Fbig.txt&amp;usg=AOvVaw2ed9iwhcP1RKUiEROs15Dz">the file from Peter Norving</a>.
This file contains around 130.000 lines of raw text that will be processed by the library to generate a working tokenizer.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install tokenizers
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">BIG_FILE_URL</span> <span class="o">=</span> <span class="s1">&#39;https://raw.githubusercontent.com/dscape/spell/master/test/resources/big.txt&#39;</span>

<span class="c1"># Let&#39;s download the file and save it somewhere</span>
<span class="kn">from</span> <span class="nn">requests</span> <span class="kn">import</span> <span class="n">get</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;big.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">big_f</span><span class="p">:</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">get</span><span class="p">(</span><span class="n">BIG_FILE_URL</span><span class="p">,</span> <span class="p">)</span>
    
    <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>
        <span class="n">big_f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Unable to get the file: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">reason</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have our training data we need to create the overall pipeline for the tokenizer</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># the overall pipeline for various well-known tokenization algorithm. </span>
<span class="c1"># Everything described below can be replaced by the ByteLevelBPETokenizer class. </span>

<span class="kn">from</span> <span class="nn">tokenizers</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">tokenizers.decoders</span> <span class="kn">import</span> <span class="n">ByteLevel</span> <span class="k">as</span> <span class="n">ByteLevelDecoder</span>
<span class="kn">from</span> <span class="nn">tokenizers.models</span> <span class="kn">import</span> <span class="n">BPE</span>
<span class="kn">from</span> <span class="nn">tokenizers.normalizers</span> <span class="kn">import</span> <span class="n">Lowercase</span><span class="p">,</span> <span class="n">NFKC</span><span class="p">,</span> <span class="n">Sequence</span>
<span class="kn">from</span> <span class="nn">tokenizers.pre_tokenizers</span> <span class="kn">import</span> <span class="n">ByteLevel</span>

<span class="c1"># First we create an empty Byte-Pair Encoding model (i.e. not trained model)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">BPE</span><span class="p">())</span>

<span class="c1"># Then we enable lower-casing and unicode-normalization</span>
<span class="c1"># The Sequence normalizer allows us to combine multiple Normalizer that will be</span>
<span class="c1"># executed in order.</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">Sequence</span><span class="p">([</span>
    <span class="n">NFKC</span><span class="p">(),</span>
    <span class="n">Lowercase</span><span class="p">()</span>
<span class="p">])</span>

<span class="c1"># Our tokenizer also needs a pre-tokenizer responsible for converting the input to a ByteLevel representation.</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pre_tokenizer</span> <span class="o">=</span> <span class="n">ByteLevel</span><span class="p">()</span>

<span class="c1"># And finally, let&#39;s plug a decoder so we can recover from a tokenized input to the original one</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">ByteLevelDecoder</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The overall pipeline is now ready to be trained on the corpus we downloaded earlier in this notebook.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tokenizers.trainers</span> <span class="kn">import</span> <span class="n">BpeTrainer</span>

<span class="c1"># We initialize our trainer, giving him the details about the vocabulary we want to generate</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">BpeTrainer</span><span class="p">(</span><span class="n">vocab_size</span><span class="o">=</span><span class="mi">25000</span><span class="p">,</span> <span class="n">show_progress</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">initial_alphabet</span><span class="o">=</span><span class="n">ByteLevel</span><span class="o">.</span><span class="n">alphabet</span><span class="p">())</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;big.txt&quot;</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Trained vocab size: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">get_vocab_size</span><span class="p">()))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Trained vocab size: 25000
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Et voilà ! You trained your very first tokenizer from scratch using <code>tokenizers</code>. Of course, this 
covers only the basics, and you may want to have a look at the <code>add_special_tokens</code> or <code>special_tokens</code> parameters
on the <code>Trainer</code> class, but the overall process should be very similar.</p>
<p>We can save the content of the model to reuse it later.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&#39;./vocab.json&#39;, &#39;./merges.txt&#39;]</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, let load the trained model and start using out newly trained tokenizer</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">BPE</span><span class="p">(</span><span class="s1">&#39;vocab.json&#39;</span><span class="p">,</span> <span class="s1">&#39;merges.txt&#39;</span><span class="p">)</span>
<span class="n">encoding</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;This is a simple input to be tokenized&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Encoded string: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">encoding</span><span class="o">.</span><span class="n">tokens</span><span class="p">))</span>

<span class="n">decoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">encoding</span><span class="o">.</span><span class="n">ids</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Decoded string: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">decoded</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Encoded string: [&#39;Ġthis&#39;, &#39;Ġis&#39;, &#39;Ġa&#39;, &#39;Ġsimple&#39;, &#39;Ġin&#39;, &#39;put&#39;, &#39;Ġto&#39;, &#39;Ġbe&#39;, &#39;Ġtoken&#39;, &#39;ized&#39;]
Decoded string:  this is a simple input to be tokenized
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The Encoding structure exposes multiple properties which are useful when working with transformers models</p>
<ul>
<li>normalized_str: The input string after normalization (lower-casing, unicode, stripping, etc.)</li>
<li>original_str: The input string as it was provided</li>
<li>tokens: The generated tokens with their string representation</li>
<li>input_ids: The generated tokens with their integer representation</li>
<li>attention_mask: If your input has been padded by the tokenizer, then this would be a vector of 1 for any non padded token and 0 for padded ones.</li>
<li>special_token_mask: If your input contains special tokens such as [CLS], [SEP], [MASK], [PAD], then this would be a vector with 1 in places where a special token has been added.</li>
<li>type_ids: If your input was made of multiple "parts" such as (question, context), then this would be a vector with for each token the segment it belongs to.</li>
<li>overflowing: If your input has been truncated into multiple subparts because of a length limit (for BERT for example the sequence length is limited to 512), this will contain all the remaining overflowing parts.</li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="recohut/notebook"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/nlp/tokenization/2021/01/22/training-tokenizers.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Jupyter notebook database.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/recohut" target="_blank" title="recohut"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
