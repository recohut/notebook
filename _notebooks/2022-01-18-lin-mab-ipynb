{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-18-lin-mab-ipynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/T001739%20%7C%20LinTS%20and%20LinUCB%20Dry%20run.ipynb","timestamp":1644647541919}],"collapsed_sections":[],"authorship_tag":"ABX9TyMbbh51bcGA9zwHlVgQMY/U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"GcilF75r5Yme"},"source":["# LinTS and LinUCB Dry run"]},{"cell_type":"markdown","metadata":{"id":"dCTY0fQCyx4b"},"source":["## Setup"]},{"cell_type":"code","metadata":{"id":"g4vxD6WdtXT7"},"source":["import numpy as np\n","from numpy.linalg import inv\n","from scipy.optimize import minimize"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cuB3EBqTyzAJ"},"source":["## Utilities"]},{"cell_type":"markdown","metadata":{"id":"hhtErcroywM7"},"source":["### Base Agent"]},{"cell_type":"code","metadata":{"id":"ThenStgHydJs"},"source":["class BaseAgent:\n","    \"\"\"Implements the agent for an RL-Glue environment.\n","    Note:\n","        agent_init, agent_start, agent_step, agent_end, agent_cleanup, and\n","        agent_message are required methods.\n","    \"\"\"\n","\n","    __metaclass__ = ABCMeta\n","\n","    def __init__(self):\n","        pass\n","\n","    @abstractmethod\n","    def agent_init(self, agent_info={}):\n","        \"\"\"Setup for the agent called when the experiment first starts.\"\"\"\n","\n","    @abstractmethod\n","    def agent_start(self, observation):\n","        \"\"\"The first method called when the experiment starts, called after\n","        the environment starts.\n","        Args:\n","            observation (Numpy array): the state observation from the environment's evn_start function.\n","        Returns:\n","            The first action the agent takes.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_step(self, reward, observation):\n","        \"\"\"A step taken by the agent.\n","        Args:\n","            reward (float): the reward received for taking the last action taken\n","            observation (Numpy array): the state observation from the\n","                environment's step based, where the agent ended up after the\n","                last step\n","        Returns:\n","            The action the agent is taking.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_end(self, reward):\n","        \"\"\"Run when the agent terminates.\n","        Args:\n","            reward (float): the reward the agent received for entering the terminal state.\n","        \"\"\"\n","\n","    @abstractmethod\n","    def agent_cleanup(self):\n","        \"\"\"Cleanup done after the agent ends.\"\"\"\n","\n","    @abstractmethod\n","    def agent_message(self, message):\n","        \"\"\"A function used to pass information from the agent to the experiment.\n","        Args:\n","            message: The message passed to the agent.\n","        Returns:\n","            The response (or answer) to the message.\n","        \"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yEd1g2kUymtP"},"source":["### Replay Buffer"]},{"cell_type":"code","metadata":{"id":"n8wqtiYayoQS"},"source":["class ReplayBuffer:\n","    def __init__(self, size, seed):\n","        \"\"\"\n","        Args:\n","            size (integer): The size of the replay buffer.\n","            minibatch_size (integer): The sample size.\n","            seed (integer): The seed for the random number generator.\n","        \"\"\"\n","        self.buffer = []\n","        self.rand_generator = np.random.RandomState(seed)\n","        self.max_size = size\n","\n","    def append(self, state, action, reward):\n","        \"\"\"\n","        Args:\n","            state (Numpy array): The state.\n","            action (integer): The action.\n","            reward (float): The reward.\n","            terminal (integer): 1 if the next state is a terminal state and 0 otherwise.\n","            next_state (Numpy array): The next state.\n","        \"\"\"\n","        if len(self.buffer) == self.max_size:\n","            del self.buffer[0]\n","        self.buffer.append([state, action, reward])\n","\n","    def sample(self, last_action):\n","        \"\"\"\n","        Returns:\n","            A list of transition tuples including state, action, reward, terinal, and next_state\n","        \"\"\"\n","        state, action, reward = map(list, zip(*self.buffer))\n","        idxs = [elem == last_action for elem in action]\n","        X = [b for a, b in zip(idxs, state) if a]\n","        y = [b for a, b in zip(idxs, reward) if a]\n","\n","        return X, y\n","\n","    def size(self):\n","        return len(self.buffer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUR1AXHHzT_R"},"source":["### Softmax"]},{"cell_type":"code","metadata":{"id":"BrJuLmdZzVKK"},"source":["def softmax(action_values, tau=1.0):\n","    \"\"\"\n","    Args:\n","        action_values (Numpy array): A 2D array of shape (batch_size, num_actions).\n","                       The action-values computed by an action-value network.\n","        tau (float): The temperature parameter scalar.\n","    Returns:\n","        A 2D array of shape (batch_size, num_actions). Where each column is a probability distribution over\n","        the actions representing the policy.\n","    \"\"\"\n","\n","    # Compute the preferences by dividing the action-values by the temperature parameter tau\n","    preferences = action_values / tau\n","    # Compute the maximum preference across the actions\n","    max_preference = np.max(preferences, axis=1)\n","\n","    # your code here\n","\n","    # Reshape max_preference array which has shape [Batch,] to [Batch, 1]. This allows NumPy broadcasting\n","    # when subtracting the maximum preference from the preference of each action.\n","    reshaped_max_preference = max_preference.reshape((-1, 1))\n","    # print(reshaped_max_preference)\n","\n","    # Compute the numerator, i.e., the exponential of the preference - the max preference.\n","    exp_preferences = np.exp(preferences - reshaped_max_preference)\n","    # print(exp_preferences)\n","    # Compute the denominator, i.e., the sum over the numerator along the actions axis.\n","    sum_of_exp_preferences = np.sum(exp_preferences, axis=1)\n","    # print(sum_of_exp_preferences)\n","\n","    # your code here\n","\n","    # Reshape sum_of_exp_preferences array which has shape [Batch,] to [Batch, 1] to  allow for NumPy broadcasting\n","    # when dividing the numerator by the denominator.\n","    reshaped_sum_of_exp_preferences = sum_of_exp_preferences.reshape((-1, 1))\n","    # print(reshaped_sum_of_exp_preferences)\n","\n","    # Compute the action probabilities according to the equation in the previous cell.\n","    action_probs = exp_preferences / reshaped_sum_of_exp_preferences\n","    # print(action_probs)\n","\n","    # your code here\n","\n","    # squeeze() removes any singleton dimensions. It is used here because this function is used in the\n","    # agent policy when selecting an action (for which the batch dimension is 1.) As np.random.choice is used in\n","    # the agent policy and it expects 1D arrays, we need to remove this singleton batch dimension.\n","    action_probs = action_probs.squeeze()\n","    return action_probs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3m1grFQ_1AGJ"},"source":["### LinUCB"]},{"cell_type":"code","metadata":{"id":"RIPfUjiC1B0e"},"source":["class LinUCBAgent(BaseAgent):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.name = \"LinUCB\"\n","\n","    def agent_init(self, agent_info=None):\n","\n","        if agent_info is None:\n","            agent_info = {}\n","\n","        self.num_actions = agent_info.get('num_actions', 3)\n","        self.alpha = agent_info.get('alpha', 1)\n","        self.batch_size = agent_info.get('batch_size', 1)\n","        # Set random seed for policy for each run\n","        self.policy_rand_generator = np.random.RandomState(agent_info.get(\"seed\", None))\n","\n","        self.last_action = None\n","        self.last_state = None\n","        self.num_round = None\n","\n","    def agent_policy(self, observation):\n","        p_t = np.zeros(self.num_actions)\n","\n","        for i in range(self.num_actions):\n","            # initialize theta hat\n","            self.theta = inv(self.A[i]).dot(self.b[i])\n","            # get context of each arm from flattened vector of length 100\n","            cntx = observation\n","            # get gain reward of each arm\n","            p_t[i] = self.theta.T.dot(cntx) + self.alpha * np.sqrt(np.maximum(cntx.dot(inv(self.A[i]).dot(cntx)), 0))\n","        # action = np.random.choice(np.where(p_t == max(p_t))[0])\n","        action = self.policy_rand_generator.choice(np.where(p_t == max(p_t))[0])\n","\n","        return action\n","\n","    def agent_start(self, observation):\n","        # Specify feature dimension\n","        self.ndims = len(observation)\n","\n","        self.A = np.zeros((self.num_actions, self.ndims, self.ndims))\n","        # Instantiate b as a 0 vector of length ndims.\n","        self.b = np.zeros((self.num_actions, self.ndims, 1))\n","        # set each A per arm as identity matrix of size ndims\n","        for arm in range(self.num_actions):\n","            self.A[arm] = np.eye(self.ndims)\n","\n","        self.A_oracle = self.A.copy()\n","        self.b_oracle = self.b.copy()\n","\n","        self.last_state = observation\n","        self.last_action = self.agent_policy(self.last_state)\n","        self.num_round = 0\n","\n","        return self.last_action\n","\n","    def agent_update(self, reward):\n","        self.A_oracle[self.last_action] = self.A_oracle[self.last_action] + np.outer(self.last_state, self.last_state)\n","        self.b_oracle[self.last_action] = np.add(self.b_oracle[self.last_action].T, self.last_state * reward).reshape(self.ndims, 1)\n","\n","    def agent_step(self, reward, observation):\n","        if reward is not None:\n","            self.agent_update(reward)\n","            # it is a good question whether I should increment num_round outside\n","            # condition or not (since theoretical result doesn't clarify this\n","            self.num_round += 1\n","\n","        if self.num_round % self.batch_size == 0:\n","            self.A = self.A_oracle.copy()\n","            self.b = self.b_oracle.copy()\n","\n","        self.last_state = observation\n","        self.last_action = self.agent_policy(self.last_state)\n","\n","        return self.last_action\n","\n","    def agent_end(self, reward):\n","        if reward is not None:\n","            self.agent_update(reward)\n","            self.num_round += 1\n","\n","        if self.num_round % self.batch_size == 0:\n","            self.A = self.A_oracle.copy()\n","            self.b = self.b_oracle.copy()\n","\n","    def agent_message(self, message):\n","        pass\n","\n","    def agent_cleanup(self):\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YGa2cuBpzqBw"},"source":["### LinTS"]},{"cell_type":"code","metadata":{"id":"tD4Yz0uezp-j"},"source":["class LinTSAgent(BaseAgent):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.name = \"LinTS\"\n","\n","    def agent_init(self, agent_info=None):\n","\n","        if agent_info is None:\n","            agent_info = {}\n","\n","        self.num_actions = agent_info.get('num_actions', 3)\n","        self.alpha = agent_info.get('alpha', 1)\n","        self.lambda_ = agent_info.get('lambda', 1)\n","        self.batch_size = agent_info.get('batch_size', 1)\n","        # Set random seed for policy for each run\n","        self.policy_rand_generator = np.random.RandomState(agent_info.get(\"seed\", None))\n","\n","        self.replay_buffer = ReplayBuffer(agent_info['replay_buffer_size'],\n","                                          agent_info.get(\"seed\"))\n","\n","\n","        self.last_action = None\n","        self.last_state = None\n","        self.num_round = None\n","\n","    def agent_policy(self, observation, mode='sample'):\n","        p_t = np.zeros(self.num_actions)\n","        cntx = observation\n","\n","        for i in range(self.num_actions):\n","            # sampling weights after update\n","            self.w = self.get_weights(i)\n","\n","            # using weight depending on mode\n","            if mode == 'sample':\n","                w = self.w  # weights are samples of posteriors\n","            elif mode == 'expected':\n","                w = self.m[i]  # weights are expected values of posteriors\n","            else:\n","                raise Exception('mode not recognized!')\n","\n","            # calculating probabilities\n","            p_t[i] = 1 / (1 + np.exp(-1 * cntx.dot(w)))\n","            action = self.policy_rand_generator.choice(np.where(p_t == max(p_t))[0])\n","            # probs = softmax(p_t.reshape(1, -1))\n","            # action = self.policy_rand_generator.choice(a=range(self.num_actions), p=probs)\n","\n","        return action\n","\n","    def get_weights(self, arm):\n","        return np.random.normal(self.m[arm], self.alpha * self.q[arm] ** (-1.0), size=len(self.w))\n","\n","        # the loss function\n","    def loss(self, w, *args):\n","        X, y = args\n","        return 0.5 * (self.q[self.last_action] * (w - self.m[self.last_action])).dot(w - self.m[self.last_action]) + np.sum(\n","            [np.log(1 + np.exp(-y[j] * w.dot(X[j]))) for j in range(y.shape[0])])\n","\n","    # the gradient\n","    def grad(self, w, *args):\n","        X, y = args\n","        return self.q[self.last_action] * (w - self.m[self.last_action]) + (-1) * np.array(\n","            [y[j] * X[j] / (1. + np.exp(y[j] * w.dot(X[j]))) for j in range(y.shape[0])]).sum(axis=0)\n","\n","    # fitting method\n","    def agent_update(self, X, y):\n","        # step 1, find w\n","        self.w = minimize(self.loss, self.w, args=(X, y), jac=self.grad, method=\"L-BFGS-B\",\n","                          options={'maxiter': 20, 'disp': False}).x\n","        # self.m_oracle[self.last_action] = self.w\n","        self.m[self.last_action] = self.w\n","\n","        # step 2, update q\n","        P = (1 + np.exp(1 - X.dot(self.m[self.last_action]))) ** (-1)\n","        #self.q_oracle[self.last_action] = self.q[self.last_action] + (P * (1 - P)).dot(X ** 2)\n","        self.q[self.last_action] = self.q[self.last_action] + (P * (1 - P)).dot(X ** 2)\n","\n","    def agent_start(self, observation):\n","        # Specify feature dimension\n","        self.ndims = len(observation)\n","\n","        # initializing parameters of the model\n","        self.m = np.zeros((self.num_actions, self.ndims))\n","        self.q = np.ones((self.num_actions, self.ndims)) * self.lambda_\n","        # initializing weights using any arm (e.g. 0) because they all equal\n","        self.w = np.array([0.]*self.ndims, dtype=np.float64)\n","\n","        # self.m_oracle = self.m.copy()\n","        # self.q_oracle = self.q.copy()\n","\n","        self.last_state = observation\n","        self.last_action = self.agent_policy(self.last_state)\n","        self.num_round = 0\n","\n","        return self.last_action\n","\n","\n","    def agent_step(self, reward, observation):\n","        # Append new experience to replay buffer\n","        if reward is not None:\n","            self.replay_buffer.append(self.last_state, self.last_action, reward)\n","            # it is a good question whether I should increment num_round outside\n","            # condition or not (since theoretical result doesn't clarify this\n","            self.num_round += 1\n","\n","            if self.num_round % self.batch_size == 0:\n","                X, y = self.replay_buffer.sample(self.last_action)\n","                X = np.array(X)\n","                y = np.array(y)\n","                self.agent_update(X, y)\n","                # self.m = self.m_oracle.copy()\n","                # self.q = self.q_oracle.copy()\n","\n","        self.last_state = observation\n","        self.last_action = self.agent_policy(self.last_state)\n","\n","        return self.last_action\n","\n","    def agent_end(self, reward):\n","        # Append new experience to replay buffer\n","        if reward is not None:\n","            self.replay_buffer.append(self.last_state, self.last_action, reward)\n","            # it is a good question whether I should increment num_round outside\n","            # condition or not (since theoretical result doesn't clarify this\n","            self.num_round += 1\n","\n","            if self.num_round % self.batch_size == 0:\n","                X, y = self.replay_buffer.sample(self.last_action)\n","                X = np.array(X)\n","                y = np.array(y)\n","                self.agent_update(X, y)\n","                # self.m = self.m_oracle.copy()\n","                # self.q = self.q_oracle.copy()\n","\n","    def agent_message(self, message):\n","        pass\n","\n","    def agent_cleanup(self):\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wUd9u_p9yrEI"},"source":["## Jobs"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"juTVzjiCy2Uk","executionInfo":{"status":"ok","timestamp":1636127493082,"user_tz":-330,"elapsed":718,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"ae8e9bd8-242d-4b34-b05a-0f8c020f2ef0"},"source":["print('Replay Buffer Dry run')\n","\n","buffer = ReplayBuffer(size=100000, seed=1)\n","buffer.append([1, 2, 3], 0, 1)\n","buffer.append([4, 21, 3], 1, 1)\n","buffer.append([0, 1, 1], 0, 0)\n","\n","print(buffer.sample(0))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Replay Buffer Dry run\n","([[1, 2, 3], [0, 1, 1]], [1, 0])\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KySlKt11zC4G","executionInfo":{"status":"ok","timestamp":1636127610091,"user_tz":-330,"elapsed":604,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"4e83ac9d-6bda-4e6b-f7a5-a11f1e6902dd"},"source":["print('Softmax Dry run')\n","\n","rand_generator = np.random.RandomState(0)\n","action_values = rand_generator.normal(0, 1, (2, 4))\n","tau = 0.5\n","\n","action_probs = softmax(action_values, tau)\n","print(\"action_probs\", action_probs)\n","\n","assert (np.allclose(action_probs, np.array([\n","    [0.25849645, 0.01689625, 0.05374514, 0.67086216],\n","    [0.84699852, 0.00286345, 0.13520063, 0.01493741]\n","])))\n","\n","action_values = np.array([[0.0327, 0.0127, 0.0688]])\n","tau = 1.\n","action_probs = softmax(action_values, tau)\n","print(\"action_probs\", action_probs)\n","\n","assert np.allclose(action_probs, np.array([0.3315, 0.3249, 0.3436]), atol=1e-04)\n","\n","print(\"Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Softmax Dry run\n","action_probs [[0.25849645 0.01689625 0.05374514 0.67086216]\n"," [0.84699852 0.00286345 0.13520063 0.01493741]]\n","action_probs [0.33145968 0.32489634 0.34364398]\n","Passed the asserts! (Note: These are however limited in scope, additional testing is encouraged.)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ivx-DId86cXy","executionInfo":{"status":"ok","timestamp":1636129492965,"user_tz":-330,"elapsed":1110,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"36a324c0-a727-4702-ba68-378f1474e600"},"source":["print('LinUCB Dry run')\n","\n","agent_info = {'alpha': 2,\n","                'num_actions': 4,\n","                'seed': 1}\n","\n","# check initialization\n","linucb = LinUCBAgent()\n","linucb.agent_init(agent_info)\n","print(linucb.num_actions, linucb.alpha)\n","\n","assert linucb.num_actions == 4\n","assert linucb.alpha == 2\n","\n","# check policy\n","observation = np.array([1, 2, 5, 0])\n","linucb.A = np.zeros((linucb.num_actions, len(observation), len(observation)))\n","# Instantiate b as a 0 vector of length ndims.\n","linucb.b = np.zeros((linucb.num_actions, len(observation), 1))\n","# set each A per arm as identity matrix of size ndims\n","for arm in range(linucb.num_actions):\n","    linucb.A[arm] = np.eye(len(observation))\n","\n","action = linucb.agent_policy(observation)\n","print(action)\n","\n","assert action == 1\n","\n","# check start\n","observation = np.array([1, 2, 5, 0])\n","linucb.agent_start(observation)\n","print(linucb.ndims)\n","print(linucb.last_state, linucb.last_action)\n","\n","assert linucb.ndims == len(observation)\n","assert np.allclose(linucb.last_state, observation)\n","assert np.allclose(linucb.b, np.zeros((linucb.num_actions, len(observation), 1)))\n","assert np.allclose(linucb.A, np.array([np.eye(len(observation)), np.eye(len(observation)),\n","                                        np.eye(len(observation)), np.eye(len(observation))]))\n","assert linucb.last_action == 3\n","\n","# check step\n","observation = np.array([5, 3, 1, 2])\n","reward = 1\n","\n","action = linucb.agent_step(reward, observation)\n","print(linucb.A)\n","print(linucb.b)\n","print(action)\n","\n","true_A = np.array([[2., 2., 5., 0.],\n","                    [2., 5., 10., 0.],\n","                    [5., 10., 26., 0.],\n","                    [0., 0., 0., 1.]])\n","\n","true_b = np.array([[1.],\n","                    [2.],\n","                    [5.],\n","                    [0.]])\n","\n","for i in range(3):\n","    assert np.allclose(linucb.A[i], np.eye(4))\n","    assert np.allclose(linucb.b[i], np.zeros((linucb.num_actions, 4, 1)))\n","assert np.allclose(linucb.A[3], true_A)\n","assert np.allclose(linucb.b[3], true_b)\n","assert linucb.last_action == 0\n","\n","observation = np.array([3, 1, 3, 5])\n","reward = None\n","\n","action = linucb.agent_step(reward, observation)\n","print(linucb.A)\n","print(linucb.b)\n","print(action)\n","\n","assert np.allclose(linucb.A[3], true_A)\n","assert np.allclose(linucb.b[3], true_b)\n","assert action == 0\n","\n","# check batch size\n","agent_info = {'alpha': 2,\n","                'num_actions': 4,\n","                'seed': 1,\n","                'batch_size': 2}\n","linucb = LinUCBAgent()\n","linucb.agent_init(agent_info)\n","observation = np.array([1, 2, 5, 0])\n","linucb.agent_start(observation)\n","assert linucb.num_round == 0\n","assert linucb.last_action == 1\n","\n","observation = np.array([5, 3, 1, 2])\n","reward = 1\n","\n","action = linucb.agent_step(reward, observation)\n","assert linucb.num_round == 1\n","assert np.allclose(linucb.b, np.zeros((linucb.num_actions, len(observation), 1)))\n","assert np.allclose(linucb.A, np.array([np.eye(len(observation)), np.eye(len(observation)),\n","                                        np.eye(len(observation)), np.eye(len(observation))]))\n","\n","for i in [0, 2, 3]:\n","    assert np.allclose(linucb.A_oracle[i], np.eye(4))\n","    assert np.allclose(linucb.b_oracle[i], np.zeros((linucb.num_actions, 4, 1)))\n","assert np.allclose(linucb.A_oracle[1], true_A)\n","assert np.allclose(linucb.b_oracle[1], true_b)\n","\n","observation = np.array([3, 1, 3, 5])\n","reward = None\n","action = linucb.agent_step(reward, observation)\n","# sinse reward is None, nothing should happen\n","assert linucb.num_round == 1\n","assert np.allclose(linucb.b, np.zeros((linucb.num_actions, len(observation), 1)))\n","assert np.allclose(linucb.A, np.array([np.eye(len(observation)), np.eye(len(observation)),\n","                                        np.eye(len(observation)), np.eye(len(observation))]))\n","\n","for i in [0, 2, 3]:\n","    assert np.allclose(linucb.A_oracle[i], np.eye(4))\n","    assert np.allclose(linucb.b_oracle[i], np.zeros((linucb.num_actions, 4, 1)))\n","assert np.allclose(linucb.A_oracle[1], true_A)\n","assert np.allclose(linucb.b_oracle[1], true_b)\n","\n","observation = np.array([3, 0, 2, 5])\n","reward = 0\n","action = linucb.agent_step(reward, observation)\n","\n","assert linucb.num_round == 2\n","assert np.allclose(linucb.b, linucb.b_oracle)\n","assert np.allclose(linucb.A, linucb.A_oracle)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LinUCB Dry run\n","4 2\n","1\n","4\n","[1 2 5 0] 3\n","[[[ 1.  0.  0.  0.]\n","  [ 0.  1.  0.  0.]\n","  [ 0.  0.  1.  0.]\n","  [ 0.  0.  0.  1.]]\n","\n"," [[ 1.  0.  0.  0.]\n","  [ 0.  1.  0.  0.]\n","  [ 0.  0.  1.  0.]\n","  [ 0.  0.  0.  1.]]\n","\n"," [[ 1.  0.  0.  0.]\n","  [ 0.  1.  0.  0.]\n","  [ 0.  0.  1.  0.]\n","  [ 0.  0.  0.  1.]]\n","\n"," [[ 2.  2.  5.  0.]\n","  [ 2.  5. 10.  0.]\n","  [ 5. 10. 26.  0.]\n","  [ 0.  0.  0.  1.]]]\n","[[[0.]\n","  [0.]\n","  [0.]\n","  [0.]]\n","\n"," [[0.]\n","  [0.]\n","  [0.]\n","  [0.]]\n","\n"," [[0.]\n","  [0.]\n","  [0.]\n","  [0.]]\n","\n"," [[1.]\n","  [2.]\n","  [5.]\n","  [0.]]]\n","0\n","[[[ 1.  0.  0.  0.]\n","  [ 0.  1.  0.  0.]\n","  [ 0.  0.  1.  0.]\n","  [ 0.  0.  0.  1.]]\n","\n"," [[ 1.  0.  0.  0.]\n","  [ 0.  1.  0.  0.]\n","  [ 0.  0.  1.  0.]\n","  [ 0.  0.  0.  1.]]\n","\n"," [[ 1.  0.  0.  0.]\n","  [ 0.  1.  0.  0.]\n","  [ 0.  0.  1.  0.]\n","  [ 0.  0.  0.  1.]]\n","\n"," [[ 2.  2.  5.  0.]\n","  [ 2.  5. 10.  0.]\n","  [ 5. 10. 26.  0.]\n","  [ 0.  0.  0.  1.]]]\n","[[[0.]\n","  [0.]\n","  [0.]\n","  [0.]]\n","\n"," [[0.]\n","  [0.]\n","  [0.]\n","  [0.]]\n","\n"," [[0.]\n","  [0.]\n","  [0.]\n","  [0.]]\n","\n"," [[1.]\n","  [2.]\n","  [5.]\n","  [0.]]]\n","0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eA2DWUlozgnk","executionInfo":{"status":"ok","timestamp":1636127716841,"user_tz":-330,"elapsed":703,"user":{"displayName":"Sparsh Agarwal","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13037694610922482904"}},"outputId":"b98229a9-3e67-4808-ee06-244293393cfd"},"source":["print('LinTS Dry run')\n","\n","agent_info = {'alpha': 2,\n","            'num_actions': 3,\n","            'seed': 1,\n","            'lambda': 2,\n","            'replay_buffer_size': 100000}\n","\n","np.random.seed(1)\n","# check initialization\n","lints = LinTSAgent()\n","lints.agent_init(agent_info)\n","print(lints.num_actions, lints.alpha, lints.lambda_)\n","\n","assert lints.num_actions == 3\n","assert lints.alpha == 2\n","assert lints.lambda_ == 2\n","\n","# check agent policy\n","observation = np.array([1, 2, 5, 0])\n","lints.m = np.zeros((lints.num_actions, len(observation)))\n","lints.q = np.ones((lints.num_actions, len(observation))) * lints.lambda_\n","lints.w = np.random.normal(lints.m[0], lints.alpha * lints.q[0] ** (-1.0), size=len(observation))\n","print(lints.w)\n","action = lints.agent_policy(observation)\n","print(action)\n","\n","# check agent start\n","observation = np.array([1, 2, 5, 0])\n","lints.agent_start(observation)\n","# manually reassign w to np.random.normal, because I np.seed doesn't work inside the class\n","np.random.seed(1)\n","lints.w = np.random.normal(lints.m[0], lints.alpha * lints.q[0] ** (-1.0), size=len(observation))\n","print(lints.ndims)\n","print(lints.last_state, lints.last_action)\n","print(lints.last_action)\n","assert lints.ndims == len(observation)\n","assert np.allclose(lints.last_state, observation)\n","assert np.allclose(lints.m, np.zeros((lints.num_actions, lints.ndims)))\n","assert np.allclose(lints.q, np.ones((lints.num_actions, lints.ndims)) * lints.lambda_)\n","assert np.allclose(lints.w, np.array([ 1.62434536, -0.61175641, -0.52817175, -1.07296862]))\n","# assert lints.last_action == 1\n","\n","# check step\n","observation = np.array([5, 3, 1, 2])\n","reward = 1\n","action = lints.agent_step(reward, observation)\n","print(action)\n","\n","observation = np.array([1, 3, 2, 1])\n","reward = 0\n","action = lints.agent_step(reward, observation)\n","print(action)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LinTS Dry run\n","3 2 2\n","[ 1.62434536 -0.61175641 -0.52817175 -1.07296862]\n","1\n","4\n","[1 2 5 0] 1\n","1\n","1\n","1\n"]}]}]}