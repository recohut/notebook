{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2022-01-07-nn-scratch.pynb","provenance":[{"file_id":"https://github.com/recohut/nbs/blob/main/raw/NOTES.ipynb","timestamp":1644444258608}],"collapsed_sections":[],"authorship_tag":"ABX9TyMP6GxsnZm6ENuHXsI4A9Ag"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3aHF9fVDtHue"},"source":["# Neural network from scratch"]},{"cell_type":"code","metadata":{"id":"iUFvTXp7s07w"},"source":["import numpy as np\n","\n","class Neuron(object):\n","    \"\"\"A simple feed-forward artificial neuron.\n","    Args:\n","        num_inputs (int): The input vector size / number of input values.\n","        activation_fn (callable): The activation function.\n","    Attributes:\n","        W (ndarray): The weight values for each input.\n","        b (float): The bias value, added to the weighted sum.\n","        activation_fn (callable): The activation function.\n","    \"\"\"\n","    def __init__(self, num_inputs, activation_fn):\n","        super().__init__()\n","        # Randomly initializing the weight vector and bias value:\n","        self.W = np.random.rand(num_inputs)\n","        self.b = np.random.rand(1)\n","        self.activation_fn = activation_fn\n","\n","    def forward(self, x):\n","        \"\"\"Forward the input signal through the neuron.\"\"\"\n","        z = np.dot(x, self.W) + self.b\n","        return self.activation_fn(z)\n","\n","\n","# Fixing the random number generator's seed, for reproducible results:\n","np.random.seed(42)\n","# Random input column array of 3 values (shape = `(1, 3)`)\n","x = np.random.rand(3).reshape(1, 3)\n","# Instantiating a Perceptron (simple neuron with step function):\n","step_fn = lambda y: 0 if y <= 0 else 1\n","perceptron = Neuron(num_inputs=x.size, activation_fn=step_fn)\n","out = perceptron.forward(x)\n","print(out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xq6gKaPk4L1T"},"source":["import numpy as np\n","\n","class FullyConnectedLayer(object):\n","    \"\"\"A simple fully-connected NN layer.\n","    Args:\n","        num_inputs (int): The input vector size/number of input values.\n","        layer_size (int): The output vector size/number of neurons.\n","        activation_fn (callable): The activation function for this layer.\n","    Attributes:\n","        W (ndarray): The weight values for each input.\n","        b (ndarray): The bias value, added to the weighted sum.\n","        size (int): The layer size/number of neurons.\n","        activation_fn (callable): The neurons' activation function.\n","    \"\"\"\n","    def __init__(self, num_inputs, layer_size, activation_fn):\n","        super().__init__()\n","        # Randomly initializing the parameters (using a normal distribution this time):\n","        self.W = np.random.standard_normal((num_inputs, layer_size))\n","        self.b = np.random.standard_normal(layer_size)\n","        self.size = layer_size\n","        self.activation_fn = activation_fn\n","\n","    def forward(self, x):\n","        \"\"\"Forward the input signal through the layer.\"\"\"\n","        z = np.dot(x, self.W) + self.b\n","        return self.activation_fn(z)\n","\n","np.random.seed(42)\n","# Random input column-vectors of 2 values (shape = `(1, 2)`):\n","x1 = np.random.uniform(-1, 1, 2).reshape(1, 2)\n","# > [[-0.25091976  0.90142861]]\n","x2 = np.random.uniform(-1, 1, 2).reshape(1, 2)    \n","# > [[0.46398788 0.19731697]]\n","\n","relu_fn = lambda y: np.maximum(y, 0)    # Defining our activation function\n","layer = FullyConnectedLayer(2, 3, relu_fn)\n","\n","# Our layer can process x1 and x2 separately...\n","out1 = layer.forward(x1)\n","# > [[0.28712364 0.         0.33478571]]\n","out2 = layer.forward(x2)\n","# > [[0.         0.         1.08175419]]\n","# ... or together:\n","x12 = np.concatenate((x1, x2))  # stack of input vectors, of shape `(2, 2)`\n","out12 = layer.forward(x12)\n","print(out12)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qL-zszHs8FRJ"},"source":["#==============================================================================\n","# Imported Modules (run above cell)\n","#==============================================================================\n","\n","# import numpy as np\n","# from fully_connected_layer import FullyConnectedLayer\n","\n","\n","#==============================================================================\n","# Function Definitions\n","#==============================================================================\n","\n","def sigmoid(x):     # sigmoid function\n","    return 1 / (1 + np.exp(-x)) # y\n","\n","\n","def derivated_sigmoid(y):   # sigmoid derivative function\n","    return y * (1 - y)\n","\n","\n","def loss_L2(pred, target):    # L2 loss function\n","    return np.sum(np.square(pred - target)) / pred.shape[0] # opt. we divide by the batch size\n","\n","\n","def derivated_loss_L2(pred, target):    # L2 derivative function\n","    return 2 * (pred - target)\n","\n","\n","def cross_entropy(pred, target):    # cross-entropy loss function\n","    return -np.mean(np.multiply(np.log(pred), target) + np.multiply(np.log(1 - pred), (1 - target)))\n","\n","\n","def derivated_cross_entropy(pred, target):    # cross-entropy derivative function\n","    return (pred - target) / (pred * (1 - pred))\n","\n","\n","#==============================================================================\n","# Class Definition\n","#==============================================================================\n","\n","class SimpleNetwork(object):\n","    \"\"\"A simple fully-connected NN.\n","    Args:\n","        num_inputs (int): The input vector size / number of input values.\n","        num_outputs (int): The output vector size.\n","        hidden_layers_sizes (list): A list of sizes for each hidden layer to add to the network\n","        activation_function (callable): The activation function for all the layers\n","        derivated_activation_function (callable): The derivated activation function\n","        loss_function (callable): The loss function to train this network\n","        derivated_loss_function (callable): The derivative of the loss function, for back-propagation\n","    Attributes:\n","        layers (list): The list of layers forming this simple network.\n","        loss_function (callable): The loss function to train this network.\n","        derivated_loss_function (callable): The derivative of the loss function, for back-propagation.\n","    \"\"\"\n","\n","    def __init__(self, num_inputs, num_outputs, hidden_layers_sizes=(64, 32),\n","                 activation_function=sigmoid, derivated_activation_function=derivated_sigmoid,\n","                 loss_function=loss_L2, derivated_loss_function=derivated_loss_L2):\n","        super().__init__()\n","        # We build the list of layers composing the network, according to the provided arguments:\n","        layer_sizes = [num_inputs, *hidden_layers_sizes, num_outputs]\n","        self.layers = [\n","            FullyConnectedLayer(layer_sizes[i], layer_sizes[i + 1], activation_function, derivated_activation_function)\n","            for i in range(len(layer_sizes) - 1)]\n","\n","        self.loss_function = loss_function\n","        self.derivated_loss_function = derivated_loss_function\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward the input vector through the layers, returning the output vector.\n","        Args:\n","            x (ndarray): The input vector, of shape `(batch_size, num_inputs)`.\n","        Returns:\n","            activation (ndarray): The output activation value, of shape `(batch_size, layer_size)`.\n","        \"\"\"\n","        for layer in self.layers: # from the input layer to the output one\n","            x = layer.forward(x)\n","        return x\n","\n","    def predict(self, x):\n","        \"\"\"\n","        Compute the output corresponding to input `x`, and return the index of the largest output value.\n","        Args:\n","            x (ndarray): The input vector, of shape `(1, num_inputs)`.\n","        Returns:\n","            best_class (int): The predicted class ID.\n","        \"\"\"\n","        estimations = self.forward(x)\n","        best_class = np.argmax(estimations)\n","        return best_class\n","\n","    def backward(self, dL_dy):\n","        \"\"\"\n","        Back-propagate the loss hrough the layers (require `forward()` to be called before).\n","        Args:\n","            dL_dy (ndarray): The loss derivative w.r.t. the network's output (dL/dy).\n","        Returns:\n","            dL_dx (ndarray): The loss derivative w.r.t. the network's input (dL/dx).\n","        \"\"\"\n","        for layer in reversed(self.layers): # from the output layer to the input one\n","            dL_dy = layer.backward(dL_dy)\n","        return dL_dy\n","\n","    def optimize(self, epsilon):\n","        \"\"\"\n","        Optimize the network parameters according to the stored gradients (require `backward()` to be called before).\n","        Args:\n","            epsilon (float): The learning rate.\n","        \"\"\"\n","        for layer in self.layers:             # the order doesn't matter here\n","            layer.optimize(epsilon)\n","\n","    def evaluate_accuracy(self, X_val, y_val):\n","        \"\"\"\n","        Given a dataset and its ground-truth labels, evaluate the current accuracy of the network.\n","        Args:\n","            X_val (ndarray): The input validation dataset.\n","            y_val (ndarray): The corresponding ground-truth validation dataset.\n","        Returns:\n","            accuracy (float): The accuracy of the network (= number of correct predictions / dataset size).\n","        \"\"\"\n","        num_corrects = 0\n","        for i in range(len(X_val)):\n","            pred_class = self.predict(X_val[i])\n","            if pred_class == y_val[i]:\n","                num_corrects += 1\n","        return num_corrects / len(X_val)\n","\n","    def train(self, X_train, y_train, X_val=None, y_val=None, batch_size=32, num_epochs=5, learning_rate=1e-3):\n","        \"\"\"\n","        Given a dataset and its ground-truth labels, evaluate the current accuracy of the network.\n","        Args:\n","            X_train (ndarray): The input training dataset.\n","            y_train (ndarray): The corresponding ground-truth training dataset.\n","            X_val (ndarray): The input validation dataset.\n","            y_val (ndarray): The corresponding ground-truth validation dataset.\n","            batch_size (int): The mini-batch size.\n","            num_epochs (int): The number of training epochs i.e. iterations over the whole dataset.\n","            learning_rate (float): The learning rate to scale the derivatives.\n","        Returns:\n","            losses (list): The list of training losses for each epoch.\n","            accuracies (list): The list of validation accuracy values for each epoch.\n","        \"\"\"\n","        num_batches_per_epoch = len(X_train) // batch_size\n","        do_validation = X_val is not None and y_val is not None\n","        losses, accuracies = [], []\n","        for i in range(num_epochs): # for each training epoch\n","            epoch_loss = 0\n","            for b in range(num_batches_per_epoch):  # for each batch composing the dataset\n","                # Get batch:\n","                batch_index_begin = b * batch_size\n","                batch_index_end = batch_index_begin + batch_size\n","                x = X_train[batch_index_begin: batch_index_end]\n","                targets = y_train[batch_index_begin: batch_index_end]\n","                # Optimize on batch:\n","                predictions = y = self.forward(x)  # forward pass\n","                L = self.loss_function(predictions, targets)  # loss computation\n","                dL_dy = self.derivated_loss_function(predictions, targets)  # loss derivation\n","                self.backward(dL_dy)  # back-propagation pass\n","                self.optimize(learning_rate)  # optimization of the NN\n","                epoch_loss += L\n","\n","            # Logging training loss and validation accuracy, to follow the training:\n","            epoch_loss /= num_batches_per_epoch\n","            losses.append(epoch_loss)\n","            if do_validation:\n","                accuracy = self.evaluate_accuracy(X_val, y_val)\n","                accuracies.append(accuracy)\n","            else:\n","                accuracy = np.NaN\n","            print(\"Epoch {:4d}: training loss = {:.6f} | val accuracy = {:.2f}%\".format(i, epoch_loss, accuracy * 100))\n","        return losses, accuracies"],"execution_count":null,"outputs":[]}]}